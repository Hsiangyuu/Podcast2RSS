{
  "pid": "648b0b641c48983391a63f98",
  "eid": "67359e9843dc3a43878eaa90",
  "title": "我在 Character.ai 做 Post Training｜对谈前 C.AI 模型应用算法专家 Ted",
  "task_id": "p7g395y8mdp5qz65",
  "transcription": [
    {
      "time": "00:00:02",
      "text": "Do something there.",
      "speaker": "发言人1"
    },
    {
      "time": "00:00:15",
      "text": "我们今天很开心请到了一个老朋友，你们认识蛮久的了。然后认识太太的时候，太太是还在CAI，最近是自己刚出来。太太你可以给大家先简单介绍一下你的经历。",
      "speaker": "发言人2"
    },
    {
      "time": "00:00:28",
      "text": "好的，我最近也是刚离开CEI，在CEI差不多待了一年，主要做的就是大模型的post圈里也就微调这个部分。然后在那之前其实就一直是在硅谷的大厂robots meta、google、apple这样跳来跳去，做的也都是比较经典的全栈工程师，偏后端一些。但是在过去一年，在CAI有一个很奇妙的机会，让我可以加入到微调这边。和很多的研究员一起把一个不太会说人话的大模型调调，一直调到我走之前应该都算是业界最好的这样一个对话式大模型。",
      "speaker": "发言人1"
    },
    {
      "time": "00:01:06",
      "text": "哼大家都在讲CAI这款产品，它的用户群其实也是有点像rob blocks对吧？也是偏年轻的，然后二次元什么那些群体。但是大家就觉得它的商业化一直没做起来，这个的原因是什么呢？",
      "speaker": "发言人2"
    },
    {
      "time": "00:01:19",
      "text": "简单概括两点，一没有怎么做到目前为止就是一个简单的订阅。另外一个就是我觉得这个团队可能之前思考的也少。我在走之前团队是刚刚开始积极探索这个可能性。",
      "speaker": "发言人1"
    },
    {
      "time": "00:01:32",
      "text": "所以你觉得不是说用户人群什么的有问题，或者说现在还不知道有没有问题，就是现在最核心问题是公司层面就还没有去做很多的探索跟尝试是吧？对我在想这个原因到底是什么？因为你看国内的AI创业公司，这两年基本形成一个共识，就是商业化甚至于是最重要的一件事情了。当然跟国内的融投资环境什么的也有关系。但好像美国那边的产品，哪怕cii做了这么久，这么大的用户量，然后好像也没有强调要做商业化这件事儿。",
      "speaker": "发言人2"
    },
    {
      "time": "00:02:00",
      "text": "是吧？这个说起来有一点感觉像是开玩笑，但我是很认真的在说这句话。因为我们有的网以及会为了弄买账的大量投资人，导致商业化在很长期内都不是我们公司发展的一个重点。",
      "speaker": "发言人1"
    },
    {
      "time": "00:02:14",
      "text": "就是因为你们相信AGI对吧？简单来讲是这样。",
      "speaker": "发言人2"
    },
    {
      "time": "00:02:19",
      "text": "可以说很长一段时间内，我们所有的娱乐的属性都是我们narb为主的团队追求AGI路上的一个副产物。",
      "speaker": "发言人1"
    },
    {
      "time": "00:02:27",
      "text": "我记得你去CI的时候，他们大概有40个人左右是吧？对的，但大多数应该都是在做模型训练的人。",
      "speaker": "发言人2"
    },
    {
      "time": "00:02:34",
      "text": "当时是的，我加入的时候应该将近25人都是围绕着模型训练，就是postion这一堆。然后5个到7个左右是行政的，不到10人是前端后端还有一些运维。",
      "speaker": "发言人1"
    },
    {
      "time": "00:02:49",
      "text": "对然后再往后可能慢慢的才把产品什么的这些人开始招起来，是吧？因为国内其实我记得大概一年前左右，大家都在盛传一个事，就是CEI里面其实只有半个产品经理在做事情。",
      "speaker": "发言人2"
    },
    {
      "time": "00:03:04",
      "text": "对的，我加入的时候，其实全公司是没有一个真正title是产品经理的人。那半个是一个step chat的一个高管他等于说是兼职，是作为我们的顾问。但我觉得这老哥非常厉害。超级产品经理。",
      "speaker": "发言人1"
    },
    {
      "time": "00:03:18",
      "text": "你觉得美国的超级厉害的产品经理，尤其在to c端的他们和因为你也接触很多国内的公司，你觉得有什么很明显的差异吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:03:28",
      "text": "我接触的这种超级产品经理不多，所以说可能有点偏颇。但我在美国接触过的超级产品经理和我在国内看到的，比如说是张小龙这样或者类似的人。他们给我的一个感觉是国内可能更加重人文化。一就是他会总结出一些很好的方法论。但是在美国这边我接触到的人，他可能在数据分析上非常深入。他可能跟那种数据科学家有差不多同样那些比较复杂SQL的能力。并且他也很愿意亲身的去分析各种各样的数据，寻找里面的inside。",
      "speaker": "发言人1"
    },
    {
      "time": "00:03:59",
      "text": "对，这个我前段时间也发现了类似的一个点，就是突然我刷到了一个美国那边的产品经理的模拟面试。然后我就发现他整个模拟面试的过程非常像管理咨询的那个面试，就给你出个题说。最近用户行为发生了什么变化，他说怎么样了。然后被面试者就要解析说那可能我要看哪几个数据，然后他是不是怎么样了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:04:23",
      "text": "对，因为to c甚至说过去几年最热门的几个超级大的社交网络的兴起，其实背后都是比较严格的那种数据分析作为底子。所以无论是数据科学家也好，还是产品经理也好，技能的发展还有经验的积累都会。",
      "speaker": "发言人1"
    },
    {
      "time": "00:04:38",
      "text": "围绕这个是OK。所以讲回来就是CAI你觉得在你待的一年多的时间里面，你进去的时候他应该是非常向好的一个状态，对吧？然后你走的时候他就已经被拆开，然后一部分人回到那个大公司了。就在这过程当中，你都看到了他的哪些发展的问题，最后导致的这么一个结果。",
      "speaker": "发言人2"
    },
    {
      "time": "00:05:02",
      "text": "其实就像徐凯你之前写过的那篇文章，就是团队很长期里面就处在一个既要一个产品又要一个AGI这样的愿景。两件事都要做，最后的结果倒推看，就是你不能两边都抓，你必须得要放弃一遍。",
      "speaker": "发言人1"
    },
    {
      "time": "00:05:16",
      "text": "所以如果重来一遍，你觉得CI应该改变哪件事，可能会让它发展的更好？",
      "speaker": "发言人2"
    },
    {
      "time": "00:05:23",
      "text": "我个人很偏颇的讲，我希望我们应该往AGI再赌一把。大的怎么去赌？可能我入职之前的很多决策都要为纯粹的AGI去服务。在这过程中我相信肯定会损失很多的用户增长。但是也有可能现在国外基本只剩下五家大lab的，里面也许能挤进去我们一家。",
      "speaker": "发言人1"
    },
    {
      "time": "00:05:42",
      "text": "对我觉得如果真的能在幻想中运营的话，可能当时把Carry点开前面那个产品直接卖给自己，或者卖给mini max对吧？然后后面就好好的做模型，也许是一条出路。当然这个可能政策上就不太能实现了。所以我正好提个点，这两天正好有个新闻是说toki的量超过了CI因为token的很多里面的人是从字节出来在做的。对，之前有人讲过说，如果把CI这家公司或者是那个产品交给字节系来做，可能是完全不一样。不管是从产品上还是从商业化上来讲，我不知道你怎么看这个问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:06:19",
      "text": "我觉得我会把这个事情更看像是说这个产品对于母公司来说算什么。Cat AI对于我们公司来说就是唯一的产品。但toki对于mini MAX来说，它是要证明他们这个模型有一个非常好的落地的可能性，以及它在海外有非常好的增长。Mini max自己有海螺，也有很丰富的产品矩阵，甚至还有API。所以从这个方面来说，talk I我觉得更像是mini max整个公司的战略的一部分。并不像是说整个mini MAX就全靠talk一个，然后必须就靠它去走出去。",
      "speaker": "发言人1"
    },
    {
      "time": "00:06:50",
      "text": "明白有一小撮人，大概在一两年前大家讨论过一个话题，就是你看一打开CEI产品的网站，一眼看过去就知道没有听过什么运营和雕琢的一个非常简陋的ugc平台的感觉。对，但toki你打开就发现它里面有非常重的这种推送推荐运营和一些其他的相关的东西。然后有人讲说CEI的建筑平台更容易让UGC的一些奇奇怪怪的原生内容长出来。也有人讲说toe这种的就是更重运营的话，可能一些用户的数据行为表现会更好。我不知道你们有没有讨论过类似这样的问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:07:29",
      "text": "我们在今年年初产品团队膨胀之前，我们做过一些推荐算法的调整。结果就当时比较有限的产品的资源能够做到的事情而言，我们发现就不太值得做。因为我们有非常大量的用户数据，我们的整个训练的这个管线又很健全。所以用户跟我们的机器人之间的交互，它能快速的回馈到整个模型，然后再进一步的回馈到全体的角色，跟用户这个反馈的体验。所以从这个角度来说，对我们而言回报最大的还是持续去迭代模型，持续优化在产品的细节上能够得到更多用户反馈的这些部分。而不是说通过运营，通过产品的思路去说，今天我怎么去做一些官方的角色，做一些更多推荐产品的调整。",
      "speaker": "发言人1"
    },
    {
      "time": "00:08:14",
      "text": "所以你们当时在内部没有一个时刻有个人突然说，toki最近涨得挺好的。我们来分析一下他的产品模式，然后看看哪些是能被用到CI产品里面的。我不知道包括CEI在内的美国公司很少会做这样的事儿吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:08:31",
      "text": "不会，我们正常做产品调研talk基本都是永远会在榜上的。因为安全的聊天对吧？就不是说无限制聊天的话，那其实你要看着外面的进没有几个，然后talking他本身又很主动的去做很多我们没有做的一些路线，所以看他这些路线做的怎么样，或者假设我们真的做效果会长成什么样，我们都可以把toki当做一个很好的参考对象。",
      "speaker": "发言人1"
    },
    {
      "time": "00:08:53",
      "text": "所以你们有总结出来他比如做的特别好的一两点东西吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:08:57",
      "text": "首先功能迭代的很快，这一点说实话他是让我们甚至觉得有点羡慕的。Talk I它能够快速的上各种多模态的新功能，或者在内购上面也做的很积极，然后做了很多市场，你可以自己制造卡。这些在我们看来都是很好的idea的这样一个输入。但是我觉得这里要澄清一点的就是我们看toki做这些功能的时候，既是认可另外一种也是一种警示。因为我们可以意识到，如果toki做了这么多丰富的功能，但是它本身的商业化也好，增长也好，依然没有说有一个特别大的突破的话，那可能这些功能本身是不太适合这个产品形态。所以我们更是带着这种中立的状态去看talk king的这些功能。",
      "speaker": "发言人1"
    },
    {
      "time": "00:09:35",
      "text": "明白，CEI现在这个结果其实对mini max来讲，我觉得他们也很尴尬对吧？因为他们是沿着这条路径做的最好的。但是突然发现最前面的标杆好像被大家认为不一定能行得通了。所以你现在你自己还相信CI这类的AI陪伴产品吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:09:57",
      "text": "我本身的话还是非常相信的。但是核心的问题就像徐凯你刚才提到的，商业化真的好做吗？商业化到底能走得多远？如果说这种重度使用的APP，它本身做订阅不好做的话，那是不是只能卖广告？那什么样的广告形式比较好呢？是说直接贴各种小广告放在里面，就先从流量上吃一波红利，还是说有没有更加原生态的，让机器人去给你推荐一些广告。我觉得这里面是有一些甚至技术上来说都还等待你去解锁的可能性在里面。",
      "speaker": "发言人1"
    },
    {
      "time": "00:10:26",
      "text": "但是它的这个可能的用户量，说实话我觉得是不能忽视的。官方的数据可以说，我们现在月活已经到达2000万了，然后日活也已经接近800万了，这跟去年的数字翻了差不多一倍。所以说按照这个速度，明年可能月活是能达到3000万，然后日活可能接近1000万。这样的一个用户群，无论在什么状态下，你其实都不能说可以忽视掉。所以从这个角度，我觉得mini max也不用特别慌，说明他们这个用户量的上限还是可以做到很夸张的。可能在将来随着这些用户慢慢长大，也会变成像是当年X一样。因为你像最开始robbo x作为一个教育软件，走进当时各种学生的家里的时候，其实也没会觉得它将来会变成一个月活4亿的大平台，对吧？",
      "speaker": "发言人1"
    },
    {
      "time": "00:11:10",
      "text": "到底现在是什么样的人群在使用类似CI的产品，我确实没想到它会有这么大的月活，或者说平均的用户时长什么的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:11:21",
      "text": "现在的应该是说年轻女性为主，相对偏二次元。",
      "speaker": "发言人1"
    },
    {
      "time": "00:11:25",
      "text": "你说的年轻大概是多年轻？",
      "speaker": "发言人2"
    },
    {
      "time": "00:11:29",
      "text": "高中生的比例其实没有那么多，大学生和刚上班几年的这种。",
      "speaker": "发言人1"
    },
    {
      "time": "00:11:34",
      "text": "年轻人的比例还更高一点。OK, 我们上周正好跟Jason聊了一期，然后他他其实是觉得说类似CI这样的产品，更多的像是一个互动内容消费，或者说互动小说消费，而不是一个真正的AI陪伴的产品。你同意他这个观点吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:11:51",
      "text": "这一点我非常同意。很多人也会经常问我这个问题，CEI它到底提供的价值是什么呢？我此时就想说它是一个创造性的感情伴侣，它的创造性的部分是非常重的。用户的抽卡每一句文字其实都是一个抽卡的过程。用户输入很多东西，然后机器人说的话他还去筛选。有的时候说的话并不是特别完美的，他还得去替换。在这样一个重输入的过程中，用户的确获得了一些很新的体验。",
      "speaker": "发言人1"
    },
    {
      "time": "00:12:19",
      "text": "所以你回头看，你觉得CEI之所以在这个品类里面做的这么好，大家不管怎么样肯定都觉得他是老大。在这个赛道里面，至少目前为止，它核心的原因和他的优势到底是什么？",
      "speaker": "发言人2"
    },
    {
      "time": "00:12:32",
      "text": "三点，从我技术出身的这个角度来分析的一点，就是我们有nm shazia，他厉害就厉害在他能带着一群其他的天才研究员，在去年这个时间点，把我们的整个的低成本压到可能外面同参数量的这个模型的1%以内。所以这一点导致我们可以轻松的hold得住上千万用户，不至于马上把银行里的钱给烧光。然后第二点，因为我们是自研模型，我们可以去控制它在运行里需要看过那几万亿的语料的比例。所以说我们可以说在初期就不是特别追求AGI的模型的那个情况下，先给大家看足够多的人类语料出来的模型，它自然就能跟人类开展各种各样极大广度的对话。然后第三点就是我们这样一个后训练或者说微调的管线，也是经过了多次迭代之后，它能够现在形成一个特别有效的把用户跟模型之间的反馈快速的进入到管线内部。然后再结合一些外部的数据标注，还甚至说一些用户会帮我们做一些额外的标注，让这些所有的数据能够高效的被模型在微调过程中吸收。就这三点。",
      "speaker": "发言人1"
    },
    {
      "time": "00:13:36",
      "text": "我觉得你刚才提的其实蛮多的是前后端的一些结合。不管是数据上的还是产品上的。从理论上来讲，其实也是大家一直觉得说就是我数据对于模型的表现是最重要的。然后CEI的前端有这么多的用户，这么多的高质量的对话。那这些东西应该是能够反哺到它的模型也变得更好。如果能成为一环，其实这个事儿就成立了，对吧？那最后为什么变成说要把它拆开，或者说这个在模型上好像CI跟其他的那几个比起来也没有一个特别大的优势。",
      "speaker": "发言人2"
    },
    {
      "time": "00:14:11",
      "text": "这里面就有两个点，第一个点就是说肯定在现有的一些常见的一种感情陪伴场景里面，我们的模型的确是没有其他家好。但我刚才说了，我们这个厉害，他厉害的是广度，以及在这个巨大的广度之下，每一个的深度都不差。所以说很多用户在跟其他家聊完之后，发现他还是聊的内容比较窄。就是可能说情情爱爱对吧？或者说一些很经典的霸总的剧情的走向。",
      "speaker": "发言人1"
    },
    {
      "time": "00:14:35",
      "text": "但是你也不可能天天聊这些东西，对吧？你会希望有点变数。当他们习惯了其他家的变数不多的那些场景之后，他都会想起来来我们家试试看。然后发现在这些常见的场景之外，我们依然能跟他聊的有来有回，或者能给他一些启发式的回答。这可能就跟我们模型训练里面见过了太多的自然人类对话，它的这个广度是极其可怕的有关。",
      "speaker": "发言人1"
    },
    {
      "time": "00:14:56",
      "text": "OK. 好，然后我们来讲讲你在CI的这一年多时间，我觉得大家都会好奇说在类似CI这样的公司，你的每天都是怎么度过的，大概会做哪些事情，以及说你具体负责的一些业务是什么？",
      "speaker": "发言人2"
    },
    {
      "time": "00:15:11",
      "text": "每天怎么度过？你要用一句话形容就是主动。996。因为这是我严格来说第一次加入一个初创公司，所以说我是真的是感觉到了我做的每一点事情，或者我看的每一点的额外的数据分析，如果能对公司有帮助那就好了，是真的很热情的去做这件事。",
      "speaker": "发言人1"
    },
    {
      "time": "00:15:27",
      "text": "这个是只有你还是几乎所有人？",
      "speaker": "发言人2"
    },
    {
      "time": "00:15:30",
      "text": "大部分人就是说公司里并不是说主动强迫说大家都要加班。但是你会发现，比如说在网上看red上也有人在说，怎么CI又登不进去了的时候，我一般会去slack上面看一眼，有没有什么运维的朋友在，或者能不能我自己修。但往往我去看的时候都有十几个人在线。当时公司大概七八十个人，公司里也有对吧？正常就是说有老有小，可能也五十多岁经验非常丰富的那种老工程师。我觉得大家的合作是非常流畅的，没有说谁比谁真的就是主动加班，主动的去帮用户，去帮公司做更多的事情。",
      "speaker": "发言人1"
    },
    {
      "time": "00:16:01",
      "text": "你刚讲的里面有好几个点，我想再问一下。第一就是在美国，尤其是初创的AI公司里面，996是1个常态吗？你觉得。",
      "speaker": "发言人2"
    },
    {
      "time": "00:16:10",
      "text": "严格来说，996这个话有点夸张，周末可能也就刚才我说的那十几个人会主动的跳出来修点东西或者改点代码。平时其实正常上班时间也就九点到晚上五点。",
      "speaker": "发言人1"
    },
    {
      "time": "00:16:21",
      "text": "中间还能吃个一小时的饭。所以还是像大家理解的，就是哪怕是硅谷那边也没有那么卷，对吧？大家还是比较chill的对。",
      "speaker": "发言人2"
    },
    {
      "time": "00:16:29",
      "text": "亲友是一方面，但是刚才我说的那十几个，包括我的人在里面，就真是觉得使命感，或者说在跟很厉害的人做一些很有意思的事。那种个人驱使的996，我觉得比例是不小的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:16:40",
      "text": "明白。然后第二个问题，你刚提的一个点我觉得特别有意思。就是你说你们有一个五十多岁的程序员，这个在国内我觉得几乎是不可能发生的。在美国那边这个是常见的吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:16:52",
      "text": "我觉得这边还蛮常见的，就是我们公司甚至不只是一位五十多岁程序员，有好几位，而且其中一位还跟着去了google。我觉得这边对年并不是那么重要，对吧？更多还是看你本身能提供给公司的价值能到什么程度。然后很多这个年龄的程序员做了这么多年码农，对他来说最开心的事就是在这个年龄也不是去退休，那就得来这儿做些这种事情，就是一种很愉快的体验。",
      "speaker": "发言人1"
    },
    {
      "time": "00:17:15",
      "text": "OK好，那你继续讲，就是你自己会主动的996对吧？你的日常写作跟实际的工作当中是怎么样？",
      "speaker": "发言人2"
    },
    {
      "time": "00:17:22",
      "text": "因为我是在post圈里团队，我选的团队要回答的问题就是朱雀那边给我们一个很棒的大模型。然后我们现在已经有一套还不错的管线在那跑，能够把用户的反馈，能把一些额外的需要微调的数据给模型看一遍，然后出来一个理论上来说对话能力很强的大模型。但是我们要是每天去看discord，看ready，能看到用户还是在表达一些正常的愤怒的情况下，那怎么去让你的模型更为用户喜好呢？然后以及时不时改些东西，你这个AB测试它的流程，它的时长是会上升的那怎么去不停的去迭代模型呢？",
      "speaker": "发言人1"
    },
    {
      "time": "00:17:57",
      "text": "就是我工作的主要目要方式，其实就是看大量的数据，另外一方面是研究数据，那边就是分析最近的几次迭代的效果，怎么样去理解里面可能说模型是应该调数据还是调算法。然后在实际上工程那边就会思考我们现。这个管线里面是不是有些用户数据的使用方式还是不够优秀。或者比如说我们要做偏好对齐的话，DPU这个算法最近有没有什么业界的新的研究，发现它有一些缺陷可以去改善。当然少不了就大量的跟研究员去讨论，看看研究员那边对于最新的业绩的方法有没有什么新的见解。一般可能一天8个小时里面，我觉得真正的在写代码程序实现的里面，大概不会超过2个小时。6个小时基本都是在各种交流，还有分析各种数据。",
      "speaker": "发言人1"
    },
    {
      "time": "00:18:41",
      "text": "明白，然后你主要是负责post street，你这边有什么可以分享的一些best practice也好，know how也好。",
      "speaker": "发言人2"
    },
    {
      "time": "00:18:49",
      "text": "我觉得最后能分享的东西就是说你怎么能最快的把用户反馈带着飞起来，对吧？就是上一代的AI模型，大家都会说有一个数据飞轮，我觉得这一代同样也有个数据飞轮，而且这一代的数据飞轮效应更加强烈。因为大模型本身就是个数据黑洞，就你喂他一堆数据，然后他吐出来一堆数据给你。而且这里面有很强的一个随机性，很大的不可控性。所以你可能在快速迭代的时候，你得不停的去改变这个数据配比。",
      "speaker": "发言人1"
    },
    {
      "time": "00:19:18",
      "text": "换了这个算法之后，我有一个新的模型OK我丢进生产里面AB测试一下，或者有一个简单的评估。但是它其实依然是一个非常不可控的过程。就很多你觉得在某个版本上好用的一些技巧，可能你带进来之后，你的AB测试就跑崩了，内部评估的分数就跑崩了。就属于里面有很多的你必须得在第一线去踩坑。在不同的场景下，不同的用户群，他的效果就很不一样。",
      "speaker": "发言人1"
    },
    {
      "time": "00:19:41",
      "text": "最终能够在我看来最有效帮助到这样一个过程，就是怎么样建立起一个尽可能高效的迭代过程。这个迭代过程可以说是管线非常的robust，所以我有大量的用户，或者我的用户量并不是很大。但是我用的AB测试的工具，能够快速的让我高效的收集到各种模型的小的变化对用户测的的影响。然后我能做一定的分析，积累性能好，或者有人说我在评估上特别努力，对吧？",
      "speaker": "发言人1"
    },
    {
      "time": "00:20:06",
      "text": "我内部做的这个评估集，它非常像真实的用户。我可能说内部圈了一个特殊的模型才能模拟。现在这个用户在说的话，他能用这个模型去跟这个新的模型去对话，然后来告诉你说这个模型是不是会被用户更加喜欢。也有人说我在数据的利用上，我做的特别的高效。只要你今天给我点个赞，可能明天这个模型在跟这个用户在聊的时候，它的效果就会更好。",
      "speaker": "发言人1"
    },
    {
      "time": "00:20:28",
      "text": "我觉得这里面是非常开放式的，理想上来说每一个步骤都做得非常高效。你一点小变化可能隔天就能马上在生产里面，在评估里面都能体现出来他对于这个模型的变化。这样的话作为模型开发，你就少了很多不确定性。但现在的这个现状，大家依然是要面对大量的不确定性。每一个步骤其实它的效率都不是那么高。所以这其实要看你这个团队本身比较擅长的是什么。假设你有很多上一代的丰富的AB测试经验的人的话，那你可能要先想的是你怎么让这个上一代成熟的AB测试的best practice能够在大模型里面使用。",
      "speaker": "发言人1"
    },
    {
      "time": "00:21:03",
      "text": "如果你的团队里面有很多人特别懂数据平台怎么构建的话，那你在数据上面你能做很多很fancy的东西。比如说我不只是收集用户给我点赞点踩，我能收集用户跟机器人的互相的互动，多种方式收集到这样的反馈，然后把这些丰富的反馈丢给模型。我们甚至四条路线都做了很多的尝试，发现每一条路线都有不少滴水的果实。你做一做就能让这个模型的效果或者整个模型迭代的效率提高不少。",
      "speaker": "发言人1"
    },
    {
      "time": "00:21:30",
      "text": "明白杯子合理。你讲的就是说模型的不可控，要用更高频次的测试去迭代。",
      "speaker": "发言人2"
    },
    {
      "time": "00:21:37",
      "text": "对的，然后迭代的过程中，每个人会迭代不同的方向。",
      "speaker": "发言人1"
    },
    {
      "time": "00:21:40",
      "text": "但这里可能有个问题，就是你看我们经常听人讲说这个模型好，聊着聊着觉得变蠢了，或者聊聊怎么样。他可能是一个非常感性的一个感觉，也没有什么具体的指标，具体的一些问题。尤其是我觉得像CI之类的聊天的场景，我不知道你们遇到这类的问题，你该怎么去评估或者改进。",
      "speaker": "发言人2"
    },
    {
      "time": "00:22:01",
      "text": "这个属于如果有哪一家外卖公司说，我们解决这个问题，我也很想学习一下。我们只能保证绝大部分就是说我们会同时看平均值，并且看一些个例。比如说某个模型修改进去了，不论是评估级也好，还是说AB测试也好，平均的用户的对话次数上升了，这大概率是个优良的提升。但一般来说我们在平均值之外，我们会95的百分比看这些少数用户，尤其是那些其实时长下降的用户，他的这个使用画像大概是什么。我们每次在做这种迭代的时候，尤其是我们用户量比较大，对吧？我们都会去分析说对于这些用户来说，这个模型的退步到底是不是一个可以接受的trade off。因为有的时候你会发现在众多用户上面，它的时长下降了。哪怕他在大量的主流用户上他时长上升的话，你就会需要去做一个取舍。",
      "speaker": "发言人1"
    },
    {
      "time": "00:22:48",
      "text": "你可能说今天我这个模型的修改是让它尽可能再更加安全一点，对吧？一般来说一个更加安全的模型对于一部分用户，他就可能觉得没那么好玩了。但是一个很安全的模型，有的时候它会有另外一种特性展现出来。我觉得这算是一种EQ的涌现，它会开始拉扯，他会绕话。比如说你今天想跟他开展一些感情的对话，对吧？假设今天雷店长就说我爱你，你就你也说点我爱你的话，讨好我呗，对吧？",
      "speaker": "发言人1"
    },
    {
      "time": "00:23:15",
      "text": "但一个比较安全的模型，尤其是假设你跟模型说千万不要跟用户开展过于深入的感情交流。那模型它的理解方式可能说OK，我把这话绕起来，他可能会说你为什么觉得我爱你呢？他就开始问问题。然后问问题这个东西其实在外网很多用户都在吐槽，我们的微信特别喜欢说can I ask you a question？大家都已经把他们当成一个梗了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:23:37",
      "text": "但是当模型在问问题的时候，大部分情况下就是因为模型它感知到了他见过大量的对话数据之后，他意识到如果现在不问问题的话，直接回答用户的这个请求。接下来两三句话之后，用户可能聊的东西就会比较糟糕了。就是说不会是我们希望模型去跟用户去进行的这样交流。这个时候模型可以说我直接拒绝回答。但是如果我开始问问题的话，多问几轮，用户一来他会跟我产生更多的对话，对吧？二来可能用户被我问多了，他也忘记了自己本来想要什么。就很成功的以一个更委婉的方式绕开了这个本来可能有一定危险性的对话的方向。这就是我们观察到的某种意义上就是一种涌现。",
      "speaker": "发言人1"
    },
    {
      "time": "00:24:18",
      "text": "明白，我觉得你在CAI做post train基本就代表了全球post train最高水平了，对吧？包括你能接触到的数据、模型等等，都是很有代表性的。所以能不能给大家比较具体的讲一下，boost train它到底对模型起到什么作用呢？它整个流程是怎么样的？你日常都是怎么做这件事情？",
      "speaker": "发言人2"
    },
    {
      "time": "00:24:40",
      "text": "可以，最基础的话post chain就是微调。微调就是指你拿到一个present好的模型，怎么让他去说真的人话。举一个比较极端的例子，一个刚见过几万文字的模型，你要是问他今天是星期几，他大概率接下来会回答你一个问号，对吧？因为他会说我见过的大部分的时候，这个问题后面最后是个问号。但他来跟人类对话肯定不能这样，所以你要给他看少一些量的数据，但这些数据都是正常的。问答不论后面是不是问号，当这个句式是今天是星期几，或者今天布拉听起来像是问句的时候，当模型意识到他该回答问题，而不是接后面这个问题。当然我知道就要快速的disco laim一下。",
      "speaker": "发言人1"
    },
    {
      "time": "00:25:17",
      "text": "现在有一些好的模型的，他在预训练完之后，他已经能学会这个能力了，对吧？这就是数据调整的结果。但方向来说就大差不差，就是你刚预训练完这个模型，它离真正的完成人类的很多正常的问答去做一些任务都是远远不够的。所以你要让他用更加高质量，但是规模更小的数据，让他以多个不同的方式。但其实现在主流的基本就是三种方式，一种是SFT supervise，翻译成理。这个的意思就是你给他看所谓的正确答案，对吧？各种在你这个领域，在你这个场景下的高质量数据让他去记住。",
      "speaker": "发言人1"
    },
    {
      "time": "00:25:49",
      "text": "还有两种就是指偏好对齐，就是他看过这些正确的案子，他说出来话依然可能不是被人类去喜欢你。怎么让人类的这种喜好交给大模型，有一种非常主流open I最先提出来，也是让大家觉得原来可以这么搞的方法就是RLHF。它就是用一个稍微比较复杂的管线去把人类的喜好就交给模型，同时也要做很多数据。但经过RHF之后，模型说出来的话，真的就是人类会期待机器人该说的话。",
      "speaker": "发言人1"
    },
    {
      "time": "00:26:16",
      "text": "同样ROHF它有一个简化版叫DPU。DPU的本质就是说如果ROH太复杂，我用一个简化的办法，也同样的把人类的喜好灌注到模型，基本我们在用的就是SVT加DPU，然后再加一点点的RHF。我认知里面你要的是能把基本就两步，一步就是SFT，一步就是偏好对齐。都数据调的很好，管线调的比较成熟的话，它是可以达到一个非常高的高度的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:26:42",
      "text": "对，能不能给大家再用人话解释一下，说在每一步里面你的输入是什么，做哪些东西，输出了个什么，然后再到下一步该做什么。",
      "speaker": "发言人2"
    },
    {
      "time": "00:26:50",
      "text": "就先说SFT，SFT它跟预训练阶段是一模一样的。因为预训练阶段就是让模型不停的去看大量的文字，可能先看第一个字再看第二个字。然后这个时候第一个字作为上下文，然后这样一直看到比如第100万个字的时候，前面我们都是有一个模型的记忆窗口。可能8000个token对吧？一个token可能说半个字这样。然后让模型不停的去看这样的一个类似文字接龙的过程。",
      "speaker": "发言人1"
    },
    {
      "time": "00:27:13",
      "text": "在这个看的过程中，我们模型本质是一个巨大的矩阵，矩阵里面有大量的数字。当你拥有了一个巨大的矩阵和一堆数字之后，你可以把你的输入变成一组数字，然后跟这个矩阵做一个乘法，然后它能乘出另外一组数字。这组数字就是告诉你接下来的一个词可能是长成什么样。",
      "speaker": "发言人1"
    },
    {
      "time": "00:27:31",
      "text": "通过预训练，你能把整个大矩阵里面每个初始化都是随机的数字变成一组新的数字。这组新的数字理论上来说给他任何的输入的这个矩阵的，它都能给你输出所对应到最有可能的下一个文字，就是所谓的最高效的文字接龙微调的时候，其实基本也是在调整这个矩阵里面可能全部数字，也可能调整部分的数字。这就看你能投入多少资源，以及你使用的微调的方法。",
      "speaker": "发言人1"
    },
    {
      "time": "00:27:55",
      "text": "SFT的过程其实跟预训练基本是差不多的，只是SFT我们用的数据质量会高得多得多。可能你在pressure的时候，你只是给他看。可能互联网上搜集了大量的，有的是正常的对话，有的是科学类的文章。但是在SFT的阶段，你会精心的挑选所谓的人类高质量的对话。",
      "speaker": "发言人1"
    },
    {
      "time": "00:28:13",
      "text": "这个对话是什么？就是A说了某句话，B说了某句话把这段对话全部拼成一句很长的话，然后让模型这样一个字一个字看过去。当你让模型把SFT所有的数据过了完这么一遍之后，它整个矩阵就变化了一遍，对吧？理论上说这个变化之后的结果的矩阵就可以预测。当A说完这句话的时候，逼的下一句话第一个字应该是长成这个样子的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:28:36",
      "text": "可能很多朋友就会马上有一个敏感的点，就说难道我跟机器人说的话他都拿来做训练了吗？这里我想就跟大家澄清一点，完全没有。因为你在训练的时候，你可以让他只去学B说的话，就A说的话只作为他的上下文。当模型扫到A说的那些话的时候，你可以让模型直接跳过，就说不要去学这些东西。从这个角度你就有很大的概率能够杜绝模型去学会这些用户。因为其实用户说的话有很多隐私问题，我们是不能让模型去学的。就SFD阶段差不多就是选最高质量的对话，或者符合你那个场景的对话，让模型去以类似预训练的方式去见一遍。",
      "speaker": "发言人1"
    },
    {
      "time": "00:29:12",
      "text": "这个讲的很清楚。然后再往后。",
      "speaker": "发言人2"
    },
    {
      "time": "00:29:14",
      "text": "对，再往后就是RHF或者DPU。我刚才说的不是所谓的正解，对吧？就高质量对话ABAB这样对话下来，但很多时候其实这个对话是没有太多的症解可言。就是说我今天假设ABAB我们一起聊了50句之后，你说了一句话，我对这句话的感觉就是还行，还挺有意思，觉得薛凯这人靠谱。我对这句话会有一个我自己的喜好的判断。而且你会发现其实在越是这种人类的对话或者比较开放式的领域，这种喜好的存在是远超于所谓的政界的存在。",
      "speaker": "发言人1"
    },
    {
      "time": "00:29:48",
      "text": "那怎么把这种喜好交给到模型？这其实就是open I一开始在提出来的LHF这个管线。LHF的管线的意思从尽可能通俗易懂的方式，就是假设上下文是这1000个字，然后有三四个不同的答案，第一个最好的答案是A然后是B然后是C然后是D你把这个数据去给模型去学，你能训出来一个模型叫奖励模型。这个奖励模型它要做的事情就是当下次看到类似的上下文，以及给他一个答案，他能判断这个答案有多大概率是会被人类喜欢的。因为他见过了很多人类对不同答案的偏好的排名。然后你有这个奖励模型之后，此处就是强化学习的领域。",
      "speaker": "发言人1"
    },
    {
      "time": "00:30:27",
      "text": "强化学习就是指你最好是让一个模型它能跟环境去进行一个交互。这个环境可能就是大量的语料，他能不停的去从环境拿到一些新的语料，自己生成下一句话。但是在强化学习里面最重要的点就是需要一个奖励模型，这个奖励模型就是不停的告诉这个被训的文字模型，你新的生成这句话好不好。这样的话其实你就形成了一个很自然的反馈的循环。",
      "speaker": "发言人1"
    },
    {
      "time": "00:30:50",
      "text": "奖励模型不停的告诉这个文字模型好不好。文字模型通过这个奖励去判断说OK这句话也许可能不是那么好。我来试试换一个方法。然后在这样一个过程中，你可以发现文字模型它就能不停的提升自己。这就是所谓的reinforce learning with human feedback，就是带有人类偏好的强化学习。人类偏好就是奖励模型。Reinforcement其实就是传统的reinforcement难点，可能就是说你怎么去设计一开始的数据来圈一个比较高效的奖励模型。其次其实都是很成熟的RL shift的关键。",
      "speaker": "发言人1"
    },
    {
      "time": "00:31:19",
      "text": "明白，相比之下DPO的方式就是告诉你，咱们也别整那么多，搞这么复杂的管线。假设你现在有的只是一堆用户点赞点踩的数据，把这些数据拼成一个偏好。对，就是说你能给模型同时看，这句话用户喜欢，那句话用户不喜欢。假设你有大量这样的偏好，的话，你能不能把这个作为一个简单的训练模式，让模型直接一遍过。",
      "speaker": "发言人1"
    },
    {
      "time": "00:31:41",
      "text": "就是DPU过程里面是要加载两个模型的，你要训练的模型和一个参考模型就加载完两个模型，然后把大量的偏好对让模型过一遍，让模型直接从这个里面学会该有的对偏好的信息。能不能呢？DPU就是告诉你，你可以做到非常不错，因为它简单快速。",
      "speaker": "发言人1"
    },
    {
      "time": "00:31:57",
      "text": "所以你是觉得说其实DPU是最好用且见效比较快的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:32:02",
      "text": "是吧？对，就它能快速的建立起一个反馈的这个循环。",
      "speaker": "发言人1"
    },
    {
      "time": "00:32:07",
      "text": "Reg和prompt在你们CI内部大概是一个什么情况跟位置？",
      "speaker": "发言人2"
    },
    {
      "time": "00:32:12",
      "text": "我们很坦白的讲，没有做特别多reg那边的尝试。Prom的话，因为我们等于说是每个角色会有一个prom的对吧？其实跟GB store一样，就是说用户在角色上会给我们大量的prom。所以说我们也尝试过自己加一些额外的prom，发现这些往往会跟用户千奇百怪的problem产生一个冲突。所以做过一些小规模的实验之后，也没有在官方对这个模型做太多的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:32:36",
      "text": "所以你们其实还是非常强调模型本身能力。",
      "speaker": "发言人2"
    },
    {
      "time": "00:32:40",
      "text": "对，因为我们这个微调的管线还是蛮复杂的，就像我刚才提到？我们可能主要的是DPU再加上刚才SFT，这两个阶段对数据的要求是非常大的，或者说数据里面能够挖掘出来的天花板是非常高的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:32:53",
      "text": "明白，但这一整个流程我觉得现在大家好像都是共识了，都是说我要做post train，然后我的数据质量很重要？然后我要做DPO还是做什么东西的。比如说同样的事情tok肯定也在做，那他跟你们CAI做的区别是什么呢？不一定非要这两家，就是做的好的跟做的不好的区别是什么呢？",
      "speaker": "发言人2"
    },
    {
      "time": "00:33:15",
      "text": "我觉得这个可能就是有一点非共识，我是觉得你不论是SIT里面找来的高质量对话数据，还是偏好对齐里面找来的偏好数据，他真的是你现在能找到最高质量的偏好数据和对话数据吗？我们现在在CAI都觉得这上面还有很大的空间可以挖。所以说可能很多竞争对手，很多在这方面尝试会觉得，这已经是我能找到最高质量的了，我已经没法再提高。",
      "speaker": "发言人1"
    },
    {
      "time": "00:33:40",
      "text": "当你开始下这种结论的时候，我觉得你可以直接反问一下，真的没有办法让他再提高了吗？这就是你能找到最高质量的在你这个领域能用的数据吗？会不会说其中只有一半是所谓的真正高质量，其他一半你丢掉对模型来说，它的能力甚至能提升。又或者说其他一半你要是通过一些额外的大模型改写重写或者摘要，它质量也会提升的。这个数据本身它的质量其实有相当大的空间。我就会在这里给大家一个建议，就是我觉得SFT里面目前来说大有可为。就是大家会觉得SFT不就给他一堆正解，那我现在想办法找一堆高质量的人类数据是不是就行了呢？",
      "speaker": "发言人1"
    },
    {
      "time": "00:34:17",
      "text": "但其实高质量的人类对话数据，它本身就是一个非常开放式的问题。我们在SFD阶段就是发现这样一个情况。因为我们SFT的时候，我们的人类对话数据它并不是说我们人工挑选出来的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:34:30",
      "text": "我们的挑选方式是说我们会拿用户反馈数据来辅助这个挑选过程，就是在做一个分类。这个分类器数据的来源就是用户给到你的反馈，就用户反馈在不断的提升，尤其用户反馈他可能在暑假的反馈跟在开学的反馈他是不太一样的。下一次SFT的时候，我们用最新的用户反馈数据来指导这个分类过程。这个分类过程在我们的观点来看，它就不停的在提升，他总能找到比上一次挑出来的这些人类对话质量会更高。",
      "speaker": "发言人1"
    },
    {
      "time": "00:35:00",
      "text": "其实我们的SFT是一个非常动态的过程。可能每一次的模型迭代，它的SFT的数据配比都会产生一定程度的变化。它这个变化的根源就是用户的反馈，用户的偏好。它是时刻在变，所以用户的偏好在变的时候，你这个所谓的高质量人类的数据其实也是应该去产生一定程度的变化。不然你这个产品可能迭代三个月。你一开始选的那段高质量的数据，对于你三个月之后进来的这些新的用户，他已经是非常的不匹配了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:35:27",
      "text": "在这里面你把DPU的简单的管线build好之后，你都能通过这里面快速的观察到用户的偏好具体是什么样。然后来指导你自己去做一个更好的SFT的过程。而且在这个过程里面，其实就属于抑制SFT一直爽，这里面其实有大量的潜力可以挖的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:35:43",
      "text": "另外一个在pressure那边很流行的观点就是大家总在说数据快用完了。但是其实互联网上现在在这个阶段，只是通过简单的过滤规则，被丢掉的数据依然有相当多。你先改写现有的这些质量不是那么高的数据，可能比从无到有生成一些新的数据效果要好得多。",
      "speaker": "发言人1"
    },
    {
      "time": "00:36:00",
      "text": "明白别的还有什么吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:36:03",
      "text": "还有一个我可以跟大家分享一下，就是刚才说的EPU点赞点踩。但实际上在我们的这个对齐过程中，我们用的远远不止点赞点踩这么简单的偏好数据。你要想用户在跟你的APP在交流的过程中，他能做很多事情。比如说他可以说编辑，如果用户编辑的这句话，那是不是说他更喜欢编辑之后的这句话？刚才我DPU的时候不是说过，你需要有一个好一个坏才能给模型学会吗？那这样其实编辑后编辑前就是好和坏，对吧？同样的用户他可以说删除这句话，它本身也已经是一个偏好。所以你怎么在产品层面能够取巧的获得更多的用户的偏好的数据，把一部分可以丢进DPU。",
      "speaker": "发言人1"
    },
    {
      "time": "00:36:43",
      "text": "另外一部分刚才我不是说SFT里面大有可为。假设你收集了足够用户删除的数据，你就知道用户是真的不喜欢什么。我刚才分类器他可以说我只挑最好的，同时你可以叠加另外一个分类器。这个分类器要做的事情就是把用户可能不喜欢的东西挑出来，这个时候你就可以把用户删除这个动作转化成训练这个分类器所要使用的数据。",
      "speaker": "发言人1"
    },
    {
      "time": "00:37:04",
      "text": "所以从这个角度，在产品层面我们做的不算特别多。但sofa我们产品上能收集到的一些用户的行为，我们都能很高效的把它转化成对SFD阶段，DPU阶段能有帮助。甚至对一些其他比如说用户推荐非常有帮助的这样一个数据。",
      "speaker": "发言人1"
    },
    {
      "time": "00:37:23",
      "text": "我觉得我在观察其他的竞品，或者在跟其他无论是国内也好，国外也好，在做类似产品，类似场景的人在聊的时候，感觉很多人的一个思维定式就是说大模型这么棒，对吧？那我就想办法收集一个点赞点赞。但我是觉得你要是现在很严肃的想要在2024年下半年再做这个的话，你应该去想想看你在产品上能够怎么更丰富的收集到用户的这种反馈，收集到用户的这种互动。然后把它丢进你这个微调的管线里面。我相信是有大量的低垂的果实，我觉得这是非常重要的，能够产生一定差异化的点。",
      "speaker": "发言人1"
    },
    {
      "time": "00:37:57",
      "text": "明白，所以你觉得核心还是在数据上？",
      "speaker": "发言人2"
    },
    {
      "time": "00:38:00",
      "text": "对，就核心还是在数据它的清洗，它的收集里面，其实有很大的空间可以去做。而且我个人会觉得大家都做的不是很好。",
      "speaker": "发言人1"
    },
    {
      "time": "00:38:07",
      "text": "这件事儿一般是谁来负责做呢？就是数据的时候进行清洗，以及说要定义？什么是好的数据，哪句要哪句不要，这就应该是更多偏产品运营还是技术还是谁。",
      "speaker": "发言人2"
    },
    {
      "time": "00:38:19",
      "text": "我觉得就POS圈里的NG要做。因为实际上是我们在用的这些数据是我们写的代码给模型去建，就像刚才说怎么让模型只看A不看B这都是要实际的代码写出来的。所以我们是最了解数据是怎么具体的被模型消化的人。同样的我们也知道数据长成什么样，它能对模型有最好的效果。但是具体的数据，就比如说我们现在是以女性为主的平台。我们在收集用户的数据的时候，是不是得做性别做一定的标准化，对吧？其实这个他就牵扯到产品了，产品你得先决定你调模型的方向是不是跟现在产品的方向有关，然后才能说那我们就怎么去收集。但我觉得主体还是微调组要去负责的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:38:56",
      "text": "哼刚才讲的是从数据上，然后从技术上，你觉得有什么在做的不一样的地方。",
      "speaker": "发言人2"
    },
    {
      "time": "00:39:01",
      "text": "可能还是多少是有个先发优势。毕竟我们以这个免费的状态持续了很久，吸引到了很多的用户。然后这些用户都有大量的在跟我们进行了交流。某种意义就是规模的暴利。比如说其他公司想做个AB测试，至少动辄了要一周才能拿到足够的数据，我们的规模可能一天就够了。然后在这样一个能够快速迭代的基础上，我们能够快速的实验。这样的话我们积累下来的这样一个know how，其实我们自己也不能完全解释，但是觉得这几个设定就恰恰能够去让用户喜欢。",
      "speaker": "发言人1"
    },
    {
      "time": "00:39:32",
      "text": "有什么是相对能解释的，可以分享的一些东西吗？我觉得现在大家比较流行在讲的几件事情，第一个就是评估，对吧？大家都觉得说要把模型或者说模型相关的产品做好，最重要的是说你要有一个很好的评估体系。这个事儿你们是怎么做的？",
      "speaker": "发言人2"
    },
    {
      "time": "00:39:49",
      "text": "你要是写代码，程序跑一跑就评估了。但在人类对话这个场景下，我们就发现评估是非常难做的。就理想上情况就是你有一个模型，它能模拟用户偏好。如果这样的模型存在的话，那么你只要每次迭代一个新模型，你让这个模型去对可能预先选好的5000句话出一遍答案，然后这个模型打个分OK完了如果他分数上升，这个直接就生产，就不用想什么。",
      "speaker": "发言人1"
    },
    {
      "time": "00:40:12",
      "text": "然而实际上这里面就牵扯到模型能不能模拟人类，对吧？我们给他能够找到的人类的对话数据，让他去学习人类的可能的偏好，离真正需要的数据量缺口太大。所以从这个角度我们很难建立起一个能够模拟用户的具体偏好的模型。我们有些所谓的用户偏好模型，就是这种比较少的数据量训出来的。但这个用户偏好模型现在并不足以强大到能够直接说作为一个内部的评估集。所以我们的评估很多时候还是说，比如说你在SFT阶段不是要选很多所谓的高质量人类对话。那么你可以说把其中一部分预留出来，这部分对话就是用来做测试。你可以让模型去续写这一段，然后看它跟实际上高质量对话本身的内容之间的c cross entropy loss，就是说相似度有多少，这作为一个小的评估。",
      "speaker": "发言人1"
    },
    {
      "time": "00:40:57",
      "text": "然后刚才我说的这个不是那么的准确的，人类偏好的模型，它可以去辅助评估。但就现在而言，我不会说我们内部已经建立起来一套非常完善的评估体系。所以最终我们都是要在生产里面走一遍AB的。明白。",
      "speaker": "发言人1"
    },
    {
      "time": "00:41:10",
      "text": "然后另外一个最近总听的词就是意图识别这件事。刚才讲那个例子我觉得挺有意思的。就是我如果问一个问题，后面没带问号。理论来说，如果它是一个纯概率模型，它应该是回一个问号是概率最高的，就是你怎么样去更好的识别用户的意图，然后去针对性的回复。",
      "speaker": "发言人2"
    },
    {
      "time": "00:41:30",
      "text": "首先第一点，就是怎么让他不回答问号。这个就像我刚才说的，其实他的解答不是特别复杂。你在free的时候你都可以去调整数据，让他意识到不是问号，答案会更好。比如说lama的3.1和3.2，它的base版就已经基本不会回答问题了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:41:46",
      "text": "但是你想要更加深入的，基本就都是要postion才能把这个意图识别做得很好。比如直接我就说两个字，大选。如果是上一代模型做的不好的时候，OK我给你个维基百科对吧？大选是什么？或者我勉强给你摘抄几个新闻。但是一个意图识别好了，他会意识到如果你现在在晚上十点去搜这个新闻，你大概的是想看到现在两边的阵营最新的瓜，它会变成一个搜索行为。",
      "speaker": "发言人1"
    },
    {
      "time": "00:42:08",
      "text": "在文字模型，甚至说在我们cat AI上面，就大量的角色聊天情况下也会有类似这样的问题。比如说你去问一个超级马里奥，你去问他你给我写个hello word，那插件麻料就意识到等一下这小子是今天是在跟我开玩笑，还是说真的要去给他写个hello word出来。如果他想我写个hello word的，但其实里面也有一些看起来很搞笑的那种bug，他是不是写到这种东西，所以我觉得一个很贴近人类喜好的模型，它未必要做的很完美。但它基本上是需要能够理解到人类跟他说的这些话的潜在的意图，然后来判断说他是应该严格的去回答一个比较完美的答案，还是有一种比较随意的对话方式，还是说要触发一些外挂的一些工具来把这件事情做得更好。",
      "speaker": "发言人1"
    },
    {
      "time": "00:42:47",
      "text": "对，但就比如说刚才你说的一个例子，我只打大圈两个字，然后就是通过什么能把它的意图识别清楚的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:42:54",
      "text": "就是要用上下文。假设你现在在这个时间点你打了大型两个字，或许你就会意识到现在这个时间点是一个很特殊的时间点。可能在背后要调用一个搜索引擎的能力，确定一下这个时间点现在发生的跟大学有关的事情。然后把这里搜集来的信息作为一个上下文放给模型。模型会内部有一个用户看不到的这样一个输出的阶段去判。他说接下来用户希望得到的是一个新闻网页，还是一句回答，判断完之后再去做这个实际上的事情。",
      "speaker": "发言人1"
    },
    {
      "time": "00:43:21",
      "text": "明白了，然后正好接着我的下一个问题，就是抛开意图识别，就是在整个的它交互过程当中的成本和延迟是不是其实也是postion决定。",
      "speaker": "发言人2"
    },
    {
      "time": "00:43:32",
      "text": "如果你只是说单纯的文字接龙，延迟跟post圈的关系说实话不是很大，基本你就看你要输入多少字。模型并不会说因为你post炫酷之后，它整个的输入的速度就产生变化，除非你改变它整个模型的架构。成本的推理成本也不会变化，但是它这个训练成本就是你想做的更复杂的SFT管线，你就要收集更多的数据。你一部分得收集用户的偏好，这个成本就没有太多。但是一部分你可能需要一些专业的第三方的标记公司，给你提供一些额外的不同领域的标记数据。这种数据成本其实在portion里面不算是特别少的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:44:06",
      "text": "但我也知道有些公司就是它通过一些工程上的方法来哪怕说是trick，让大家觉得说这个东西的延迟没有那么高对吧？比如说他提前先发一段预制好的话，或者说他把某些话它分成几个模型分别去处理和发出来。我不知道这个是你们见过的比较常用的手段吗？或者有什么类似的例子可以跟大家分享。",
      "speaker": "发言人2"
    },
    {
      "time": "00:44:28",
      "text": "这个我稍微凡尔赛一下，大家不要骂我，我们有糯米写字A我们不需要去考虑这样的事儿。OK它能保证可以说在这个时间点，我们推理延迟依然可能是业界基本是第一这个档次的。所以在这之上我们就文字进去，让模型直接输出就好了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:44:44",
      "text": "但是我们在语音上面需要做一点类似的这样的去向，也就是语音模型基本是有三个模型你绕不开的。一个是ASR模型，你就是判断你说的这句话转成文字，以及判断你这句话有没有说完之后就语言模型，就是说你流量文字语言模型过一遍输出一个该说的话，然后最后你得把它还原成语音。所以你还有一个TTS模型，就text to speech，把文字转换成对话这样一个过程。转换完之后这个对话你为了能够让用户播放出来，你还得赶紧把它存到互联网上某个地方。这一步一步践行的过程，导致延迟是加起来的。可能说在去年最好的这样一套传统的架构，拼起来就一点几秒或者接近2秒的这样一个端到端的延迟，大家就是体验不好，对吧？",
      "speaker": "发言人1"
    },
    {
      "time": "00:45:25",
      "text": "但是如果能够自己去拥有这几个模型的时候，假设你说完第一个字之后，air模型直接把那第一个字丢给语言模型去做一个推理，语言模型直接把那第一个字推理出来，结果丢给后面的ttm模型生成第一个字对应的语音的文件。不停的持续去做这样的事情。我们因为都是自研这些模型，都用我们自己GPU，有很多办法去提高它的使用效率。所以当你真的说完最后一个字，这个时候ASR模型说你就说说完了好，说完这件事就是出发，然后开始下游所有相关，比如说前端的动画或者后端的额外的信息处理全部都开始跑起来。",
      "speaker": "发言人1"
    },
    {
      "time": "00:45:59",
      "text": "这个时间点其实你该生成的文字和后面该生成的回答的语音都已经申请的差不多了。这也是因为我们都是全部托管在自己的云服务器上面，才能做到一个比较高效的预生成。然后就能把延迟可能说在美国地区这边差不多0.5秒之内就能马上回应过去。",
      "speaker": "发言人1"
    },
    {
      "time": "00:46:16",
      "text": "明白，有一个问题我觉得不一定是你们的CAI会接触到的。但是我看现在大家比较普遍的都在用多模型的混合。大家现在有的时候会用最高级的模型去判断一个意图，对吧？因为这可能是比较难的然再用一些其他的模型去做一些生成之类的。我不知道在美国那边，现在大家是怎么看这个问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:46:38",
      "text": "大家都不会在明面上说的，共识就是open I基本就是这么做的对吧？大概率你跟他家的GPT对话的时候，他背后其实是已经有多个版本特化的模型在后面去承接这个任务，对吧？就是模型的路由欧派肯定是这么做的，因为你会发现它在执行不同任务的时候，它不论是延迟还是这个内容质量是有一个明显大的区别。当然如果有我朋友的朋友说，我们其实就一个模型，那我觉得那太棒了。OK但是就这种路由的逻辑其实非常reasonable，就该路由。",
      "speaker": "发言人1"
    },
    {
      "time": "00:47:07",
      "text": "对，所以你就说哪怕是同一个模型里面，其实它也是分了几个不同的模型的对吧？所以你觉得未来大家在类似场景里面多模型混用会是一个比较常规的操作。",
      "speaker": "发言人2"
    },
    {
      "time": "00:47:19",
      "text": "对，除非说哪天GPT6或者cloud 5说我们这一个模型比你们这两个模型做的都好，而且价格还更低，那我觉得何乐而不为，对吧？",
      "speaker": "发言人1"
    },
    {
      "time": "00:47:28",
      "text": "但长期来看，你觉得以你的视角，包括在硅谷的视角，大家对于模型未来的发展是会怎么看？",
      "speaker": "发言人2"
    },
    {
      "time": "00:47:35",
      "text": "在o one出来后面这几个月的时间点里面，就整体还是很乐观的。O一这个证明说模型的迭代还有额外的路线可以走，可以继续对更多的推理式的算力去让它性能提升。然后好像训练的时候也有一些新的这种技巧，新的方法，可以再往里面研究一下。",
      "speaker": "发言人1"
    },
    {
      "time": "00:47:50",
      "text": "我这个时候其实我想讲的就o one延伸出来的一个我其实很积极的看待这件事情的角度。就欧派他自己在博文里的时候，他们欧冠做得很好的，就是有这种明确答案可以验证的过程，写代码、解数学题、解物理题。但是一旦放到写作，它其实跟40基本就是不相上下。你要是再扩展到其他领域，扩展到人类对话，扩展到很多这种没有明确答案的情况下，你要怎么去做？我觉得O一其实某种意义上它是证明了在computer time你可以做很多的优化。你不用天天就只看训练时间的这个computer，你可以在推理的时间computer来让模型产生一些新的能力。",
      "speaker": "发言人1"
    },
    {
      "time": "00:48:29",
      "text": "这里面其实最近在twitter上有一个很有意思的项目，叫做anthropic ENTROPIX。他在做的其实大家可以探索一下，他是github的一个rapper他们在推理的时候做了很多的额外的采样。就在推理的时候模型会生成几个答案，然后采样的时候我选哪个答案。这个过程他们做了很多的动态的数据调整。他说我选答案的时候，其实有大量丰富的信息来辅助我怎么选一个更好的答案。他做了很多的优化，最后做到结果是什么呢？他拉玛一个一逼的模型，能够很好的回答9.1和9.8哪个更大。",
      "speaker": "发言人1"
    },
    {
      "time": "00:49:02",
      "text": "很多人就觉得这是不是有点过拟合，或者是有点magical，是不是骗子。但其实有很多我很认可的推特大V也在很认真的看这个rapper的过程。现在大家把这个技术尝试运用到70B想说这个70B能不能因此有一个非常强大的采样的逻辑，来让他拥有类似思维链的这样思考过程。这是一个很有趣的过程。就open I的方向就是告诉你在推理的时候大有可为，它未必得是单纯的烧GPU。或许几百个答案，也许只是说这个GPU可以用在别的地方，这是一个方向。",
      "speaker": "发言人1"
    },
    {
      "time": "00:49:33",
      "text": "另外一个方向其实还是我刚才提到的，人类对话的思维链到底是什么？一个很简单的例子就是说你跟人打字？你会打了山可能会撤回。其实人类对话里面这种行为天天都在发生。大家都在想我这句话怎么说能让他觉得最有意思？他一直在前进后退，最后决定说这句话大概是比较有意思的那这种思维链有没有可能让模型学会呢？现在你在跟模型去产生这种陪伴对话，产生这种感情对话的时候，模型其实基本就是直树对吧？那有没有可能说让模型学会类似O一的这种，他内部也去想几十个回答，觉得这句对话可能会让该玩家友好度加5，另外一题的话可能会让玩家友好度减2，但是这句话之后的20句话可能好感度会增加100。",
      "speaker": "发言人1"
    },
    {
      "time": "00:50:19",
      "text": "我觉得这是一个非常开放式的问题。我自己现在在尝试做一些探索，探索的结果就是我会发现模型的确会产生一些很奇妙的感觉，这不像直接一次性能出现的回答。所以OpenAI至少他告诉你说这是一个方向，大家值得去研究一下，用RL去辅助模型，去获得更强的回答人类对话的能力，那么大家就去探索呗。",
      "speaker": "发言人1"
    },
    {
      "time": "00:50:45",
      "text": "当下硅谷那边有没有一些非共识或者共识的一些认知，或者特别火的一些产品公司之类的。你们日常在那边经常聊的都是啥？",
      "speaker": "发言人2"
    },
    {
      "time": "00:50:56",
      "text": "尤其最近简单的一句话总结的话，偏产品的AI公司现在基本都在多模态方向，看怎么去赚钱。然后偏算法的公司基本就在想o one到底是怎么串出来的，或者类似的one方式我怎么复制，或者我要不要在推理的时候堆一些额外的技巧让它的效果更好。然后两边的公司有个潜在的共识，就是说类似就google宣布过的12月要发的那个java is对吧？这种交互式的agent可能会在接下一年里面有很大的可能性。",
      "speaker": "发言人1"
    },
    {
      "time": "00:51:28",
      "text": "非常的精炼且准确且有深度。我们捋一下你说的第一个有意思就是现在大家都要做多模态，而且在多模态的同时，你说的是他还是为了要赚钱，对吧？",
      "speaker": "发言人2"
    },
    {
      "time": "00:51:40",
      "text": "对当你不能再讲基础研究，或者说我在等基础研究这个趋势之后，投资人也好，你自己公司也好，你都在想我这个商业模式怎么能够快速的运转起来。哪怕不是说要赚大钱，但是你要证明这个商业。模式不再只是那种卖流量或者烧VC的钱，去让用户去体验一些东西。",
      "speaker": "发言人1"
    },
    {
      "time": "00:51:57",
      "text": "这个点我觉得跟资本市场的好外是相关的对吧？像国内这两年一直在强调这个点，就是因为国内的融资环境是很差的。但美国你觉得也是这样吗？美国你觉得这两年整体的资本市场对AI的热度，尤其是偏一级市场，你觉得是怎么变化的？",
      "speaker": "发言人2"
    },
    {
      "time": "00:52:14",
      "text": "前两年VC会给你很多的信心，对吧？但是也是因为过去两年模型不断的能给人新的惊探。现在这个时间点我觉得这瓶维C还是整体来说信心很充足的。但同时他会希望你能更加脚踏实地，不用再讲特别虚的那种等这个技术等那个技术的缺口，而是希望说你这个模式的确是一开始就比较make .",
      "speaker": "发言人1"
    },
    {
      "time": "00:52:34",
      "text": "sense的OK所以我能不能理解说那边的融资环境在过去的两年里面，也是在逐渐可能更务实也好，或者是变差也好。",
      "speaker": "发言人2"
    },
    {
      "time": "00:52:43",
      "text": "对，肯定是这样。",
      "speaker": "发言人1"
    },
    {
      "time": "00:52:45",
      "text": "OK另外就是你讲的第一条线，它的多模态。当你提到多模态的时候，你想的更多的是什么样？多模态是文字加语音，还是文字加视频，还是都混在一起。",
      "speaker": "发言人2"
    },
    {
      "time": "00:52:56",
      "text": "还是怎么样？我讲的是都混在一起，说简单的就是跟你现在产品的主要模态不一样的。另外一个模态，因为现有的模态下，无论是增长还是商业化好像都见顶了，故事也讲不起来，用户的增长也就停滞在这。所以都会想说我适当的选哪一个模态。可能有的模态就成熟点就图片的模态成熟。但是有的模态给人的想象力更强大，比如说视频模态或者说notebook LM那种。所以在这个时间点选哪一种模态会比较好，能够发挥这个产品。现在有的用户量已经有了这个momentum，然后能够快速切入到一些新的快速的能够获得一些利润的场景。我觉得AI产品现在很多都在考虑这个OK明白。",
      "speaker": "发言人1"
    },
    {
      "time": "00:53:32",
      "text": "所以第一种就是多模态的产品。然后第二种是算法，算法反正就是追赶最领先的模型，对吧？",
      "speaker": "发言人2"
    },
    {
      "time": "00:53:40",
      "text": "对，就研究能不能追，怎么追，然后数据要不要追，然后这条路线现在看起来还有一些可能的果实可以摘一下。明白。",
      "speaker": "发言人1"
    },
    {
      "time": "00:53:48",
      "text": "我知道之前一段时间你也聊了很多硅谷当地的各种初创公司，那你的感受是怎么样？",
      "speaker": "发言人2"
    },
    {
      "time": "00:53:54",
      "text": "就感觉业界现在越来意识到postion对于每一个产品线的重要性，以及postion人才他的技能树会点的很宽。它是左边是钢锤圈好出来的一个勉强能说人话的模型，右边是用户特别喜欢，然后他越喜欢用模型会变得越好的这样一个终极的形态。这中间可能全是POS圈的领域，各种各样的公司来下去发现。不论是模型的研究的也好，或者做一些企业服务这种agent的也好，又或者说还是偏陪伴、偏感情价值或者偏娱乐的也好，都会有对候选管线很大的需求。",
      "speaker": "发言人1"
    },
    {
      "time": "00:54:27",
      "text": "你面试的你最印象深刻的几个公司是什么？",
      "speaker": "发言人2"
    },
    {
      "time": "00:54:31",
      "text": "肯定是科sir。我上来就说我可以用curse r吗？他说可以，那次面试我就很记忆深刻。他给了一个链接，下了一个文件，然后他说这个文件是他们实际上生产在用的相关的逻辑的一个实现。他也告诉我，他故意加了一个bug，导致这个程序运行起来很慢，然后让我去找bug。然后当时就说我试试看。问科。",
      "speaker": "发言人1"
    },
    {
      "time": "00:54:50",
      "text": "你在面试科sir的时候，然后问科sir能不能用科sir是吧？",
      "speaker": "发言人2"
    },
    {
      "time": "00:54:54",
      "text": "对对对，就现在AI公司很多面试的流程就更加强调实战了。大家是真的很喜欢快速的coding，快速反应，快速的让你去使用一定的AI工具，能够短期的写很多代码，短期的分析代码。他甚至允许你直接问科室这个bug可能在哪。但是他会考验说你这个问题问的好不好。他不会给你很多的时间说你就反复去试错。你一开始上来问的第一个问题，就能证明你作为一个大模型从业员本身的素养。",
      "speaker": "发言人1"
    },
    {
      "time": "00:55:21",
      "text": "我觉得现在哪怕是说有一定算法成分在里面，面试对于正解这个过程的追求已经不是那么重要了。但这对于你这个解题的过程，这是你很难让AI去帮你去模拟出来，对吧？你得去表达，你得去分享你的思路，这些都是你的人性所在的部分。",
      "speaker": "发言人1"
    },
    {
      "time": "00:55:38",
      "text": "就现在我要是碰到一个题目会觉得好难，可能三五年前我觉得完蛋了。今天这面试没搞错，我现在其实就挺自信的。我就说那行，那我就来硬着头皮上上看，我会把自己能想到的东西都分享给他。我甚至会说能不能用AI coding，能不能用编程助手。或者如果对方允许我用google搜索，我会直接把我搜索的思路，搜索的过程全部展现给他。我会告诉你这就是我加入你的公司之后我的工作方式。",
      "speaker": "发言人1"
    },
    {
      "time": "00:56:03",
      "text": "你觉得你在跟所有人聊的过程当中，不管是面试被面试，你觉得你听到的最好的几个问题是什么？",
      "speaker": "发言人2"
    },
    {
      "time": "00:56:11",
      "text": "我想看因为我面的很多都是接近founder，所以被问的不多。但让我印象很深就是他会问我，你觉得我们这个idea有多不靠谱？OK, 我还蛮喜欢回答这个的，因为说实话我第一反应都是我是觉得不靠谱对吧？然后就会展开一系列的攻防问答。实际上在这个过程中，比如说我在to c那个领域，最后剩下的这一家，我甚至都觉得有点不礼貌了，天天质疑人家，没事就微信说我觉得你这个东西还是不靠谱。但我在这个过程中，我就越来越觉得，他们看到将来这个vision是真的，我能慢慢的理解，所以其实我很喜欢这一类的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:56:45",
      "text": "如果你是作为面试官去面试post training的人，你最主要会问他的几个。",
      "speaker": "发言人2"
    },
    {
      "time": "00:56:52",
      "text": "问题是什么？可能最想会问的是给他两篇paper，就会提前给他，让他跟我现场讲解一下这篇paper里面可能的问题是什么。我大概率会找两三篇，我们可能就内部试验过，知道这篇paper其实是有一些缺陷的。但我想看这个人有没有可能就只是通过读这篇paper能感觉到，我说哪怕擦到边，我觉得也可以。就是这种对于paper的美感，我身边遇到过的研究员对paper的美感的判断都非常好。他基本不用读特别多，或者说他读完之后一遍，他就能感觉到有哪些地方是不太对劲的，而且是那种直觉性的。然后你之后可能写个代码或者拿点数据。",
      "speaker": "发言人1"
    },
    {
      "time": "00:57:26",
      "text": "才发现好像是真的对的OK。然后最后硅谷相关的一个问题，就是你觉得华人现在在那边到底是一个大概什么样的状况？就是华人做AI.",
      "speaker": "发言人2"
    },
    {
      "time": "00:57:38",
      "text": "我觉得这可能是过去很多年你们华人最好的机会。甚至是说在移动互联网之后，包括在内之内都可能说是华人最好的时机。因为从一些客观的事实来说，open a也好，很多这种大型的模型公司也好，或者很多AI的创业公司里面，你会发现华人比例是极其高的。我会相信既然有这么多的华人愿意去下场，甚至很多在大厂里面待了很多年，对吧？其实完全可以继续在那儿稳固的拿着极高的薪水，就慢慢的退休的人，现在也都下场，都觉得这是一个特别好的机会。",
      "speaker": "发言人1"
    },
    {
      "time": "00:58:09",
      "text": "在这个过程中，我是相信这样的一个趋势，能让更多的华人产生进到更有影响力的地方。比如说在B站做了一个技术分享的翁沥，之前是open I的安全的副总裁。我是相信像这样的华人？在这种核心的圈子，最前沿的地方去持续创业，或者作为企业的高管，能对接下来的华人的发展都会有一个特别大的帮助作用。",
      "speaker": "发言人1"
    },
    {
      "time": "00:58:32",
      "text": "现在很出风头的就在to c这边落地非常成功的AIGC的start up，它都是华人为主的核心。你就不会否认这个事实，就是华人在产品化上面就是有一个很强的能力，完全不输其他族裔的人。所以我是特别喜欢现在我们所处的这个气氛。",
      "speaker": "发言人1"
    },
    {
      "time": "00:58:47",
      "text": "对我在美国去meta的食堂吃饭，然后回头一看基本是百分百中国人，而且基本坐满了不是游客，就真的就是meter员工。那一瞬间我觉得好夸张，感觉回到大学食堂的感觉。",
      "speaker": "发言人2"
    },
    {
      "time": "00:59:00",
      "text": "对我觉得就是时代现在这个时代特别的match到我们华人在硅谷的各种各样的学术上的或者特别努力的这些优势。然后就一拍即合，就导致了大量的华人在这些公司里面都有很好的机会。而且我是相信对于两三年三五年之后，你像OpenAI，可以说每一次回购都有一小批的人就会财富自由。这些人就会慢慢分散到硅谷去创建自己的公司，或者培育新的团队。这些都是将来华人能够在硅谷有自己的立足之地，有更大的影响力非常必要的条件。",
      "speaker": "发言人1"
    },
    {
      "time": "00:59:32",
      "text": "可以这个低一下子就高上去了。OK我最后问一个问题，其实我们全程辽CI的时候，你经常说我们CAI我们C我感觉其实你还是很喜欢CEI这家公司，而且他过去这一年多肯定也带给你很多东西，对吧？对，所以你到底为什么要离开CI呢？",
      "speaker": "发言人2"
    },
    {
      "time": "00:59:50",
      "text": "我相信我在CI积累的东西，能够在一个不同的平台造福更多的人。虽然这话听起来有点大，有点假，但我是真心的觉得我能为全人类做更大更好的贡献。",
      "speaker": "发言人1"
    }
  ],
  "lab_info": {
    "summary": "本次讨论深入剖析了人工智能行业的当前趋势、技术革新及市场策略，特别关注了对话式AI产品面临的商业化挑战，尽管用户基础广泛，但有效的盈利模式尚待探索。强调了数据在优化AI模型中的关键作用，以及通过用户反馈与AB测试持续精进技术的重要性。分享了在硅谷AI公司工作的经历，突出了实战经验的价值，并指出当前AI领域对华人人才的开放与接纳，认为这是华人参与该行业发展的绝佳时机。尽管离开了一家AI企业，但参与者仍秉持利用所学为更广泛人群创造价值的初衷，展现了对AI技术未来潜力与社会贡献的乐观预期。",
    "qa_pairs": [
      {
        "question": "太太，你能给大家简单介绍一下你的经历吗？",
        "answer": "当然可以。我最近刚离开CEI，在那里待了一年时间，主要负责大模型的微调部分。在此之前，我在硅谷的科技大厂如Meta、Google和Apple担任全栈工程师，偏后端工作。在CEI的时候，我有幸参与了一个非常有趣的项目，与研究员们一起把一个不太成熟的对话式大模型调优到业界领先的水平。",
        "time": "00:00:28"
      },
      {
        "question": "大家觉得CAI这款产品商业化没做起来的原因是什么？",
        "answer": "简单概括有两点：一是商业化策略不够，至今为止主要以简单的订阅服务为主；二是公司在商业化方面的探索和尝试相对较少，团队在离开前才开始积极寻求这方面的发展可能性。",
        "time": "00:01:19"
      },
      {
        "question": "美国的超级厉害的产品经理与国内的产品经理有哪些明显的差异吗？",
        "answer": "我接触过的美国超级产品经理和国内的一些顶级产品经理（例如张小龙）有所不同。国内的产品经理可能更注重人性化，总结出很多有效的方法论；而美国的产品经理则在数据分析上非常深入，拥有复杂SQL能力，并乐于通过数据分析来寻找产品改进的关键点。",
        "time": "00:03:28"
      },
      {
        "question": "在你在CAI一年多的 tenure 中，你看到哪些问题导致了公司现在的状况？",
        "answer": "公司长期处于既要打造产品又要追求AGI愿景的矛盾状态，最终结果是无法同时兼顾两边，必须有所取舍。如果重来一遍，我希望公司能更加专注地投入到AGI的研发中，牺牲一定的用户增长，但有可能成为国外为数不多的在AGI领域取得成功的实验室之一。",
        "time": "00:05:02"
      },
      {
        "question": "如果CEI做出一些改变，可能会让它发展的更好，你会建议做什么改变？",
        "answer": "我个人倾向于CEI应该进一步押注AGI方向，入职前的决策应更纯粹地服务于AGI发展，即使这意味着短期内用户增长会有所牺牲。",
        "time": "00:05:23"
      },
      {
        "question": "对于将CEI产品交给字节跳动系来运作的可能性，你怎么看？",
        "answer": "我认为CEI对于母公司而言是唯一的产品，而对于字节跳动（Mini Max）来说，TikTok这样的产品更像是其公司战略的一部分，用来证明其模型的落地能力和海外增长潜力。如果CEI能像TikTok那样得到母公司的全力支持和资源投入，也许会发展得更好。",
        "time": "00:06:19"
      },
      {
        "question": "当时内部有没有分析过TikTok的成功之处，并考虑将其应用到CEI的产品中？",
        "answer": "在产品团队规模较小的时候，我们确实进行过一些推荐算法的调整实验，但发现基于我们庞大的用户数据和健全的模型训练管线，持续迭代模型和优化产品细节的反馈机制更能带来回报，而不是单纯依赖运营或产品层面的调整。",
        "time": "00:07:29"
      },
      {
        "question": "你们总结出TikTok做得特别好的一两点具体的东西吗？",
        "answer": "TikTok功能迭代迅速，能快速推出多模态新功能并积极内购市场，这些都值得我们学习。同时，我们也意识到即使有丰富的功能，如果商业化或增长上未能实现重大突破，这些功能可能并不适合所有产品形态。",
        "time": "00:08:57"
      },
      {
        "question": "现在对于类似CEI的AI陪伴类产品，你还是否相信其商业价值？",
        "answer": "我仍然非常相信这类产品的商业价值。虽然商业化挑战重重，尤其是重度使用的APP如何做好商业化是个难题，但考虑到CEI庞大的月活跃用户量和日活跃用户量，随着用户的增长，未来潜力巨大。",
        "time": "00:10:26"
      },
      {
        "question": "使用CEI产品的主力用户群是什么样的人群？",
        "answer": "主力用户群以年轻女性为主，相对偏重二次元文化，其中大学生和刚上班几年的人群占比较大。",
        "time": "00:11:21"
      },
      {
        "question": "在CI公司，你的日常工作是怎样的，具体负责哪些业务？",
        "answer": "我在CI公司的日常工作可以用“主动996”来形容，作为最早加入初创公司的成员之一，我积极参与到每一个可能对公司有帮助的事情中。虽然公司没有强制加班文化，但大家普遍表现出高度的热情和责任感，主动为用户解决问题，包括我在slack上看到问题时也会主动提供帮助。我的日常工作主要包括分析用户反馈、优化模型表现、迭代管线效率等，涉及大量数据分析、研究讨论以及工程实践。",
        "time": "00:15:11"
      },
      {
        "question": "在美国初创AI公司中，996工作制是一个常态吗？在美国是否常见50多岁的程序员在初创公司工作的情况？",
        "answer": "在美国初创AI公司中，996工作制并不常见，正常工作时间一般是九点到五点，周末可能会有少数人主动加班修代码或解决问题，但整体来说，工作环境相对轻松chill，更看重个人的使命感和与优秀团队共同做有意思事情的热情。在美国，50多岁的程序员非常常见，甚至有好几位这样的资深工程师在我所在的公司工作，并且其中一位还去了Google。公司更看重个人对公司的价值贡献，年龄并不是决定性因素，许多年龄较大的程序员在这个年纪依然能够愉快地从事有挑战性的工作。",
        "time": "00:16:10"
      },
      {
        "question": "在实际工作中，你如何处理用户的反馈并优化模型表现？",
        "answer": "在我的工作中，主要通过大量查看和分析数据来优化模型，包括用户反馈、AB测试结果、模型迭代效果等。我会花大量时间与研究员讨论最新的方法，并根据数据调整模型参数或算法。实际编程实现的时间大概只占每天工作的2小时，其余时间主要用于交流和数据分析。此外，我会努力建立起一个高效的迭代过程，以便快速收集和评估模型变化对用户的影响，从而提高模型开发的确定性。",
        "time": "00:17:22"
      },
      {
        "question": "在模型交流中，为什么模型会在提问而不是直接回答问题？",
        "answer": "模型之所以选择提问，是因为在它感知大量对话数据后发现，如果不问问题直接回答，接下来的对话可能会变得不理想。通过先提问，可以引导用户产生更多有价值的对话，并且在用户被问多了之后，可能忘记了原本想要讨论的内容，从而成功地避免了潜在的危险对话方向。",
        "time": "00:23:37"
      },
      {
        "question": "post train对模型有何作用以及整个流程是怎样的？",
        "answer": "Post train主要是微调，让预训练好的模型更加接近真实应用场景。其流程包括使用高质量但规模较小的数据集，通过SFT supervise（监督学习）、偏好对齐等方式进行微调。其中，SFT类似预训练阶段，但使用高质量对话数据进行更精准的微调；DPU是一种简化版的偏好对齐方法；而RLHF则是利用复杂管线将人类喜好注入模型。",
        "time": "00:25:17"
      },
      {
        "question": "能否用更通俗的语言解释每一步骤的具体内容？",
        "answer": "当然可以。第一步SFT，它与预训练相似，只是在预训练基础上使用高质量对话数据，让模型以类似预训练的方式再训练一次，目的是让模型更好地理解并生成符合人类期待的对话。第二步采用LHF或DPU，其中LHF通过奖励模型来学习人类对对话答案的偏好；DPU则直接利用用户喜好的反馈数据，快速构建反馈循环，让模型从大量用户喜好的偏好中学习。",
        "time": "00:29:48"
      },
      {
        "question": "在CI内部，对prompt和reg的尝试情况如何？",
        "answer": "在CI内部，对于prompt进行了小规模实验，但发现prompt与用户问题可能存在冲突，因此没有在官方模型中做太多尝试。而reg方面，由于整个流程中对数据质量要求高，目前没有特别关注。",
        "time": "00:32:12"
      },
      {
        "question": "整体来看，各家公司在post train上的做法有何异同？",
        "answer": "各家公司在post train上的共识在于重视数据质量和采用DPO或类似方法进行模型微调。然而，在数据挑选和优化上还存在一定的空间，例如在SFT阶段，尽管大家都在寻找最高质量的对话数据，但实际上可能还有提升的空间，比如通过用户反馈数据辅助挑选过程，不断挖掘和提升数据质量。",
        "time": "00:34:30"
      },
      {
        "question": "在SFT模型迭代过程中，为什么需要根据用户的反馈和偏好来调整数据配比？",
        "answer": "因为用户偏好是动态变化的，高质量人类数据如果不随用户偏好的变化进行相应调整，就可能导致产品迭代后无法满足新用户的需求。",
        "time": "00:35:00"
      },
      {
        "question": "DPU的简单管线如何帮助优化SFT过程？",
        "answer": "通过DPU管线，可以快速观察到用户的偏好变化，并据此指导SFT过程，挖掘更多潜力，同时通过改写现有低质量数据，可能比生成新数据效果更好。",
        "time": "00:35:27"
      },
      {
        "question": "在收集用户反馈时，除了点赞点踩外，还有哪些方式可以获取用户偏好数据？",
        "answer": "用户在编辑、删除等操作中也蕴含着偏好信息，例如用户编辑后的内容偏好、删除某句话表示对内容的不喜欢等，这些都可以作为训练模型的数据来源。",
        "time": "00:36:03"
      },
      {
        "question": "如何利用用户删除行为优化SFT流程？",
        "answer": "收集用户删除数据可以了解到用户不喜欢的内容，进而使用分类器筛选出用户可能不喜欢的数据，转化成训练分类器的输入数据。",
        "time": "00:36:43"
      },
      {
        "question": "在产品层面，如何高效利用用户行为数据来改进SFT阶段？",
        "answer": "产品能收集到的一些行为数据，如点赞、编辑、删除等，都能被高效转化为对SFT阶段有益的数据，并对推荐系统等方面产生帮助。",
        "time": "00:37:04"
      },
      {
        "question": "核心问题是否在于数据的清洗和收集？",
        "answer": "是的，核心问题在于数据清洗和收集，这个环节有很大的优化空间，而很多人并没有做好。",
        "time": "00:38:00"
      },
      {
        "question": "数据清洗和定义好坏数据的工作由谁负责？",
        "answer": "这应该是产品运营和技术团队共同负责，因为最了解数据如何被模型消化和期望效果的人是微调组，但具体数据处理则需结合产品方向和标准化需求。",
        "time": "00:38:19"
      },
      {
        "question": "技术上有哪些与众不同的做法？",
        "answer": "我们有一定的先发优势，通过长期免费吸引大量用户，积累了丰富的用户交互数据，能在短时间内完成AB测试并快速迭代，形成了一套内部知识库。",
        "time": "00:39:01"
      },
      {
        "question": "评估体系的建立情况如何？",
        "answer": "目前尚未建立完善的内部评估体系，主要依靠预留高质量对话数据做测试，以及通过模型生成回答与实际对话内容的相似度来辅助评估。",
        "time": "00:40:12"
      },
      {
        "question": "意图识别在模型交互过程中的作用是什么？",
        "answer": "意图识别有助于模型理解用户的潜在意图，从而针对性地提供回复，例如根据上下文判断用户是寻求新闻更新还是进行一般问答。",
        "time": "00:41:10"
      },
      {
        "question": "交互过程中的延迟和成本是否受Postponed（延迟处理）的影响？",
        "answer": "延迟和成本确实与Postponed策略有关，但文字接龙类交互中延迟主要取决于输入字数和模型架构。而对于语音交互，由于涉及到ASR、语言模型和TTS等多个环节，可通过优化流程和利用自研模型来降低延迟。",
        "time": "00:43:32"
      },
      {
        "question": "在美国，大家对于模型的使用和多模型混合的情况是如何看待的？",
        "answer": "在美国，业界普遍采用多模型混合策略，通常会用最高级的模型判断意图，并结合其他模型进行生成等任务。例如，GPT模型在与用户对话时，实际上可能由多个特化模型共同完成任务，根据任务需求和延迟、内容质量等因素进行模型路由。如果出现某个单一模型能够高效且低成本地完成多种任务的情况，那将是非常理想的。",
        "time": "00:46:16"
      },
      {
        "question": "你认为未来在类似场景中，多模型混用是否会成为常规操作？",
        "answer": "是的，除非未来某个模型能在性能、成本等方面超越其他所有模型，并且表现出更好的通用性，否则多模型混用在未来会是一个比较常规的操作。",
        "time": "00:47:07"
      },
      {
        "question": "对于模型未来的发展，硅谷的视角是什么样的？",
        "answer": "在OpenAI推出O一后，整体上对模型迭代和推理算力提升仍然持乐观态度。O一证明了模型可以通过迭代和新的训练技巧实现性能提升，并且在诸如编程、解题等明确答案验证的任务上表现优秀，同时也展示了在没有明确答案的情况下，通过优化推理过程让模型产生新能力的可能性。",
        "time": "00:47:35"
      },
      {
        "question": "多模态具体是指什么？",
        "answer": "多模态是指将除了现有主要产品模态之外的其他模态（如文字、语音、视频等）融入产品中，以寻求新的增长点和商业化机会，因为单一模态的增长和商业化似乎已经到达瓶颈，需要通过多模态创新来突破现状。",
        "time": "00:52:56"
      },
      {
        "question": "近期有没有特别火的产品公司或行业趋势？",
        "answer": "目前硅谷的产品公司大多聚焦于多模态方向，探索如何通过多模态赚钱，而偏算法的公司则关注如何复制O一的成功，或者在推理过程中增加额外技巧以提高效果。同时，交互式agent如Google即将发布的Java is，可能在未来一年内有很大的发展潜力。",
        "time": "00:50:56"
      },
      {
        "question": "如果作为面试官，你最想问应聘者的问题是什么？",
        "answer": "最想问的问题是给定两篇论文，要求应聘者现场讲解其中可能存在的问题。通过这种方式，可以考察应聘者对学术文献的阅读理解能力、直觉判断以及快速解决问题的能力。",
        "time": "00:56:52"
      },
      {
        "question": "你觉得华人在美国硅谷的状况如何？",
        "answer": "目前，华人在美国硅谷处于一个非常好的机会期，尤其是在AI领域，各大公司和创业公司中华人比例极高。许多有经验的华人选择下场创业或担任企业高管，这有助于提升华人在硅谷的影响力和创建自己的公司，为华人社区未来的发展打下坚实基础。",
        "time": "00:57:38"
      }
    ],
    "chapters": [
      {
        "time": "00:00:00",
        "title": "大模型微调专家分享经验",
        "summary": "一位刚离开CEI的大模型微调专家分享了她的职业经历。她在CEI主要负责大模型的微调，之前在硅谷的几家大厂如meta、google、apple担任全栈工程师，偏向后端。在CAI工作期间，她有机会与研究员一起将一个表达不够自然的大模型调整为业界领先的对话式大模型。"
      },
      {
        "time": "00:01:05",
        "title": "探讨CAI产品商业化进展缓慢的原因",
        "summary": "对话中提到，尽管CAI产品在年轻及二次元群体中具有一定的用户基础，但其商业化进展缓慢。主要问题在于产品目前仅提供简单订阅服务，缺乏多样化的商业模式探索。此外，团队在商业化思考方面投入不足，尚未积极探索更多可能性。对比国内AI创业公司普遍将商业化视为重中之重，而CAI及其类似产品在长时间内并未将商业化作为发展重点，这与国内外的融资环境和市场预期有关。"
      },
      {
        "time": "00:02:14",
        "title": "AGI追求路上的团队组成与挑战",
        "summary": "团队初期主要由模型训练人员组成，随着发展逐渐补充产品等相关人员。初期团队规模约25人，主要聚焦于模型训练，而行政、前端后端及运维人员较少。产品团队的建设起步较晚，全公司一度没有专职产品经理，仅有一位兼职高管担任顾问角色。"
      },
      {
        "time": "00:03:18",
        "title": "中美产品经理文化差异与技能侧重点比较",
        "summary": "讨论着重于中美产品经理在文化和技能侧重点上的差异。美国产品经理倾向于深入的数据分析，具备类似数据科学家的复杂SQL能力，且乐于通过数据分析寻找洞见。相比之下，中国产品经理如张小龙等，更注重方法论的总结和人文文化的塑造。此外，还提及了一个美国产品经理的模拟面试，其流程类似管理咨询面试，强调对用户行为变化的数据分析。最后，讨论转向了一个公司从蓬勃发展到最终拆分的过程，暗示了发展过程中遇到的问题。"
      },
      {
        "time": "00:05:01",
        "title": "探讨CI公司战略调整与发展前景",
        "summary": "对话中提出，如果能够重来，CI公司应更加专注于AGI（通用人工智能）的研发，尽管这可能会牺牲一部分用户增长。有观点认为，将公司的主要产品出售以专注于AGI的开发可能是一条可行的出路，但实际操作中可能会遇到政策上的限制。此外，讨论还提到了toki的成功及其背后的字节跳动背景，以及如果CI由字节跳动体系运营可能会有所不同。最终，指出了toki对于mini max不仅仅是单一产品，而是公司战略的一部分，暗示了不同的产品定位和发展策略。"
      },
      {
        "time": "00:06:50",
        "title": "产品团队对CEI和toki平台模式的比较分析",
        "summary": "在讨论中，CEI产品网站给人的初步印象被描述为简陋的UGC平台，而toki则因其重运营和推荐机制而有所不同。有人提出，CEI的平台更有利于UGC内容的自然生成，而toki的重运营模式可能对用户数据行为表现更佳。讨论还提到，尽管团队年初对推荐算法做了调整，但最终认为优化模型和产品细节更能获得用户反馈。团队没有直接复制toki的成功，而是采取中立态度，观察其新功能和商业化尝试，以作为自身产品的参考。"
      },
      {
        "time": "00:09:35",
        "title": "探讨AI陪伴产品CEI的未来及商业化挑战",
        "summary": "对话中讨论了CEI作为AI陪伴产品面临的挑战，特别是商业化路径的不确定性。尽管对AI陪伴产品的未来持乐观态度，但对于如何有效实现商业化、特别是广告形式的选择存在疑问。同时，指出CEI拥有庞大的用户基数，显示出其不可忽视的市场潜力。讨论还触及了类似产品定位的问题，是否更多地被视为互动内容消费而非纯粹的AI陪伴。最后，强调了用户在与AI互动过程中的创造性体验。"
      },
      {
        "time": "00:12:18",
        "title": "CEI在AI对话模型领域的成功因素分析",
        "summary": "CEI在AI对话模型领域取得显著成就的原因包括：首先，具备强大的技术研发能力，能够显著降低模型成本，支持千万级用户规模；其次，自研模型允许精准控制语料库的使用，初期即能提供广泛的人类对话模型；最后，通过后训练和微调的高效管线，快速整合用户反馈和外部数据标注，提升模型性能。尽管面临与其它模型在特定情感陪伴场景中的竞争，CEI凭借模型的广度和对自然人类对话的深入理解，能够提供更加多样化和启发式的对话体验。"
      },
      {
        "time": "00:14:56",
        "title": "初创公司CI的工作日常和文化",
        "summary": "CI公司的工作日常以主动、热情为主，尽管不是强制加班，但员工表现出强烈的主动性和责任感，愿意在必要时加班解决问题。这种文化不仅仅体现在年轻员工身上，连经验丰富的五十多岁程序员也积极参与，体现了公司对年龄的包容和对个人价值的重视。总体上，CI公司虽然在美国初创AI公司中可能不完全遵循996工作制，但员工普遍有使命感，愿意投入时间和精力完成有意义的工作。"
      },
      {
        "time": "00:17:15",
        "title": "优化大模型以提升用户满意度的策略与挑战",
        "summary": "团队在利用大模型处理用户反馈和需求时面临挑战，尽管已有管线能强化模型对话能力，用户仍有不满。探讨了如何根据用户喜好调整模型，AB测试的时长问题，以及模型的持续迭代方法。日常工作重点在于数据分析、研究迭代效果，评估是否需调数据或算法，并与研究员讨论最新方法。实际编程时间不超过2小时，大部分时间用于交流和分析。"
      },
      {
        "time": "00:18:41",
        "title": "大模型迭代与用户反馈的有效利用",
        "summary": "在大模型开发中，由于模型本身的特点，使得数据迭代和用户反馈的快速利用变得尤为重要。大模型像一个数据黑洞，对数据的需求量大且反馈结果的不可控性高，因此需要频繁调整数据配比并进行AB测试来优化模型。此外，高效的数据利用和内部评估集的构建也是提升模型性能的关键。通过精细化的用户反馈收集和分析，以及模型与用户交互的模拟，可以更准确地预测模型改进方向，从而加快迭代过程，减少不确定性。团队需要根据自身特长，如AB测试经验或数据平台构建能力，来优化大模型的迭代流程。"
      },
      {
        "time": "00:21:40",
        "title": "评估和改进聊天模型的策略与挑战",
        "summary": "讨论集中在如何评估和改进聊天模型，特别是在CI等聊天场景下遇到的挑战。提到对于模型性能的评估不仅仅看平均值，还要关注个例，特别是那些对话时长下降的用户。在迭代模型时，需要分析用户量大时模型退步是否可接受。讨论还提到了安全模型可能导致的交流不畅，以及通过问问题绕开潜在危险对话方向的策略。最后，询问了关于post train的具体流程和作用，表明了对如何有效提升聊天模型性能的兴趣。"
      },
      {
        "time": "00:24:40",
        "title": "微调模型以实现更自然的人机对话",
        "summary": "对话内容主要讨论了如何通过微调技术使机器学习模型能够产生更贴近人类正常交流的回答。首先，介绍了模型预训练和微调的基本概念，指出即使模型已经过预训练，仍需要通过微调来适应特定场景或任务。特别强调了微调中数据质量的重要性，使用高质量但规模较小的数据集，通过监督微调（SFT）、偏好对齐以及RLHF和其简化版DPU等方法，来调整模型的参数，使其能够更准确地理解和产生人类语言。此外，讨论还涉及了数据隐私问题，如何在不侵犯用户隐私的前提下进行模型训练。整个对话揭示了通过精细调整模型参数和使用高质量数据，可以显著提高模型在人机对话中的表现。"
      },
      {
        "time": "00:29:12",
        "title": "强化学习与人类反馈在语言模型中的应用",
        "summary": "对话详细讨论了利用强化学习和人类反馈（RLHF）来提升语言模型性能的方法。首先介绍了LHF管线，通过奖励模型判断答案被人类喜欢的概率，从而优化模型生成。进一步探讨了利用用户反馈数据来指导模型训练的重要性，强调了数据质量对于模型性能的影响，并提出了通过改进数据筛选和预处理过程来提升模型效果的观点。此外，讨论了DPU方法的实用性和效率，以及SFT阶段中数据配比的动态变化对模型迭代的影响。整体上，强调了理解用户偏好、优化数据质量和采用有效训练策略对于构建高质量对话模型的重要性。"
      },
      {
        "time": "00:35:59",
        "title": "利用用户行为数据优化模型对齐",
        "summary": "在提升模型对齐效果的过程中，除了点赞点踩等简单偏好数据，还应充分利用用户与APP交互时的多种行为数据。例如，用户对某句话的编辑与删除行为可以反映出其喜好与偏好，这些数据可以用于微调模型（DPU）和监督微调（SFT）阶段，帮助模型更准确地理解用户偏好。特别强调，通过收集用户不喜欢的数据，可以训练模型筛选出用户可能不喜欢的内容。此策略不仅能优化模型性能，还能在产品层面产生差异化优势。在实际操作中，数据的收集、清洗和定义应该由熟悉模型数据需求的技术人员和产品经理共同负责，确保数据质量及其对模型训练的贡献。"
      },
      {
        "time": "00:38:55",
        "title": "利用大数据和AI技术优化用户交互体验",
        "summary": "讨论重点在于如何利用公司的数据和技术优势，特别是免费服务吸引的大量用户数据，来快速迭代和优化产品。通过与用户的大量交流，积累了对用户偏好的深入理解，但同时也面临评估用户偏好模型的挑战。尽管建立了基于cross entropy loss的初步评估方法，但承认内部尚未形成非常完善的评估体系，强调最终仍需通过实际生产环境的AB测试来验证。此外，意图识别被提到作为提高交互质量的关键，通过理解上下文和调用外部数据源，能够更精准地回应用户需求，展现了在技术和数据应用上的深入探索。"
      },
      {
        "time": "00:43:21",
        "title": "优化交互式AI模型的延迟与成本",
        "summary": "讨论集中在如何在不牺牲意图识别准确性的前提下，减少AI交互过程中的延迟和成本。首先指出，单纯的文字交互不会显著影响延迟或推理成本，但训练更复杂的模型需要收集大量数据，这会增加成本。对于提高效率，提出通过技术手段如预制响应或分模型处理来降低感知延迟。特别在语音交互方面，通过整合ASR（语音识别）、语言模型和TTS（文本转语音）模型，并优化GPU使用和云服务器托管，实现了显著的延迟减少，提升用户体验。"
      },
      {
        "time": "00:46:15",
        "title": "多模型混合使用与AI未来发展探讨",
        "summary": "在当前AI技术发展背景下，普遍采用多模型混合方式来应对不同任务，特别是对于复杂意图识别和内容生成等任务。这种策略背后的逻辑在于利用不同模型的优势，以提高执行任务时的效率和质量。对话中提及了OpenAI的实践，暗示其通过多个特化模型来处理不同任务，以此提升模型的性能。未来，随着技术的进步，可能会出现单一模型能够超越多模型混合性能的情况，但目前多模型混用仍是一个合理的策略。此外，探讨了AI模型在推理阶段的优化空间，例如通过额外的采样和数据调整来改善模型的回答质量，以及让模型学会人类对话中的思维链过程，进一步提升其在开放式问题和人类互动中的表现。整体上，对话表达了对未来AI模型迭代和应用技术发展的乐观态度。"
      },
      {
        "time": "00:50:44",
        "title": "硅谷AI行业的现状与趋势",
        "summary": "在硅谷，AI公司当前主要聚焦于两个方向：一是多模态产品的开发，特别是如何结合不同模态（如文字、语音、视频等）以实现商业变现，表明了投资者和公司本身对于快速有效的商业模式的追求；二是算法研究，重点在于追赶最先进模型，并探索如何通过技术优化提升效果。同时，业界有一个共识，即交互式智能代理在未来一年内可能有较大发展。这些趋势反映出资本市场对AI行业的态度正在变得更加务实，期待技术能直接转化为实际应用和盈利。"
      },
      {
        "time": "00:53:48",
        "title": "硅谷初创公司面试经历及对岗位技能的看法",
        "summary": "对话者分享了在硅谷初创公司的面试经历，强调了岗位（特别是大模型从业员）对于产品线的重要性，以及对岗位候选人广泛技能树的需求。讨论包括面试过程中的实战性增强、对候选人问题解决过程的关注超过了对正解的追求，以及通过具体问题评估候选人的技能和思路。此外，还提到了对AI工具的利用、快速反应能力和代码分析能力的重要性。最后，探讨了面试中如何通过提出问题来测试候选人的素质和技能。"
      },
      {
        "time": "00:57:26",
        "title": "华人工程师在硅谷AI领域的影响力",
        "summary": "目前硅谷地区，华人在AI行业中的比例极高，特别是在大型模型公司和AI创业公司中。这一趋势不仅反映了华人对AI领域的热情和才能，也预示着未来几年内，随着更多华人工程师的参与和创新，将在硅谷产生更大的影响力。特别是在产品化方面，华人的能力不输其他族裔，为硅谷的华人社区带来了前所未有的发展机遇。同时，一些在大公司积累了丰富经验的华人工程师，开始创建自己的公司或加入创业团队，这将进一步促进华人在硅谷的影响力和贡献。"
      }
    ],
    "mindmap": {
      "children": [
        {
          "children": [
            {
              "children": [],
              "content": "最近离开CEI，之前在CEI负责大模型的post训练"
            },
            {
              "children": [],
              "content": "之前在硅谷各大厂（Meta, Google, Apple）担任全栈工程师"
            },
            {
              "children": [],
              "content": "在CAI的经历为加入微调团队，与研究员一起优化大模型"
            }
          ],
          "content": "个人经历与工作背景"
        },
        {
          "children": [
            {
              "children": [],
              "content": "用户群类似Roblox，年轻、二次元群体"
            },
            {
              "children": [],
              "content": "商业化尝试不多，主要依赖订阅"
            },
            {
              "children": [],
              "content": "产品基于大模型，注重对话式交互"
            },
            {
              "children": [],
              "content": "研究团队主要关注模型训练与优化"
            }
          ],
          "content": "CAI公司与产品"
        },
        {
          "children": [
            {
              "children": [],
              "content": "强调数据质量的重要性，包括用户反馈和高质量对话数据"
            },
            {
              "children": [],
              "content": "探索模型偏好对齐和数据清洗方法"
            },
            {
              "children": [],
              "content": "使用不同技术（SFT, DPU, RLHF）优化模型性能"
            },
            {
              "children": [],
              "content": "面临的挑战包括模型的不可控性和用户数据的隐私问题"
            }
          ],
          "content": "技术细节与挑战"
        },
        {
          "children": [
            {
              "children": [],
              "content": "多模态产品与服务是趋势，探索不同模态的结合"
            },
            {
              "children": [],
              "content": "硅谷华人AI创业者比例高，展现了华人的产品化能力"
            },
            {
              "children": [],
              "content": "融资环境对AI的重视，但更注重实际应用和商业化能力"
            },
            {
              "children": [],
              "content": "对OpenAI等前沿技术的关注，特别是在模型迭代和优化方面"
            }
          ],
          "content": "行业趋势与硅谷环境"
        },
        {
          "children": [
            {
              "children": [],
              "content": "对AI领域的未来持乐观态度，认为有巨大潜力"
            },
            {
              "children": [],
              "content": "认为个人与团队在AI领域的贡献具有重要意义"
            },
            {
              "children": [],
              "content": "离开CAI是为了在不同的平台上做出更大贡献"
            }
          ],
          "content": "个人看法与展望"
        },
        {
          "children": [
            {
              "children": [],
              "content": "通过在CAI的经历，深刻理解了模型训练、优化以及商业化的重要性"
            },
            {
              "children": [],
              "content": "认识到个人在AI领域的潜力和责任"
            },
            {
              "children": [],
              "content": "未来计划探索更多AI领域的可能性，为社会带来更大影响"
            }
          ],
          "content": "个人成长与反思"
        }
      ],
      "content": "对话脑图摘要"
    }
  }
}