{
  "pid": "61cbaac48bb4cd867fcabe22",
  "eid": "666274a763c334a2fbf008ce",
  "title": "EP 54. 深度对谈顶尖AI开源项目：大模型开源生态, Agent 与中国力量",
  "task_id": "kvjony7mrokynlx3",
  "transcription": [
    {
      "time": "00:00:03",
      "text": "欢迎来到onboard，真实的一线经验，走心的投资思考。我是Monica.",
      "speaker": "发言人1"
    },
    {
      "time": "00:00:09",
      "text": "我是高宁，我们一起聊聊软件如何改变世界。",
      "speaker": "发言人3"
    },
    {
      "time": "00:00:15",
      "text": "大家好，欢迎来到on board，我是Monica。先跟大家汇报一下，上周日我们在北京举办了on board第一次线下听友会，我们真是被听友们的热情感动到了。大周末的从上午九点到下午三点，从机器人到AI创业投资到软件出海，真的是干货满满。一百多个人的场地座无虚席，直到最后一刻大家都还不忍离开，真是太感谢了。我们正在努力整理精华文字稿，也敬请期待我们更多的活动。好，回到这一期的播客，也真的是非常应景。因为就在今天凌晨阿里最新的通义千问模型queen 2也发布开源了72B的开源模型，表现也是相当的可圈可点，大家赶紧去关注一波。",
      "speaker": "发言人1"
    },
    {
      "time": "00:01:06",
      "text": "今天的话题就是大模型的开源生态。生成式AI的发展突飞猛进的这一年多的时间里，开源真的是不可忽视的一个话题。一方面开源模型能力发展迅猛，从meta lama 3到mr的开源模型，他们对于GPT4这样的闭源大模型能力的赶超实在让人惊艳，也极大加速了很多AI产品的落地。除了开源模型本身，围绕大模型的生态，从推理、加速、开发工具到agent等等整个技术站的丰富，虽然已经诞生了像lang chain这样的现在阶段性的头部公司，似乎一切都还只是开了个头。这里边还有一股不可忽视的力量，就是来自中国的开源模型。随着阿里的通义千问系列，还有换方的deep sick，还有灵异万物的E等中国团队主导的模型在国际舞台上展露光芒，莫妮卡觉得在谈论中国大模型的发展的时候，我们不可以忽视这些国人的工作。不仅仅是魔改和追赶，也有很多值得我们骄傲的地方。不管怎样，对于关注生成式AI发展的每个人，开源都是一个绝不能低估的话题。",
      "speaker": "发言人1"
    },
    {
      "time": "00:02:19",
      "text": "今天莫妮卡请来的几位嘉宾都非常有代表性。首先是返场嘉宾，来自hugger face的王铁镇说他是连接中国和世界开源AI生态的关键人物，真的是不过分。从google TensorFlow的时代到hanging face早期员工铁证，在国际视角中对整个AI开源生态以及中国力量的观察可以说是相当的深入。还有来自通义千问团队的俊扬，俊扬不仅是queen在全球开源社区里的主要发声人，见证了开源一路的摸爬滚打。它也是现在最受关注的agent开源项目之一，open的核心团队成员。如果你听过上一期的on board对AI程序员和agent的讨论，那open devin这个名字应该会很有印象了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:03:10",
      "text": "最后一位更是学术界的大牛，他所负责的项目绝对是大名鼎鼎。这就是已经成为行业标准的大模型推理框架BLM的作者卓涵。李卓涵是伯克利大学的PHD，他所在的skylab可以说是开源a info的黄埔军校。从百亿美金的data bricks到10亿美金的any skill，也就是开源计算框架ray的商业化公司。除了VLM，李卓涵还参与了像chat arena、VQA等一系列国际知名的开源项目。它从大模型周边生态和info的国际视角，更是有很多非常有技术理想的干货。",
      "speaker": "发言人1"
    },
    {
      "time": "00:03:49",
      "text": "我们的缘起是要谈论开源大模型生态，没想到嘉宾们都太宝藏了。我们的话题延伸到了agent、开发工具、info等等跟大模型有关的各个方面。原始录制在近4个小时，前半部分我们通过辽BLM以及open devo为代表的软件开发agent背后的技术和生态等话题。下半部分我们回到大模型开源的主题，畅谈了底层基础大模型开源闭源生态未来可能演进，开源模型商业化，跟我们过去在大数据时代看到的data bricks之类的开源商业模式会有哪些异同。最后非常一线务实的讨论，如何做一个有国际影响力的开源项目，当然还有关于数据评测等等，真的是非常全面又不失一线从业者深度的探讨，相信你听起来会非常的过瘾。索性这一次我们就不分成两部分了，大家可以对着sonos里边的时间戳直接跳转到你感兴趣的话题，虽然莫妮卡觉得每个话题都很好，都很值得去倾听。",
      "speaker": "发言人1"
    },
    {
      "time": "00:04:57",
      "text": "介绍了这么多，我还要声明一下，节目里面重点聊到的开源社区哈根face，还有几个开源项目，比如阿里千问open devin 01，万物的EVLM等等。我们都没有收取任何的广告费，完全是嘉宾的走心分享，真的是全程无广。当然如果你们或者其他AI公司考虑赞助一下我们用爱发电的on board，我们也是非常欢迎的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:05:23",
      "text": "好，三个多小时的硬核马拉松就开始了，大家enjoy。欢迎来到欧也，欢迎几位嘉宾来到onboard。按照惯例，一开始还是让几位做一个自我介绍，介绍一下你自己的这个经历，怎么开始进入到这个AI还有of source就是开源的这样一个有工作的。开源领域总是涌现出非常多有意思的这工作，所以也让大家这个方法也分享一下。你们最近看到了一个你们觉得很有意思的在AI领域的一个开源的一个项目，或者说公司都可以。我们就从这个铁证开始，铁证是我们的返场嘉宾。",
      "speaker": "发言人1"
    },
    {
      "time": "00:06:00",
      "text": "大家好，感谢莫妮卡。记得上次我们聊的非常开心，然后可能都有一年多了，其中感觉也这个行业也发生了翻天覆地的变化。自我介绍的话，我想我叫王铁震，是high face的工程师。我主要的工作也是希望跟国内的开源社区一起合作，然后共同去推动这个开源行业的发展，然后帮大家在国际上获得更多的影响力。",
      "speaker": "发言人4"
    },
    {
      "time": "00:06:25",
      "text": "我个人的经历，之前我是在谷歌做这个tensor for。就是当年1117年18年的时候，谷歌做了一个开源的框架。大家可以把它想象成谷歌版本的py toch。然后后面因为公司内部组织结构的一些影响，然后就没有办法做开源。但我实际上是非常喜欢开源的，所以就跳到了一个我自己比较欣赏的公司，就是hanging face。",
      "speaker": "发言人4"
    },
    {
      "time": "00:06:46",
      "text": "我们一直是希望通过这个开源的力量，让AI的这个能力普惠到每一个人。我最近其实看到很多非常有意思的项目，我觉得我最近看那个安卓capacity的的twitter的时候，他有一条推特特别打动我就是我们现在在研究所有的这些AIAI当然可以给我们作为提供一些情绪化的价值，然后做做一些陪伴什么的。但其实它有一个非常重要的目的，就是说当我们仰望星空的时候，我们要做星际的探索。那其实光靠人的这个能力是非常受限制的。我们有这个宇宙飞船，大家都要就是大部分的人类的屈体都要冷冻冬眠。但是我们可以让积极性来控制我们在这个漫长的星际旅行中保持航向。这时候其实我们是需要让大模型在一个非常稳定的环境下能够持续运转并且不出问题的。",
      "speaker": "发言人4"
    },
    {
      "time": "00:07:39",
      "text": "现在我们的所有的这个代码，其实我感觉就是很难保证这个要求。我们经常遇到就是说这个线程死死掉了。那我们的办法也很简单，我们就control c把它关掉，然后重新开一个就行。这个里面有很多原因，比如说他可能是因为python，或者说他的代码写的不是工业级质量。那android assy他做他在做一个项目叫然后让点C那他的目标就是说他用这种茜写嵌入式系统的这种方法来去写这个大语言模型的这些能力包甚至包括训练的一些能力。",
      "speaker": "发言人4"
    },
    {
      "time": "00:08:10",
      "text": "把所有的比如说动态内存的调度，把所有的什么一些动态的逻辑控制，都限定在某一个范围内，让这个模型能够持续的不停的运转下去。这样就是我们可以把它部署在很多很关键的，比如说生命维持系统，宇宇航，或者是我们的无人驾驶的车里面。我觉得这种项目就特别有意思。因为传统上我们一般讲这大概模型是非常researching就是研究项的一些项目。大家他主要的目的是让它能跑出来，对我们对它的最好的场景非常感兴趣。但是随着这个技术的发展，实际上当我们真正把大语言模型应用到每个人的生活的时候，我们要关注的是它最差的场景它不能崩塌。他也能够提供一些非常重要的价值，我就是对这个项目特别感兴趣我，觉得这个其实是一个标志性的，这个项目其实就是说我们已经把大模大宇宙模型从实验室安全走出来，然后目标是让它走向星辰大海。",
      "speaker": "发言人4"
    },
    {
      "time": "00:09:05",
      "text": "刚才非常感谢铁证的这个分享，每次铁证分享的这个项目，我觉得都非常让耳目一新。的确这个angle passes离开了OpenAI之后，感觉他更忙了，就天天不停的干视频，干代码。所以他的twitter和youtube都强烈的建议大家去去去关注。而且铁证刚才也说的非常的谦虚。",
      "speaker": "发言人1"
    },
    {
      "time": "00:09:27",
      "text": "作为应该算是having face在国内为数不多的员工之一，对吧？我整个开源社区里面真的做的非常多的贡献。包括后面我们也会提到，其实铁证也在这个国际社区里面分享了很多。我觉得大家可能在整个国际社会上，大家没有注意到的一些中国的开源的发展。这对于整个行业我觉得作用非常大。",
      "speaker": "发言人1"
    },
    {
      "time": "00:09:48",
      "text": "写真你是哪一年来来哈根face的？我想应该也还比较早期。觉得哈根face这家公司作为一家开源公司，你觉得当时为什么会吸引到你？",
      "speaker": "发言人1"
    },
    {
      "time": "00:09:58",
      "text": "我其实很早就开始关注汉英face，从他们最早的几波融资的时候，因为我们当时在谷歌内部做一个跟行业face非常类似的这种项目，但是做的思维可能是有点不一样。谷歌是希望说尽量用一个平台来展示他自己的模型。Hanging face作为一个中立的第三方，其实他希望所有人在哈根face来发模型。他想做这个中立这一点，它支持所有的framework，就是不管是pyto CH，tensor floor还有CICKR等等这些框架的模型，他都希望支持。他还是为这些开发者提供了很多基础的服务。比如说现在大家觉得说，我跑我跳我跑这个大模型，我其实就应该很简单，我就应该告诉他我这个模型叫什么。然后import一下这个模型，然后会给他一些输入，它就有一个输出，这应该就是一些标准化的接口。",
      "speaker": "发言人4"
    },
    {
      "time": "00:10:42",
      "text": "其实当年并不是这样，当年其实你要找一篇paper，你要找它对应的代码，然后你要克隆下来，然后装它的所有dependency。可能你这一套都弄完，然后话已经两三天过去了。然后你再把这个代码跑，不是你想要，你再去尝试，实际上当年是比较痛苦的，所以还用face，当年做了transformers这些这库之后，其实蛮受这个社区的欢迎。我也是那个时候就开始关注海宁face在做的一些工作。然后我实际加入的时候是22年底，然后也是感觉比较幸运，就是我一加入了OpenAI就发了ChatGPT。所以就感受到整个一个生成式AI的这个呃洪流就跟着往前走，还蛮开心的。",
      "speaker": "发言人4"
    },
    {
      "time": "00:11:20",
      "text": "可能大家都把这个hyden face提供了便利，这个当做理所当然了。当年的确是我觉得我觉得是一个一个开创者。好，谢谢铁证。好，接下来我们就听听俊阳给大家介绍一下你自己。",
      "speaker": "发言人1"
    },
    {
      "time": "00:11:34",
      "text": "Hello大家好，非常感谢莫妮卡的邀请。我叫林峻阳，我是通义千问团队的一名算法工程师，主要做大语言的模型和多模态大模型的一些研究，并且现在是负责开源相关的事情。然后现在大家都看到快这个系列，然后应该听众应该有像我们的群友，所以应该在群里可能会认识我。然后我个人的经历的话，其实我比较幸运。我是20年的时候其实开始做大模型相关的事情，当时我们其实就开始做百亿网上参数的模型，然后一直做到现在。现在的话基本上就all in one这一个系列。",
      "speaker": "发言人3"
    },
    {
      "time": "00:12:17",
      "text": "Queen的话其实包括大语言模型，大家比较熟悉。另外还有BL的模型，就是视觉语言的模型，最近看的比较有趣的一些开源项目open UI，我觉得其实还有点意思。因为做这个open demo的原因，对一些open开头的一些项目其实都有些兴趣。然后open UI的话其实是WNB那边做的一个项目，你可以通过自然语言的话去帮你给一些UI相关的东西。当然他这个东西其实还做的还比较早期。但是因为认识造B的人，之前他们做播客节目也有些相关的一些介绍，我觉得他们其实还是挺有想法的。然后有时候比如说我自己想写个什么UI啥的，但是我自己不太会。我觉得如果真的有这种东西做的比较好，比较稳定化，我甚至可能会购买这样的产品。",
      "speaker": "发言人3"
    },
    {
      "time": "00:13:10",
      "text": "其实刚才俊阳也提到，除了这个通义千问之外，居然也是非常也是一另外一个我觉得非常有意思的项目。其实也是我们前两期里面有提到，就是这个AI程序员这个SVH里面非常令人瞩目一个项目叫open devin的核心贡献者。也可以来跟大家简单介绍一下open devin OK。",
      "speaker": "发言人1"
    },
    {
      "time": "00:13:30",
      "text": "Open deven的话，其实这个项目的发起还是有一些幸运的一些成分的。突然间就火，因为那一天的话其实是devon刚发布，其实他那个demo的话确实非常的impressive。然后那天刚好跟我的同事在聊天，我们说我们要不先发起一个open demo的一个项目。",
      "speaker": "发言人3"
    },
    {
      "time": "00:13:53",
      "text": "我刚开始的初衷的话其实是因为我是做快的原因。我其实想的是说如果我们将来有demo这样的东西的话，我们其实希望背后的模型不是闭源的模型，比如说像PPT four，而是能够用local的large language model去实现这一些功能，这个是比较原始的初衷但是在开源社区发起之后，其实就有很多的人参与进来。这个项目就发展壮大的速度就非常的快。工程相关的东西的话，就有很多非常热血的开源的同学就把它给建设起来。现在的话其实我们基本上像UI还有一些基本的功能的话都是比较齐全。然后现在的话我们也有相应的agent在sweet dash like上面的话。",
      "speaker": "发言人3"
    },
    {
      "time": "00:14:44",
      "text": "最新的昨天晚上的话，我们其实达到了25%的通过率。其实效果暂时来看的话还可以。我们接下来下一步的话，其实还是让他能够在真实的场景当中，解决我们日常真的会应用到一些问题，而不是去只是去测一些benchmark。所以接下来也敬请大家期待我们后面的发展，就是现在这个项目还比较早期。",
      "speaker": "发言人3"
    },
    {
      "time": "00:15:09",
      "text": "在open source里边，在整个stack里面，各个层级的其实都可以去开源。应该算是David acx应用层的一个一一个开源，跟今天我们聊到的LM和这个info的开源都会有些不一样。好，谢谢俊阳。",
      "speaker": "发言人1"
    },
    {
      "time": "00:15:24",
      "text": "想问一下俊想问俊阳一个问题，就是千问的英文名字已经正式是困了是吗？因为我一开始听到困的时候，我还以为大家在困的那个词，所以现在其实已经定下来了，不是去问是是就是就困了是吧？",
      "speaker": "发言人4"
    },
    {
      "time": "00:15:39",
      "text": "最后卓涵。",
      "speaker": "发言人1"
    },
    {
      "time": "00:15:41",
      "text": "感谢莫妮卡邀请。大家好，我是李卓翰，然后目前是加州大学伯克利分校的学生，我研究的领域是机器学习系统。在这个领域我们通过系统优化来提高机器学习算法和模型的执行速度。以及我们会研究如何做更大规模，更加高效的并行执行等等一系列的问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:16:02",
      "text": "在过去一年半的时间里面，我和我的实验室的小伙伴们一起创建了一个叫做VLM的开源项目。VLM是一个大语言模型的推理和部署的引擎。然后VRM包括了一系列的优化技术。比如我们首先提出的配置attention算法，以及像是其他的比方说continue batching CUDA graph模型、量化模型、并行previous catching spectate decoding等等一系列的技术。并且也包括很多我们为推理costumes一些扩大kernels。然后这些技术一起提高了大语言模型的推理速度和吞吐量，使得我们和直接使用hugin face上的模型做推理相比，能有大概1到2个数量级的吞吐量的提升。然后除此之外，VRM和hugin face社区也是紧密集成的。我们支持一一键部署hugging face上的多数主流的模型，就比方说像lama mr以及之前我们提到的千问等一系列的模型。",
      "speaker": "发言人2"
    },
    {
      "time": "00:17:01",
      "text": "然后在做这个VIM开源项目的过程中，最让我兴奋的事情是很多人以及很多公司都有使用我们的项目来部署自己的大模型。然后作为一个开源项目，其实想要具体知道谁用了我们的项目不是一件特别容易的事。但是也有很多公司来主动联系到我们，来反映的一些他们使用到的问题。或是给我们提交一些pull request，来加入一些新的feature。然后我们所知的各种云厂商，比如说AWS sage maker，google cloud vertex AI service以及oracle cloud上的LM service背后都是用了VLM，以及我们在微软azure云上也是默认的推理引擎。然后我们在比方说像苹果data breaks，IBM link robo ks snowflakes等等一系列公司都有部署。",
      "speaker": "发言人2"
    },
    {
      "time": "00:17:52",
      "text": "对，刚刚主持人也问到了最近关注的开源项目。然后最近我特别关注的一个开源项目是open David，也正好今天君阳在这里，我关注open devin的主要原因是随着VRM在推理优化方面的持续进步，我们认为这种general针对所有task的推理优化会变得越来越少。所以我们觉得有必要进一步利用来自于这种应用层面的信息，来提供特定于具体应用的一些优化。然后open dev是一个足够复杂，但又是一个非常有用的一个agent框架，它会非常多次复杂的调用这个大语言模型。这为我们在推理引擎方面的优化提供了更多的可能性，也有很多这种research的机会。",
      "speaker": "发言人2"
    },
    {
      "time": "00:18:36",
      "text": "这个就非常有意思了。我我那我再问几个问题，就是你们当时怎么想到做做这个VLM的？",
      "speaker": "发言人1"
    },
    {
      "time": "00:18:42",
      "text": "2022年年底在ChatGPT launch之前，我们在学校里面set up了一个大语言模型的demo。当时service模型还是facebook的OPT175B这是一个非常古早的一个模型。然后我们一开始set up这个demo的目的，其实是为了宣传我们之前的一个开源项目alpa ALPA。然后是一个自动做模型并行推理和训练的一个研究项目。但是在部署这个demo的时候，我们发现我们的demo会特别慢，并且GPU的利用率也非常低。这时候我们就意识到大语言模型推理本身是一个值得关注的问题。然后在当时市面上也应该也完全没有任何专门做大模型优化的开源系统，所以我们就打算自己做一个。",
      "speaker": "发言人2"
    },
    {
      "time": "00:19:32",
      "text": "然后我们在做到一半的时候，发现大模型在做推理的时候，GPU的memory是一个非常大的瓶颈。因此我们在多次迭代之后，提出了一个新的attention的计算方法，用配置的attention来解决这个问题。然后我们利用操作系统里面的paging和virtual memory的技术来管理transformer里的attention操作里用到的KV cash memory。我们发现能这能够比之前的state of d2提高大概4至5倍的内存利用率。以及最后我们能够有大概四倍的吞吐量的提升。",
      "speaker": "发言人2"
    },
    {
      "time": "00:20:05",
      "text": "然后我们大概是在2023年的二月份左右，开始把page attention作为一个research idea来实现。并且我们开始写paper并且build的这个research research prototype。这个research prototype也就是后来的vm然后大概到了四月底的时候，我们把论文投出来了。然后我们大概又花了两个月的时间来polish我们的open source code，让它能够比较方便的能够立刻使用起来，能够easy to set up。然后大概在2023年6月底的时候，我们release了我们的open source project。后来我们也非常幸运，能够有很多人使用我们的project。然后我们也一直很积极的维护到了今天。",
      "speaker": "发言人2"
    },
    {
      "time": "00:20:47",
      "text": "他们可以跟大家简单解释一下，是不是一个推框架。对于对这块不是那么了解的同学。",
      "speaker": "发言人1"
    },
    {
      "time": "00:20:54",
      "text": "大家都知道训练一个大模型是一件非常困难，非常昂贵的事情。然后我们经常听到像万卡集群之类的词。但是其实在一个机器学习模型的整个生命周期里面，就是在训练完成之后，部署这个大模型的成本才是真正的大头。这背后的道理其实也很直接，就是因为训练模型的时候，我们总共也只用训练一次，训练完成就结束了，这是一个单次的支出。但是在部署的时候，一个大模型需要一直接受外部的请求，这个也让部署变成了一个长期的成本。",
      "speaker": "发言人2"
    },
    {
      "time": "00:21:29",
      "text": "所以为了不降低这个部署的成本，在部署机学习模型的时候，我们一般会做一系列的优化来降低这个推理的成本。就比方说像是做一些模型的量化，比如说低精度来做一些计算，以及像是需要准备一些专门为推理优化的一些扩大kernel之类的。因此我们会需要一个专门的推理系统，或者说是推理框架来做这些推理时需要的优化。然后在这些优化当中，一个非常重要的优化是batching。也就是说我们把很多个请求合并成一个大的batch，然后来一起喂给GPU来执行。这样子我们就能够有更多的请求被并行处理，也就能够提高这个GPU的利用率。",
      "speaker": "发言人2"
    },
    {
      "time": "00:22:12",
      "text": "而对于一个比较传统的机器学习或者深度学习模型来说，比方说一个图像分类的模型，这个batching其实是比较简单的。就先来的请求可以先等待一会儿，然后等足够多的请求来了之后，我们可以把这些请求都batch在一起，然后再一起喂给模型来跑，但是对于一个大语言模型LM来说，这个batching就变成了一件比较复杂的事情。因为大家用过ChatGPT或者任何其他大语言模型都知道，他们的输出都是一个一个字输出的。而这个特点会给batting带来两个问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:22:46",
      "text": "第一个问题是说就是对于同一个请求来说，我们需要不停的调用LLM来生成下一个词。然后这整个过程其实是一个会变得非常的长。然后如果一个新来的请求要等到上一个请求完全结束才开始，就会等待太久的时间。然后解决这个问题的办法是continue batching。然后具体来说，如果前一个请求在执行到一半的时候来了一个新的请求。我们会在前一个请求执行的过程中直接加入新的请求来避免等待。",
      "speaker": "发言人2"
    },
    {
      "time": "00:23:18",
      "text": "第二点是说，即使我们有了continue special不同的请求，仍然会有不同的输入和输出的长度。因此他们在推理的过程中的内存占用也会经常的变化，导致很多内存被浪费。这个也是我们刚刚提到的page attention解决的问题。我们用操作系统里面的配件的方式来消除这种内存的浪费，也总的来说就是大模型的推理系统就需要做一系列的优化来提高推理的效率，从而降低这个推理的成本。VOM整合了我们刚刚提到的所有的这些优化的方法。所以因此和直接用rugged face transformer相比，我们能把推理的成本降低1至2个数量级。",
      "speaker": "发言人2"
    },
    {
      "time": "00:23:59",
      "text": "VRM已经是已经是这个呃行已经是这个行业标杆编程的一个best practice。铁证有什么要补充的吗？",
      "speaker": "发言人1"
    },
    {
      "time": "00:24:09",
      "text": "对，谢谢周欢的介绍。我觉得VM是一个特别牛逼的项目。然后我其实是这样看的，就是从去年初拆GPT出来，当时大家都在说，这个东西好是好，就是太贵了。但是我们看看到现在这个deep sick已经把100万的这个token的价格降到了一块钱价格的下降，可能有百倍的提升。然后我其实一直很好奇说这个百倍的提升到底怎么拆解？是就是它这个提升主要的原动力来自于那然后我知道像flash的这种技术肯定是对他有很大的帮助。但是我好奇从比如说大家直接拿这个python的transformers推理，升级到用VOM这种专用的推理引擎加上KD开始。然后这种去去推理这个中间的一个性能的提升，我不知道你们有没有做过测量，就是有多少提升是就是大家从原始的python推理切换到BOM带来的这一个。",
      "speaker": "发言人4"
    },
    {
      "time": "00:25:01",
      "text": "非常好的问题。我们最一开始发布的时候，我们和这个直接使用hunting face比较，我们大概可能有一个24倍的一个提速。而这个里面大概有有有这么几个技术。第一个是第一个是我们叫做continuous batching电子这个技术。这个LM大家使用的时候知道，就是你的不同的就不每一个用户的输入的长度会不一样，然后每个用户输出的长度也这个LM输出的长度也会不一样。然后如果我们按照以前的方法，把不同的这些LM的输输入和输出不同的这些请求给back在一起的话，他们会有我们会得到很多很多的padding，这个padding会非常影响我们执行的效率，因为这些都是浪费的计算。",
      "speaker": "发言人2"
    },
    {
      "time": "00:25:44",
      "text": "然后continue investing做的事情就是说，如果我们能够把best当中移出来。然后每当有一个新的request进来的时候，我们可以把它加到正在执行的这个base当中去，能够让执行继续下去。而不是说一个新来的一个request，需要等之前的这些全部结束才可以开始执行。然后这一个技术本身能够可以带来大概八倍的一个效率的提升。",
      "speaker": "发言人2"
    },
    {
      "time": "00:26:07",
      "text": "在有了continue special之后，我们会发现每次即使做了ue special，每次能够base多少request。是被这个内存，也就是我们所说的这个KV cash所高度的限制住的。因然后我们会发现有很发现在之前的很多系统当中，很多的那KV开始的内存其实都是被浪费掉的。因此我们我们我们也发明了一个新的technique，叫做配置attention。然后配置attention这个technique能够帮助我们更好的利用去体验的这些KV开始的memory。然后能够让让我们在每一个推理的过程当中，能够伴随更多的request。这使得我们能够再进一步的提高大概三倍。所以我们当时的24倍大概是这么来的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:26:50",
      "text": "推理的过程当中还有很多更一般的针对于这个推理模型的优化。就比方说我们可以换一些，就是针对于推理有一些更快的一些qua kernal。然后有一些其他的一些各种各样的系统上的优化。然后比方说做一些pantile，这可以能够再大再提升，比方说4到5倍。然后这样子我们可以到了最后可以比就是用现在的这些推理引擎和直接使用一个patch ch模型做一个简单的推理，可以达到一个100倍的加速。",
      "speaker": "发言人2"
    },
    {
      "time": "00:27:17",
      "text": "了解，多谢。我觉得这是非常重要的工作。这个protocol Price降下来，我们就是各种应用，包括open dev这种才能实际的被大家用清。对，非常感谢。",
      "speaker": "发言人4"
    },
    {
      "time": "00:27:27",
      "text": "多谢铁站。后面本来我想提这个问题，正好你就帮我问了。所以这个找对嘉宾很重要。",
      "speaker": "发言人1"
    },
    {
      "time": "00:27:32",
      "text": "其实我是想代表快的话，官方感谢一下BLM。因为如果没有BRM的话，其实我们国内外影响力没有办法把它做大。BLM就是尤其在我们快1.5开源的时候，还真的帮了我们非常多。现在的话基本上我们默认推荐给大家做推理部署的都是比较我去年到今年接触的比较多的客户，他们基本上用开源模型的话，尤其用我们的模型都是会选择用DRM来做部署。如果玩大语言模型的小伙伴，没有用过BLM的话，一定要来试一下。",
      "speaker": "发言人3"
    },
    {
      "time": "00:28:10",
      "text": "正好从正阳你们算是这个使用者的角度可以聊一聊。一个是说VLM解决了大家的什么问题，那之前大家是怎么去处理这些问题的那另外一个就是说那你觉得同样是用用边来做来做这个推理，那国内跟国外的做法有什么不一样吗？因为我想你们的社区里面，因为应该两边都可以看到。",
      "speaker": "发言人1"
    },
    {
      "time": "00:28:31",
      "text": "这是个很好的问题。我用更直白的方式来表达，尤其是我们看第一代的时候的话，我们很多用户在跑我们的模型的时候，经常会反馈说各种很慢，很很不方便这类的问题。但是一般他们用BR之后的话，最直观的体验就是它的速度非常快。而且的话一般来说你要选择去部署你的模型的话，一般确实是需要一个推理部署的框架。DRM也好，any face的TGI也好，都是非常不错的框架。",
      "speaker": "发言人3"
    },
    {
      "time": "00:29:08",
      "text": "DRM的易用性的话对比较多用户来说用起来比较方便。因为他学习成本的话相对来说比较低，所以我们很多用户其实都实际的就在用BLM部署，然后做一些小的应用。直到我去走访一些客户，甚至是一些相对比较大的企业，其实他们自己内部的话都会去使用BRM来作为他们的部署。因为他们自己有比较多的机器去serve大语言模型。那这时候的话选用DLM的话是一个非常不错的选择。当然有一些大公司可能自己去开发，可能把能把性能做得更好。但是对于绝大部分公司来说，单元模型可能只是它业务当中的一个部分，它不是all in在这上面。这时候选用VRM，然后它的性能相当不错。",
      "speaker": "发言人3"
    },
    {
      "time": "00:30:00",
      "text": "国内和国外的话，我觉得其实状况都差不多。因为在国内的话用BLM的我见到的是比较多的，当然也有一些的话用英伟达的TSRTLM。上海浦江的话那边也有一个比较不错的deport。但看起来的话还是我们的用户量会更大一些。",
      "speaker": "发言人3"
    },
    {
      "time": "00:30:23",
      "text": "前面主要还提到说像open dev d这样的这个应用，你会期望他对于你们的帮助会表达。你可以进一步跟大家elaborate一下，为什么会觉得像open the open open devon这样的这种应用，它有什么特点？对于你们来说怎么去选择一些。这种优化的方向，为什么他对你们会是比较有帮助的？",
      "speaker": "发言人1"
    },
    {
      "time": "00:30:43",
      "text": "我是一个PHD student，所以我们可能更多的想看接下来的这个研究的方向，在这推理这一步，在我们比方说在BM完成了我们现在想要做的这些各种各样的优化之后，那再接下来的未来我们怎么样能够进一步提升LM推理的这个performance。我们觉得这个有如果就是给我们一个transformer model，然后在一个理解GPU上面跑。如果是面对一个general workload，如果我们对这个workload本身它没有什么information的话，那我们能做的优化其实是非常的有限的。然后这些优化我们可能我觉得我们整个学术界大家都已经explore差不多了。我们需要做的事情是把这些优化更加有机的整合到一起。但是如果想要在这些优化这之外再进一步的话，那我们需要做的事情是你给我看这个。",
      "speaker": "发言人2"
    },
    {
      "time": "00:31:29",
      "text": "比方说我们一个specific application specific use case，它有哪些特点能够给我们所使用。我觉得open devon本身是一个非常有意思的一个use case。首先它是对一个LM的一个非常复杂的一个调用。就是你不光是简单的比方说跟他一问一答，甚至他超越了multi run conversation。你是比方说你会先进行几个跟一个A镇进行几个几轮对话，然后你再会和一个环境交互，然后你的环境会给再给你一个feedback，然后你再通过这个环境的figure black，你再喂给另外一个其他的LLM。",
      "speaker": "发言人2"
    },
    {
      "time": "00:31:58",
      "text": "然后我就觉得这个和这个L和这个LM有这种非常复杂对话。它可以我们可以把这些对话的information来告诉我们的后端的推理引擎。就比方说OK，我们接下来在这一轮对话之后，我们会知道这一轮对话的结果会马上feed给另外的一个software。And然后这个soft engine来执行完了之后，会马上再去query LM。我们在推理引擎当中，我们可以提前的把上一轮对话的结果给case到推理引擎当中。然后在下一轮对话下一轮结果来的时候，我们就可以重新利用之前上一轮推理的这个结果来做下一轮的预测，而不需要整个重新开始。这样子可以让我们的这个LM推理的效率进一步降低，然后也是大家使用的成本进一步的降低。",
      "speaker": "发言人2"
    },
    {
      "time": "00:32:38",
      "text": "其实像去年也出现了像什么auto GPT，什么baby AGI那些需要一系列推理的这种agent的框架。就是我好奇像open devin这样，八年前我们遇到这些他的需求会有什么不一样。",
      "speaker": "发言人1"
    },
    {
      "time": "00:32:53",
      "text": "对我的感觉是open David在我看来是一个更加成熟的一个agent system。就好像我觉得当年的这个auto GPT可能只有一来一回。但是这个open demo可能会有更多的一个不同的environment的交互，可以有一个更加复杂的use case。然后我觉得open，我其实个人还是非常喜欢open David的背后的这个open source community。我觉得它是一个非常serious group of people that is actually working to build a great product。所以我觉得在这个前提下，我觉得open dev是一个非常吸引我的一个项目。对，俊阳可以再去补充。",
      "speaker": "发言人2"
    },
    {
      "time": "00:33:28",
      "text": "可以。就auto GPT的话，其实我们去年有关注他像一个老板，我发送一个需求，然后他就把多部整个就给推出来。他们的社区其实也很活跃，但是A准这种框架的维护下去，其实往往大家会发现说这个A准好像没有真的达到真实可用的程度。所以慢慢大家就用的就越来越少，整个热度它就下来。但是会有层出不穷的新的义诊的框架出现。今年的话其实David的出现就是一个很好的例子。今天大家虽然都在谈义诊，但是说起义诊的例子，好像也就拿出David好像是比较impressed的一个case。",
      "speaker": "发言人3"
    },
    {
      "time": "00:34:12",
      "text": "其实我觉得这个agent它会更加明确一些，我就是在解决coding相关的问题。举一个例子，比如说像这个swe bench这边的话，我是根据这个意识然后去写这个goole request，然后去解决这个仓库的问题。我觉得他更加聚焦之后的话，他把这个问题定义的相对来说会更加清楚了。因为刚才卓翰也提到了open demo的一个特点，就是它其实是一个多轮交互的过程。",
      "speaker": "发言人3"
    },
    {
      "time": "00:34:46",
      "text": "我们需要解决一个代码相关的问题的时候，首先这个agent的话先做planning。Planning的话的意思就是说我先想我大概要做哪些步骤，我大概要干哪些事情。接下来的话我就一步一步去干。我通过这个urge language model把代码写出来。写出来之后然后到上海环境去进行运行。运行之后的话我就会有一个观察，然后根据他返回来的内容的话在进一步的去做下一步的决定。所以它其实是一个多轮交互的过程，我觉得这个给VRM带来了一些新的挑战。",
      "speaker": "发言人3"
    },
    {
      "time": "00:35:28",
      "text": "因为在A准这种场景，多轮对话的过程当中，其实有很多东西是有办法给cash起来去降低我们的成本的。我举一个例子，我们现在在测这个swe bench，其实是非常痛苦的一件事情，因为它真的非常昂贵。因为我们现在的agent的话，它背后的垃圾number model是用GPT four。目前绝大部分的开源的模型的话还达不到相应的水平能够做好agent这个工作。那GPT4其实就很贵，因对我们是多轮的对话。比如说我第一轮我就把这个信息传进去，然后第二轮的时候的话，我又得把我这个历史给它传进去。所以其实我们整体使用起来的话非常昂贵，评测一次的话都得用上几百刀。所以如果说BLM能帮我们把成本给打下来，然后我作为开发queen的成员，当然也希望有更多的开源的大模型了，能把水平做到GPT four level。那我觉得open devin的话应该能发展的更加迅速。",
      "speaker": "发言人3"
    },
    {
      "time": "00:36:32",
      "text": "我觉得就是像open and dive这样子的agent的应用，可以给我们主要带来2点2个优化的点。第一个就是说previous cash就是像比方说它是一个跟一个L进行多轮的对话。所以我们可以知道，比方说前面这一轮结束之后，我们下一轮就要使用。然后那我们可以立即，我们也可以告open demo，可以告诉VM我们前面这一段对话需要被开始下来。我们可以直接在在就像就像刚刚说的，我们要进行下一轮调用的，可以就是直接重新利用之前算过的这些结果，不需要重新的计算，可以节省这个成本。",
      "speaker": "发言人2"
    },
    {
      "time": "00:37:04",
      "text": "第二个点就是说对，就是运营在open devil里面，你的多轮对话其实是受控的。这点和比方说一个你serve普通chat ble的不一样。就假设你好像serve一个一个普通面向人的chat，but你是不知道那一个人在下一个是在什么时候，多久之后才会给你一个回复的。但是在open dev d里面，你下一轮对话的什么时候回复是一个非常可控的事情。因为你是在做一个和一个software交互，然后和这个soft交互的时间，你可以高概率你是能够提前预测这一个时间的。然后我们可以利用这个时间差，再做一些我们在这个scheduling上的一些优化。然后我们可以能够达到一个更更快乐的一个更高的一个效率。对哎其实我。",
      "speaker": "发言人2"
    },
    {
      "time": "00:37:45",
      "text": "还想问一下俊阳和浊环，就是刚才我们聊到其中有一些技术可以支持这种agent。但是现在GPD4它的这种API提供的方式是说你每次都要重新计算。就算你这些token要被重用，但是你下次把他扔过来的说还是要重重新按照同样的价格去去算起。",
      "speaker": "发言人4"
    },
    {
      "time": "00:38:02",
      "text": "然后我看到推特上也有在说这个Jimmy I将要支持这个context cashing这种技术。我不知道说这种技术你们有听过吗？或者说有了解吗？这个基础他跟perfect catching会有一些相似点吗？",
      "speaker": "发言人4"
    },
    {
      "time": "00:38:18",
      "text": "见面从我的角度来看的话，我其实更关心gme的长序列的能力。因为刚才提到open devin实际上它是在做多轮的交互，它的context话题其实会非常的长。所以当初用到gamine的话其实是关心这个点。我还没有具体去了解他context cash他是怎么去做的，但我知道他们确实对医诊方面的话会去做相关的优化的，这非常关担心agent相关的内容，最近不是也有像project extra，他的demo的话其实也非常的出色。我之前看开源社区的话，可能会对agent相关的，以及跟deploy yet的结合，可能是SG line这一个框架，会在前端后端的话都会去做一些相关的工作，让部署推理部署的话跟agent这种类型的东西更好的适配在一起。",
      "speaker": "发言人3"
    },
    {
      "time": "00:39:17",
      "text": "是的，对，其实这个prefer cash也是一个我们在VRM里面现在非常关注的一个内容。其实我们最近也在写一个blog post来想要讲一讲我们关于做我们如何做preface cashing，在UM如何做preface cashing的事儿。对给大家简单。",
      "speaker": "发言人2"
    },
    {
      "time": "00:39:31",
      "text": "介绍一下什么是preface cashing。然后为什么接下来你觉得是一个比较重要的一个工作？",
      "speaker": "发言人1"
    },
    {
      "time": "00:39:37",
      "text": "我以为perfect cash是避免重新prefer的那个时间。然后因为你可以把之前已经输出的东西都都记录下来，然后第二次就不用prefer之前特别长的一个context。但我感觉我可能我是不是理解错了。",
      "speaker": "发言人4"
    },
    {
      "time": "00:39:52",
      "text": "对我觉得其实可能不光是preview，就是即使你是这样多轮对话这么一个情况，你也是可以比方说把第一轮对话的。以及第一轮对话LM输出的这些内容，再加上第二轮对话的当当你第二轮对话u user输入一个新的时候，你其实整你之前第一轮对话的全部的新context，就包括之前第一轮对话的这个输入的prom，以及第一轮对话输出的这些这些tokens。他们所对应的整个的KV开始，你都可以在你的你的memory当中开始起来。然后在下一轮，然后你再在接下来开始在第二轮对话的时候，你可以直接从第二轮对话的prom开始计算。而不需要把之前的所有的第一轮和第一轮的之前所有的这些context来做一个做一个cash。",
      "speaker": "发言人2"
    },
    {
      "time": "00:40:36",
      "text": "刚才聊到的几个话题，我觉得都是跟都是跟这个agent相关。而且我们的确也看到了，我最近正好这个german I还有这个open I的发布会上，我想一些一些跟开发者的这种API的一些更新。其实我想也会越来越多的出现跟agent相关的能力。我想问一下俊阳，在你看来现在的这个无论是他这个foundation model本身的能力，还是说周边它的生态也好，就是对于做open eleven这样的这样的agent项目，你觉得你可能在过去半年你觉得已经帮带解决的比较好。你觉得还有哪一些接下来你是希望看到整个生态能够去补全的一些gap的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:41:20",
      "text": "你就站在open dev的角度来看，我觉得可能只是一些别的agent相关的一些东西，任务解决的还行。比如说一些简单的，他有一些做像summer ization，然后写个report之类的，他们自己做的这种agent。比如我之前关注的像酷EAI的这个项目，他最近也上了文达的课程。那他会教大家去做一些简单的一些agent，然后应用到我的一些落地场景。但它其实本质上做的一些任务的话并不复杂。比如说就是做一些摘要、翻译，还有一些文字的一些创作和生成。在推特上的话，其实也有一些人专门是帮别人做这种写作的agent，能让你写出比较生动的文章、小说。它其实背后的话其实就是programing，我觉得这种其实做的还好。",
      "speaker": "发言人3"
    },
    {
      "time": "00:42:18",
      "text": "但是如果你看像coding这种场景，今年coding真的是非常火。Coding这种场景的话，如果你只是简单的做all engineering的话，你是很难解决实际的问题的这也是为什么说demo它虽然仅仅只是一个demo，但是大家会看到它真的是帮助大家真实的在我的场景当中用起来了。比如说机器学习的工程师，他可以用它去，比如说举个例子，比如说就用BRM去部署模型。",
      "speaker": "发言人3"
    },
    {
      "time": "00:42:51",
      "text": "比如说我现在是一个BRM的小白，然后我对large model的推理的话完全不了解，我使用前其实是有些问题的那如果这个时候的话，我能够通过open open deven，然后我去问他说我想把这个honey face上面的某一个模型，比如queet部署在哪哪，然后这个时候的话open dev能帮我解决。我觉得这会是一个很有意义的事情。所以今年我觉得这件事情还是挺有可能发生的，就是因为我们现在用的GPT four的话，然后再配上open demo的agent的框架的话，已经有一些迹象说明做到这件事情可能并不是太远。我觉得至少过两三个月的话，应该就能够看到说以后比如说一个前端的工程师，然后这时候我就问open David，然后这个怎么做？然后帮你做一个前端出来机器学习的工程师。比如说今天我想find一个模型，这时候我就通过open deven说i would like to find to my model，I would like to find to这个时候话open demo帮你把这件事情给做完。然后最近我其实跟auto AWQ的作者还比较熟，然后他非常关注David和open dev d，他自己也是devon的内测的用户，然后他其实就经常去问David说，我怎么把我一个DF16的模型。就现在大部分开源的模型，它其实都是就我们谈DF16就是在谈这个模型的精度，然后希望把它量化到更低的精度，那这样的话它的部署的成本的话会更小。",
      "speaker": "发言人3"
    },
    {
      "time": "00:44:35",
      "text": "AWQ就在做这样的事情，然后他就会去说我怎么把我的模型给量化，然后他自己的代码库有不少的issue，然后也希望说David你能不能帮我解决我这个a auto AWQ这个代码库的问题。我觉得这些都是在真实的帮助大家。所以今年我觉得coding是一个非常火热的方向，也是非常值得大家投入的方向。",
      "speaker": "发言人3"
    },
    {
      "time": "00:45:00",
      "text": "我觉得如果说以后要更要有更多的公司，我就要开始去打更复杂的像open devin这样的这种agent产品的话。你觉得现在你看到这些开发工具，你觉得可能还有哪一些需要可能不足的这个地方，你觉得还有哪一些可能是需要希望能够看到这个社区里边能够这种工具的层面能够弥补，让大家能够更好的更容易的去做更复杂的agent。",
      "speaker": "发言人1"
    },
    {
      "time": "00:45:28",
      "text": "这个问题我一时半会还没有一个很完整的答案。因为做open demo这个事情，很多人我们在这里面rush的非常快，需要的东西真的非常多。我举一个例子，如果你想做出一个好的agent，你肯定少不了好的评测。因为如果你都没有合适的评测的话，我只是一些demo其实是没有意义的。大家在真实场景当中的话，不知道怎么利用它解决问题，也不知道它的稳定性是怎么样的。我们其实就找了不少的评测，但是目前看下来可能也就sweet bench。",
      "speaker": "发言人3"
    },
    {
      "time": "00:46:07",
      "text": "但是swe bencher的话，这个评测其实做起来其实是比较痛苦的一件事情。就是你需要做比较多的工作，让这个评测才能把它给跑起来。所以我们其实是希望能有一些工具，比如说他就是能做一些评测的服务。这样的话研究领域的人员，比如说现在的NLP的博士，或者是做agent相关的博士，他们能把更多的精力放到agent方面的创新，而不是在一些其他的问题上面。花了太多的时间。其实不仅仅是工具了，包括这个数据环境也好，一个好的评测以及配上稳定的评测的服务。所以我们现在其实也会跟比如说像e to b去进行一些合作，他们会去支持我们。未来的话会希望说这个评测服务的话在云上稳定的运行。",
      "speaker": "发言人3"
    },
    {
      "time": "00:47:06",
      "text": "将来的话研究界的人员创新了一个好的aga我在我自己本地稍微试了一下，我觉得还他还不错，那我就放到这上面提交上去就可以做这个测试。另一方面，今天大家想打造好的agent的话，大家其实不希望这个agent它其实是非常昂贵的。因为大家做应用的话还是希望成本能够降下来。所以像推理侧，像BLM或者是其他的一些相应的一些框架。",
      "speaker": "发言人3"
    },
    {
      "time": "00:47:37",
      "text": "刚才提到的如果能提供一些比较好的支持的话，配合当前的large language model，就是开源的大模型的发展的话，我觉得会做的比较好一些。开源的大模型的话，我觉得现在距离他完成好这些任务还是有一些距离的那即便是闭源的模型的话，像GPT four，像cloud three ops它其实也有不小的距离。所以我们可能最近在想说，我们能不能自己去构造一些合适的数据。因为这个dome的话相对来说还是比较小。如果我们能构造一些合适的数据去反推，用这一个模型让他在某一些场景做的比较好，比较稳定，那我觉得企业可能就可以用起来。所以反正方面的工具的话也会是我们比较需要的知识，主要还是数据。因为今年环境其实是相对来说比较简单。对。",
      "speaker": "发言人3"
    },
    {
      "time": "00:48:30",
      "text": "因为我看到你在另外的一些这种分享中也提到了，多模态癌症的作为未来一个趋势，也大越来越多大模型支撑越来越越来越强多模态能力，能跟大家讲讲那那要做一个多模态的agent可能他又会面临哪些挑战？那在头顶这样的场景，这个多模态能力有可能给我们又带来哪些想象空间？",
      "speaker": "发言人1"
    },
    {
      "time": "00:48:56",
      "text": "我觉得这个问题非常好，因为我们团队非常关心多模态，我这里指的是queen这个团队。因为queen这一边的话其实做luck lk model其实只是第一步。然后我们其实做快BL，尤其是快BL max的话效果还不错。我们现在其实看到的是一个agent的话，如果你仅仅只能看到代码和文字相关的信息，他所做出的决策他不一定是最优的。他往往需要看到这个物理世界的信息，然后才能做的更好。",
      "speaker": "发言人3"
    },
    {
      "time": "00:49:34",
      "text": "我想到一个例子，是跟手机比较相关的一个例子。就是今天如果我做一个多模态agent的话，其实我们希望这个agent能帮我们干什么呢？比如说它可以帮助我们去操作我们的手机屏幕，那我就可以用自然语言的方式去进行交互。其实这是一个很好的idea。因为在google的demo当中的话，我们其实也看到它其实可以去帮助视障人士。就是盲人的话，他其实可以通过自然语言交互的方式的话去使用手机，这是一个非常好的事情。那这里的话其实这就需要他对多模态的理解。毕竟这个手机屏幕上的这个操作的话，它其实看到的东西才是最直观的。之前有一个客户跟我提到一个点，就是说你如果输一段复杂的代码进去，让这个模型理解的话，模型其实是也会很难理解。就像人一样。",
      "speaker": "发言人3"
    },
    {
      "time": "00:50:30",
      "text": "比如说今天互联网的产品，移动端的这个产品，它其实经常做出很多功能上面的变化，然后界面也会发生很大的变化，但是人总是能够看得懂他的代码的话，可能已经变得非常多了。但是人不管变什么样的颜色，不管变什么样的logo，人就知道该去干什么样的事情。我们希望能够通过VR的能力的话，让agent能够实现这一个点。我就直接输入图片，输入我的录屏，然后模型就可以根据我看到的东西去做出正确的决策。我觉得这个才是最直观的方式。不然的话如果你都是以输入代码的形式去做的话，一方面你的序列会非常长，另一方面的话这个事情非常不直观，也是给foundation model本身带来了过大的挑战。",
      "speaker": "发言人3"
    },
    {
      "time": "00:51:22",
      "text": "接下来的话大语言模型肯定不仅仅是只是大语言模型。接下来的话整体的趋势就是一个大一统的趋势，将各种模态各种任务都统一到同一个模型当中去。我觉得今年的话如果能够把VL和agent结合起来的话，我觉得A准的能力能够更上层楼。",
      "speaker": "发言人3"
    },
    {
      "time": "00:51:42",
      "text": "记得俊阳一开始有也有提到，就是有一个UI生成的工具。其实我觉得就是把open dev和视觉的VL的模型结合起来，其实用来创作这个UI是非常爽的。比如说就拿就像刚才说的这个手机的这个例子，其实大语言模型是不会理解到他写的这个代码的这个设计体验，就是用户用起来这个体验是什么样子。如果能给他一个看到最后的这个图片，然后他能够想象到这个和我们的需求有多远，然后再去有一个feedback去去不断的去调整它的这个代码。以至于它最后生成的这个产品是它的使用体验和用户要求的这个使用体验非常一致。我觉得这种就的就是对于这个代码生成的质量会也会有蛮蛮大的帮助的。",
      "speaker": "发言人4"
    },
    {
      "time": "00:52:28",
      "text": "对对对，甚至它也可以用来做一些评测。比如说我们让这个大语言模型去生成一些东西，然后最后大语言模型生成了一个UI那我们怎么去评价这个UI好不好？那就拿这个VL就是视觉的这个模型来做一下比较。然后现在其实很多大语言模型，比如说debt什么这些，其实他都是拿ChatGPT这个语言模型做做评测。未来他也有可能在评测代码生成的时候，用VR的模型来做一个评测。肉眼看一下说你生成的这个代码的UI组件的样子和我的诉求是不是一样的。",
      "speaker": "发言人4"
    },
    {
      "time": "00:52:58",
      "text": "其实在去年年初的时候，这个transformer作者之一出来做那个adapt那家公司，其实它展示的也是这种在这个GOI的界面上去做操作的能力。那那我们其实现在看到那一类的工具，好像落地的还不是那么多。那是不是也许就是因为过去这一年多模态的能力还不足以支撑我们刚才所描述的那种场景。",
      "speaker": "发言人1"
    },
    {
      "time": "00:53:25",
      "text": "对，accept他们的话，其实做的是都有那个系列的模型。他从学术上来说的话，其实是一个很不错的一个设计。但是去年多模态其实直到今年，现在最好的当然是GPT four b。即便大家跟GPT four b比较接近，你会发现4B的效果还是具体实际应用还是会有一些远。",
      "speaker": "发言人3"
    },
    {
      "time": "00:53:51",
      "text": "因为多模态模型所带来的幻觉其实是远比大语言模型要大的，它往往很多事情认不出来。而且现在的多模态的模型的幻觉程度，我可以用一个例子来说明。尤其是跟实时性相关的一些信息。比如说今天有一台汽车，它是个什么样的品牌。这个时候的话，如果你的多模态模型你把它给认错了，其实是会给客户带来非常大的损失。所以今天要解决的这些问题其实都非常的难。",
      "speaker": "发言人3"
    },
    {
      "time": "00:54:24",
      "text": "包括落到这个模型对图片信息上面的理解的话，对于一些小物体的细节的一些信息，因为图片的信息往往其实都比较复杂。这个上面的话会有一些非常细小的一些文字，还有一些检测当中的话。比如昨天有一个客户跟我提到说检测落叶这一个问题。然后我们的模型就没有办法把非常模糊的这个落叶给它检测出来。这个时候其实对你实际的这个应用的话都会造成比较大的影响。",
      "speaker": "发言人3"
    },
    {
      "time": "00:54:56",
      "text": "从我的角度来看的话，我觉得大家还是要更加关注多媒体的foundation model本身的能力的提升。我觉得现在还是比较远的，就今天大家都会提说GP4已经非常不错了。大家非常清楚的瞄准说，如果我今天能做一个last model，达到GPT4 level，我就已经在很多场景当中能用上，这是自然语言的场景。但是在动态的这个场景当中的话，目前我的直观感受是大家觉得GPT4V距离可用还是有非常大的距离的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:55:30",
      "text": "讲讲到这个我就多补充一句。其实去年大家我觉得还有很多讨论说，这个做模型的公司然后吞掉多少要吞掉多少应用，对吧？但其实我们看到即使即使在模型能力有了很大的一个提升了。你会发现从一个很好的一个模型或者模型能力上的提升，它转变到一个一个应用场景，一个更好就更好的一个实现。其实中间有很多很多事情，有产品相有这个模型相关的，有工程相关的，也有UIUX设计相关，其实是很长的一条链路。所以我觉得所以我觉得今年，我们会看很快看到很多这种多模态能力上的提升。我觉得其实应用侧仍然有很多我觉得值得很多工作，也有很多值得期待的这个地方。",
      "speaker": "发言人1"
    },
    {
      "time": "00:56:16",
      "text": "在这个agent这个话题我就多问我就多问一句，去年我们就看到很多这种agent，大家怎么看待这个事情？为什么去年会有那么多agent的framework？那到最后真正要真正要用起来，到底我们需要的是一个怎么样的agent framework？这个东西应该是由一个大模型公司来去做的吗？还是说你们觉得做应用的公司，或者甚至一个第三方来去完成这个事情。",
      "speaker": "发言人1"
    },
    {
      "time": "00:56:43",
      "text": "社区的角度来说，就是大模型公司。现在大模型公司基本上要么是大厂或者是明星的创业公司。我觉得其实可以把这种机会留给更多的创业公司或者是开发者群体。",
      "speaker": "发言人3"
    },
    {
      "time": "00:57:01",
      "text": "首先这个领域的想象力非常的大，然后可以做的事情也非常的多。另一方面的话，它相对来说不太耗资源。因为很多时候的话，大家是使用API就能把这件事情给做起来。其实我在国内也看到很多很不错的开发者，他自己去做出一些一整的框架，你还真的去帮助到不少的用户。包括meta GPT以及是biz LM，其实他们的A准的话都是可以帮到不少人我觉得这件事情是一个术业有专攻的事情。因为在大模型公司里边的话，的算法人员主要是比如说是LP的PHD，大家专注于说我怎么把这个LM的效果提升。有一些可能会涉及今天还会有人在做模型结构的一些优化。然后怎么训练，怎么构造我们的retraining以及post training的这个数据。",
      "speaker": "发言人3"
    },
    {
      "time": "00:57:55",
      "text": "我觉得大模型的公司应该去关注这件事情，把模型效果给提上了，然后大家基于你的模型上面去做应用和开发。然后应用和开发的话，它其实是整个社区的事情。因为其实会有很多你想象不到的天才就在民间。",
      "speaker": "发言人3"
    },
    {
      "time": "00:58:13",
      "text": "他可能并不是博士，他可能其实就是普通的一个开发者，但是他对于这一方面很有想象力，他旁边就nearing的手感非常好，然后他的开发能力也比较强，那他就有可能做出比较有趣的一些成果出来。所以我觉得这个机会的话其实是留在开源社区里边。我比较期待开源社区以及是一些应用型的创业公司的话，做出比较有趣的议论出来。",
      "speaker": "发言人3"
    },
    {
      "time": "00:58:41",
      "text": "我觉得我的理解的话，A类的其实分两部分。第一部分是大语言模型本身，对吧？你这个大语言模型怎么样能够让它能够更好apply to这种agent workload。这个也就是俊阳刚刚说的大模型公司需要做的事情。",
      "speaker": "发言人2"
    },
    {
      "time": "00:58:55",
      "text": "我觉得另一方面本身就是要给agent提供一个环境。我觉得这个事情本身是一个非常适合open source community做的一个事情。就比方说我要去订一张机票，那我怎么样能够把订一张机票这个environment提供给这个大语言模型。然后比方说像还有一个比方说我想要再跑一些代码，那些代码可能会单元模型可能会生成错误的代码，它可能会比方说生成RMRF，然后把你整个系统给删掉。你怎么样能够把保做一个像一个container west environment，然后让这个大语言模型能够安全的在这么一个环境里面跑。我觉得这是这个环境这部分本身肯定我觉得不是这些大学模型公司的一个特长，我觉得是更多的是像开源社会有开源社区的一些有很多能够在这边做的事儿，也是很多很多机会在这边。",
      "speaker": "发言人2"
    },
    {
      "time": "00:59:41",
      "text": "对我觉得这个事情可能还要再就是往后退一步，就是从更大的一个角度去看，就是说未来到底是一个更开放的生态。然后每个人每个企业做自己最擅长的事情，各司其职。比如说做模型公司，像周安说的，就是他可能公司的文化凝聚在一起，就是非常研究项的这种风气，然后所有公司的资源都是在围绕这一块来服务。那他可能能做出非常好的模型，但他不一定有各行各业的know how，他不一定有开源社区这么多的巧思，把这些agent的设计的天才放到另外一层，然后让他们来专门做设计各种agent甚至还有另外一层是说如何拿这些agent更快的去创造更多的内容。甚至最下一层可能还有很多麦克做培训，他们也做了非常重要的工作，是这样一种大家合作的这种形态。就不管是开源还是闭源，至少它是分层。然后每一层可以拥有一个相对标准的接口，然后和下一层进行紧密的联动。",
      "speaker": "发言人4"
    },
    {
      "time": "01:00:39",
      "text": "还是说未来会出现一个从鱼头吃到鱼尾，这么一个巨无霸形式的存在。它又做模型又做应用，甚至连下游的跟用户建立了连接，所有的培训什么的全都做，是到底是哪一样的形态？我觉得就是我我个人认为就是这种开放的形态可能还会发展的更好一点。我们如果去看过去人类的历史，这个兼容机打败这种独用的独享的机器。",
      "speaker": "发言人4"
    },
    {
      "time": "01:01:08",
      "text": "然后因为每一个厂家他都可以把自己的这些能力贡献进来，是一个非常开放的生态系统。哪怕像苹果这种相对来说硬件上比较封闭的系统，其实它也有很多很强大的供应链来完帮他完成硬件的设计。还有组装并不是苹果自己做。比如说苹果这个公司的基因，它绝对不会跟富士康去抢生意，因为就是完全不一样的经营的模式，然后公司的这个治理的理念都是完全不一样。你想尝试创造一个巨大的公司，把两个苹果富士康揉在一起，我觉得非常的难。",
      "speaker": "发言人4"
    },
    {
      "time": "01:01:39",
      "text": "然后即使苹果相对来说比较封闭的这个体系，它在软件层面它也是比较开放。他自己可能并不擅长做这个AI那么他也愿意。比如说可能最近有一些报道说，他可能会跟OpenAI去去合作。我觉得就是这种合作的心态，其实最后就导致说我们一定会像刚才说的这种分层合作的这种模式。",
      "speaker": "发言人4"
    },
    {
      "time": "01:01:58",
      "text": "那我我也我是觉得大模型公司可能在大模型的这个领域能做的非常好。然后agent这些公司在这个agent领域能做的非常好，他们并不是会一定是谁就被谁吃掉了。当然肯定是有一些公司想要往下，就是从从大模型往下做一步，也在做应用。但是还会有许许多多其他的模型供应用层的开发者去做选择。当然我就是这是一点。然后另外一点，其实双方都要从彼此的这个know how里面去去学。",
      "speaker": "发言人4"
    },
    {
      "time": "01:02:28",
      "text": "比如说像刚才我们提到标M和这个open demand，实际上本身就是在生生态的不同的层面。但其实双方都需要从另一方那里拿到需求和拿到一些目前的解决方案。它也是一个非常紧密合作的关系。然后如果说未来就没有这个应用层公司的活动了，那我感觉这个行业它的创新能力也会非常下降的很厉害。然后可能也并不是我们的最一个最优解。最后还是希望我们各司其职。",
      "speaker": "发言人4"
    },
    {
      "time": "01:02:59",
      "text": "我可能特别好奇，就出现那么多做agent framework的公司。为什么我们需要一个agent frameworks？为什么我们看到了这么多？比如说在推理这个领域，我一讲到推理我就想到VR，但是想到大家用的最多的agent for，好像很难想到一两个。那是过去是模型能力的问题呢？还是说你觉得还有哪些还有哪些问题是这些agent分我还没有能够解决的？",
      "speaker": "发言人1"
    },
    {
      "time": "01:03:25",
      "text": "明白，我我我觉得这里面其实有很多agent的这个场景，其实现在的模型还差不多可以接受。比如说用react然后去做一些简单的搜索什么这些agent场景，其实用现在模型已经可以完成，那肯定不是说模型侧的能力的限制。我觉得更多的是现在还处在一个大家去竞争这么一个氛围。就是我们可以拿这个前端框架的这么一个竞争来看。就是在比如说五年前、十年前，你可能看到每隔几天就有一个新的GS的出现了。然后GS的社区大家也都很活跃，然后各种去去尝试。然后截止到现在我们还有view，然后react and可能影响力小一点。",
      "speaker": "发言人4"
    },
    {
      "time": "01:04:07",
      "text": "但是我们还是有非常多的这个框架来来补，为什么呢？是因为这些开发者他们的诉求是不一样的，每个人的品味审美也是不一样的。然后这些框架的设计者，他可能自己去想象的这个场景也是稍有不一样的。所以我觉得就是面向开发者的这种众口难调的场景，其实你很难说在这么早的一个时间上就把所有的东西统一。",
      "speaker": "发言人4"
    },
    {
      "time": "01:04:32",
      "text": "甚至有些人他甚至到现在还不用各种agent的framework。然后他就是自己去去手写一下这个逻辑。他觉得比如说鞍山这种API的接口太太过于难用了，并不符合他的胃口，我觉得这很正常。我们现在有很多各种各样的framework work，因为这个framework它的创造并没有特别复杂。比如说你现在说这个machine morning framework，你现在去创造一个pyto ch，其实这个就很难。因为他已经积累了这么多年的工程量相当的大，它做了非常多的优化，但是agent规模它远远没有到这个阶段。",
      "speaker": "发言人4"
    },
    {
      "time": "01:05:07",
      "text": "这样有什么补充？",
      "speaker": "发言人1"
    },
    {
      "time": "01:05:08",
      "text": "我是一个build模型的人，我们团队的话其实有做A准相关的工作，但我们更多的是关注怎么去提升这个模型本身agent的能力。所以我之前其实不是很理解，既然都是prom engineering，为什么我需要一个agent的framework？其实直到我们团队在开发快agent这个系列，然后到今天open David，因为你会发现就是这个engineering，如果你直接自己去手写去做的话，确实是非常麻烦的一件事情。所以的话其实大家会去想说，我怎么用一个frame把它封起来。因为去年刚开始的时候，我们就在关注像LangChain，看着他一步步发展的代码变得越来越复杂。我觉得even frame这件事情，为什么有这么多的公司或者是有这么多框架，但是却没有一个很dominate的。本质上这件事情还是相对简单的一些，因为它背后还是from engineering，只是大家以不同的开发方式将其封装起来，它给不同的应用。但是大家其实真正关心的是说有没有一个真正好用的agent。",
      "speaker": "发言人3"
    },
    {
      "time": "01:06:23",
      "text": "所以从这个角度上来讲的话，我觉得agent framework很难出现一个很dominating的框架。因为确实刚才铁证提到众口难调。有人觉得你这个代码这样写不好，我就写另外一种形式，然后我就提出一个新的框架。像刚才我提到那个crew AI这个项目，其实它现在非常火，也有很多企业在用。",
      "speaker": "发言人3"
    },
    {
      "time": "01:06:46",
      "text": "最早的时候就是如果要一个人在开发，也就是说这件事情其实很多时候可能一两个人就能够把它给做起来。这就没有办法避免说今天如果大家都在做general，这个世界可能就会出现非常多的框架。如果沿着这个趋势去看的话，我其实反而希望说每个框架可能更加focus一些。就把几件事情给他做好。然后大家用你这个框架的时候就能拿到非常稳定的结果，我觉得这个是大家希望的一个事情。",
      "speaker": "发言人3"
    },
    {
      "time": "01:07:18",
      "text": "我目前就是看到有一些做一些writer的一些agent，然后他自己就去写了一个agent的一个frame。他昨天还在跟我聊，然后他在推特上说，如果我把它开源出来的话，大家不要嘲笑我的prompt。但是它的效果真的非常好。它可能有一些看起来很粗糙的东西，但是实际上的效果非常好。我觉得大家其实最终关心的是效果方面的问题，而不是说你这个brainbow怎么设计的问题。",
      "speaker": "发言人3"
    },
    {
      "time": "01:07:46",
      "text": "对你刚才提到了两次这个cruel AI如果大家感兴趣的话，这个link我也放在show note里面。我看到他可能挺有意思，是一个是一个multi agent的一个mult agent的一个框架。然后他自己的这个text line是AI agents for real use cases。对于还不是那么了解这个项目同学给大家介绍一下，到底什么是什么是cruel AI他在做事情为什么得到了大家的关注，怎么看待这个multi agent的这种架构？比如说在open dev d这样的以后要处理越来越复杂的这种开发的问题，那如也会需要用到这个毛巾agent的架构吗？",
      "speaker": "发言人1"
    },
    {
      "time": "01:08:25",
      "text": "这个问题很好。首先酷AI站在我的角度的话，其实本质上还是又一个agent的brain board，但我目前看的就是后发者往往会更加被大家喜欢。因为他把很多前人的一些毛病给他解决，他变得更加好用。荣耀这个人的话我其实关注了他比较久，然后他是非常热情的去用很多example去告诉大家我这个东西怎么去用，不断的去跟开发者去进行交互。他刚开始就他一个人在做这件事情，但是你可以看到他现在的contribute话，其实已经达到了七十多个人。他现在也开了一个公司，然后也有enterprise的这个方案，这也是一个比较常见的路径，这是第一个问题，我相对比较淡的解答一下。",
      "speaker": "发言人3"
    },
    {
      "time": "01:09:18",
      "text": "然后第二个的话其实是关于猫咪agent mod age的话，因为一般来说我们拿一个单元模型，然后把它封起来，那你可以称之为用engineering把它封起来，然后它可以称之为一个single agent。但是其实因为这些last model的这个能力本身，或者是说他处理复杂context的能力，它还是会比较有限的。因为真实的场景它的context会非常复杂。那这个时候的话，你就可以去做一些任务的分解，让不同的人去干不同的事情。就像一个公司一样，有人负责人做技术，有人负责做运营，有人负责做产品。就类似于这样的那你就可以这个agent你就让他扮演产品经理，另一个agent的话你就让他去扮演运营，还有一个agent的话就让他扮演这个前后端的。比如说他就是一个coding agent，然后更上一层，你可以有一个老板，然后你就可以做一个老板的ager，这个老板的话就去做战略决策，planning相关的这一些事情。所以它其实可以让多个大语言模型去进行协作。",
      "speaker": "发言人3"
    },
    {
      "time": "01:10:31",
      "text": "我个人觉得话就是如果你的这种框架如果你封装比较好的话，其实你背后它本质上还是大于模型。它究竟是single还是猫体，其实是没有那么fundamental的一些区别。但是从当前的技术水平来说的话，很多时候猫咪agent表现出来的这个效果其实就是要更好。因为分解好任务再去干这些事情的时候，这些模型实现的效果其实会更好一些。所以它主打mark agent这个特点，然后让大家接受我我觉得我觉得是合理的。从今年来说的话，如果今年还是主要是GPT four level，因为我不知道GPT five或会是什么样子，因为距离他们推出估计也不会太远。但从现在复欧的这个水平来看的话，我觉得解决真实任务的话，可能还真的是需要这种note agent的框架的。",
      "speaker": "发言人3"
    },
    {
      "time": "01:11:28",
      "text": "我可以其实理解就是当你用multi I agent，其实相当于说我还是把人类的先验知识放进去了。你要做一个这样project，你需要有一个product major的，你需要有一个engineer，我人为的把它给分做了一个任务的拆解。但是如果说我们说要看的更有LM，他有自己更他自己可以拆解的比比我们能够拆解的更好，那也许那个时候就不需要猫推枕了。",
      "speaker": "发言人1"
    },
    {
      "time": "01:11:53",
      "text": "对我觉得这个是合理的。因为本质上我们讲prom的话，其实就是把一些人类的先验知识给它放进去。去年我们就不断的在谈大家怎么做from engineering。大家还有非常复杂的prom cookbook，不同的场景我要用什么魔法prom，然后ChatGPT才能表现的比较好。但今天的话你即便是用一些开源模型，你会发现你就你想怎么问就怎么问就好了，你只要把你想做的事情给讲清楚。其实就OK，那猫TA枕的话，其实我们只是把一部分分解的一些任务提前给做好。",
      "speaker": "发言人3"
    },
    {
      "time": "01:12:29",
      "text": "但是很多很多一些难题的话，也都是交给这个large land model去做。比如说planning这一件事情的话，其实难度其实还蛮大的，但是其实看起来垃圾。那个model今天做的还挺好，说不定他做的可能比人还要好。如果像更强的语言模型出来之后的话，可能这个语言模型就能把这件事情给包办了。所以像3 altman说，如果GPT5出来之后，可能很多创业公司又不复存在，我觉得也不是不可能的一件事情。",
      "speaker": "发言人3"
    },
    {
      "time": "01:13:03",
      "text": "看来我们要赶紧在GPT five之前把这个发了，在GPT five之后可能就他都要再有变化了。其实刚刚铁证提到就是你我觉得你刚刚用这个比如前端框架做一个类比挺有意思。就是在一开始我们有很多前端框架，尤其是新的技术出来以后，你会不停看到有新的前端框架出来。那那到最后我们看到整个的行业肯定是越来越去越来越是收敛的那如果说我们想要从别的这些框架的眼镜中学习到一些东西的话，那最后能够占据比较主流位置的这些框架也好，这种类似的这种process他们是做对了什么呢？这些对于我们现在这些想要做一个framework的这些开发者或者公司，有什么经验可以学习吗？铁证有没有什么可以分享？",
      "speaker": "发言人1"
    },
    {
      "time": "01:13:52",
      "text": "我觉得这个就是特别好的问题。其实我也是想学习一下，比如说千万的一些经验，看千万做的这么成功，然后这里面俊阳已经做对了非常多的这个事情。我觉得这里面应该是有很多很细节很小的东西。不是说我们能够快速的建立一套大的方法论，然后照着这个复制一定就能成功。",
      "speaker": "发言人4"
    },
    {
      "time": "01:14:15",
      "text": "根据我之前前端开发的这个经验，我感觉就是这些前端框架到最后剩下的这几个，其实它都有一些共性。就是说它里面这些前端框架都有大量的成功案例，然后甚至是开源社区就有大量的代码。比如说我现在说我想要用绿写一个博客，我去给到B搜，我能找到很多现成的项目，然后拿过来稍微改一改就OK了。那我可能是整整体上来讲，我不需要学律，不需要学这个框架或者怎么样。我看他的代码，我简单改一改配置，改一改logo，那我就能做一个我自己的网页出来。可能我花十分钟都不到。我觉得这个对于新用户的adoption实际上是非常的重要的。快速运动的就能拉新。",
      "speaker": "发言人4"
    },
    {
      "time": "01:14:57",
      "text": "现在的这些H的框架，刚才陈阳也提到，有一个做writer的项目，其实也要有自己的agent框架。他自己就是自己的一个一个showcase。他知道说怎么样他他把这个书可以做出来，其实他就能遇到很多现实中的问题，然后把他的这个框架做的更加的完善，我觉得这是非常重要的一点。",
      "speaker": "发言人4"
    },
    {
      "time": "01:15:17",
      "text": "然后另外一点就是说当这些用户从show case里面快速搭建一个玩具的时候，他其实希望不停的把自己想要的新功能加进来。那这时候就要看到它这个框架它的设计是不是足够可扩展性。就是之前我们做follow那个框架的时候，其实当时我们自己分析下来，之所以做的不如拍套是好的一个原因，也是我们的这个可扩展性不足。我们主要面对的是，比如说google的一些使用场景。对于外面发生了什么，就是对于社区里面发生了什么，大家的这个呼声反响不够热烈。但是因为它的这个框整个框架的设计实际上是非常挺吻合的。所以如果你就是goole的这个社区的开发者想要进来贡献一个新工能的话，他要改的地方非常的多，而且也很难去去推动这个事情。就是我觉得如果说大家想要做一个这种agents的框架，那么如何能够设计一个比较好的架构，让不仅自己的这个需求能满足，并且能很容易的让社区的这些新的能力也加进来。",
      "speaker": "发言人4"
    },
    {
      "time": "01:16:18",
      "text": "然后有一个很好的社区环境，让大家在这儿一起能讨论，然后能达成一定的共识，然后往前推进，这个是很重要的。然后第三个我感觉非常重要的一点就是，需要有大量的人去创造大量的资料。就是比如说大家去买东西也一样。如果发现我搜到所有的这个品牌的商品全都是这个公司来讲，那我可能天然的就对这个公司失去了信任。因为我觉得所有我在网上能搜到的东西，无非就是他的广告。但是如果我去搜这个东西，那我先搜到的可能甚至都不是这个人这个公司的自己的一些材料，而是这个买家秀。然后是很多社区上很多人在说这个东西做的非常的好。我觉得这种可信度就会高很多。",
      "speaker": "发言人4"
    },
    {
      "time": "01:17:00",
      "text": "所以我感觉就是放到这个框架上来讲，是不是让这个框架能够被大家接受，然后大家喜欢，然后大家就天然的产生一种信任感。是说有没有更多的人，就除了你自己能够去掌握你的框架。讲你的框架，讲他的一些不足，讲他的一些好处，然后讲自己的一些经验。我感觉不管是困还是VOM在这个方向其实做的也都是蛮好的。可能也是今天他们这些框架非常成功的一个原因。",
      "speaker": "发言人4"
    },
    {
      "time": "01:17:29",
      "text": "铁证不愧是开源的OG，我觉得讲的非常好。铁证讲的很清楚，也就是在你自己设计这个的时候，是一种不要居高临下，我教你做事儿的这种心态。还是说真的是把这些开发者，把你的用户放在第一位去设计。我觉得我我觉得非常我相信所有对于不只是开这种框架，我就相信对于所有要做这些开发者工具的的公司，应该都会很有启发。",
      "speaker": "发言人1"
    },
    {
      "time": "01:17:54",
      "text": "我们回到在这个开源社区里面非常核心的一个问题，开源大元模型。就是这个铁证应该是几个月前都要分享的一篇一篇报道，就是说booming of the chinese speaking l ms我觉得这个铁证可以跟大家简单的介绍一下。现在你看到的在这个hugging face上，L这个开源打LM的整体的情况。你觉得过去这过去这一年一年多，我觉得你觉得看到的比较重要的一些变化和进展是怎样的？",
      "speaker": "发言人1"
    },
    {
      "time": "01:18:29",
      "text": "好像没问题，我刚刚看了一下汉英face上面开放的这些模型，就是所有public的这些模型和数据集。数据是这样，就是现在我们大概是有66万个模型，然后148000就是接近15万个数据集。然后如果算上private可能还没有release出来，或者是一些闭源的模型和数据集的话，那这个数据可能还要再再翻翻个倍。然后这个数据相对于我加入公司的时候，其实已经翻了不知道多少遍。我记得我刚入职的时候，我写过第一个PPT，然后就是讲还上有多少个模型。当时好像是15万个，现在是66万个，大概翻了四倍，然后数据集也是翻了非常多倍。然后我感觉整个社区其实是发展的非常的好的。",
      "speaker": "发言人4"
    },
    {
      "time": "01:19:13",
      "text": "Chat b刚出来的时候，我甚至还在想说，大家都用ChatGPT，还会有人去去开源这种模型，或者说未来可能只有几个巨头能开源？这些中小开发者其实还有什么样的机会，或者说谁还会在汉尼飞的上传很多的这个模型。其实现在我们看到就是这种微调技术的发展，还有比如说model merger这些quantization，甚至不同format之间的转化，这其实都给开源的开发者非常多的机会。比如说妈妈，它其实不仅仅是meta一个人发的这个lama模型，其中大量的模型可能是各种语言微调的lama，比如说最近的chinese拉玛，然后还有就是各种lama的quoniam ation，甚至是GGUF，就是各种不同的格式，MOX的这种格式。所以其实这个开源社区是非常这样的活跃的。",
      "speaker": "发言人4"
    },
    {
      "time": "01:20:01",
      "text": "然后最近看到一个非常有意思的趋势，就是去去年初的时候，那个时候可能中国的这个社区参与度，其实就在大语言模型这个方面，大家还都是比较懵逼的这个状态。然后参与度比较少，后面慢慢的就有很多国内的大医院模型，开源的就出来了，做的非常不错。但是那个时候大家遇到一个特别大的问题，就是数据集不足。就有很多开源的中文模型，但是并没有很多开源的数据集。",
      "speaker": "发言人4"
    },
    {
      "time": "01:20:30",
      "text": "我感觉经过过去这一年多的发展，其实开源数据集的领域我们也是做了非常多不错的工作的。比如说你现在今天去看那个韩英的上面data，排名前两个的数据集都是国人做的数据集。一个是泰康B他们做的MMLU pro，就是一个新的MMLOU的benchmark的data。然后另外一个是map做的这个matrix，它是从cop coral和其他一些数据集里面筛出来的一个非常高质量的中按数据集。所以我发现这个中国社区其实在这方面长得非常快。",
      "speaker": "发言人4"
    },
    {
      "time": "01:21:03",
      "text": "然后最新的就比如今年的一个趋势，就是说我看到有很多国人驱动的这个社区，国人驱动的国际合作的一些组织或者项目出来了。就是我觉得open David和这个BOM都是一个非常好的例子，除此之外还有比如说open video，或者是像刚才我们提到MMAP，它就是一个虚拟的researcher之间一起合作的组织。都是有有非常强的战斗力。所以我就看到有这么多非常好的工作。",
      "speaker": "发言人4"
    },
    {
      "time": "01:21:30",
      "text": "然后那个时候正好我们之前在韩就是之前在韩国的这个同事的后来辞职创业了，他在首尔办一个小的conference。然后我就说好，我就问他说去讲点什么？他给我的一个建议就是讲一点anti intuitive的这个事情可能大家会比较感兴趣，就是反常识的这个事情。我就想其实就是当大家提到大语言模型的时候，大家的第一个反应就是妈妈就是可能google提供的这个芝麻。当大家可能对中国的这些贡献并没有那么的了解，那我就说OK我就做一个这方面的演讲，正好去去宣传一下。然后在做这个PPT的过程中，我就发现其实整个中文的大语言模型的社区发展的是远远比我快的。然后我当时做了一个表格，就是通过三个类别，去把不同的中文的基座模型和他们的衍生模型都都列了一下。然后在我做那个表之前，我都没有想到说我们已经有这么多中文的大语言模型，还有各个领域微调的模型出来了。",
      "speaker": "发言人4"
    },
    {
      "time": "01:22:41",
      "text": "然后那天去现场去去讲的时候，还问了他们一些life question。比如说你们听过哪些中文的大语言模型，因为韩国就在我们旁边，然后呃其实还是比我想象的要好一点。然后大家知道什么百川、千问，然后大家当时大家可能对deep sik的认知还比较少。然后呃就是后面大家也问了很多非常有意思的这这个问题，然后我就感觉其实我们应该多出去讲讲。",
      "speaker": "发言人4"
    },
    {
      "time": "01:23:13",
      "text": "我觉得俊阳做了非常不错的这个工作。但是作为整体，我觉得其实我们在海外的发生还不是特别的多。我感觉这里面其实很多人对中文，就中国的这个模型行业的发展，尤其是中国的这个模型保持这种开源开放的这种心态，然后去去分享很多新的技术。比如说dept最近在paper里面详细的讲了MLA的这种新的技术，其实还是有很多自来粉。那那我们也希望说能够在海外造成更多的影响，甚至是一些破圈的影响。",
      "speaker": "发言人4"
    },
    {
      "time": "01:23:51",
      "text": "非常感谢这个铁证的分享。我觉得我们也很需要更多像你和像俊阳这样的这样同学在一个中国界社区上去去跟大家分享中国的进展。刚才提到有很多从数量上的增加。刚刚提到的那个几十万的这个大模型里边大概有多少是开源的，有多少是闭源。你怎么看开源里边几种开源的这个方式有没有什么是比如说hand face，或者说你自己个人比较推崇的。",
      "speaker": "发言人1"
    },
    {
      "time": "01:24:21",
      "text": "没错，就刚才我们提到说在hung face。你今天去看那个hang face网页，你在上面点那个models的，它就会给你一个数字，就66万个这个模型。这些模型都是开源，就是你点进去每一个模型，你都能看到它的这个模型的权重，然后模型的model card，然后有一些是闭源，但是不是在这66万里面，这个数据是我们内部才能看到的。但是我没有这个权限，我不是root所以我也不太清楚。但我知道说大概就是算上闭源的话，可能超过一个million的就100万个模型。",
      "speaker": "发言人4"
    },
    {
      "time": "01:24:52",
      "text": "然后在这些开源的模型里面，其实它也是有不同层次的开源。就是我感觉开源不是一个开源包闭源这是一个非常对立的角色。其实开源到闭源之间有一个非常漫长的光谱，就是我们从最边上这个闭源开始说，其实闭源模型它自己也有一个开源的生态，因为他把这个API放出来，那我们围绕刚才我们谈到的这个agent，这整个一系列的开源的框架，最下面的基座全都是check GPT。那这个其实它也是有一定程度的开源的影响的。OK那我们从那边再往往过来说，那我可以就是说开源的话，我只开一个模型的权重，就是现在我们叫它open service，就是开放权重的这个模型。这种模型你可以使用它它就像一个点EXE文件，你在网上下来之后，你双击一下它就可以跑了。",
      "speaker": "发言人4"
    },
    {
      "time": "01:25:38",
      "text": "但是你想对这个模型是怎么来的做一些了解，其实你是不知道你不知道这个模型用了什么样的数据，用了什么样的训练时候的技术，有些模型会发一个technical paper，会简单讲一下它的这个技术。也有模型可能写的很含糊，然后就一笔带过。OK比这些模型在开源再进一步的他就可能会开放更多的信息，包括他的数据集是怎么来的。然后他可能给你一个脚本，然后你可以把这个脚本一跑，然后你能大概知道说他这个数据集是一个什么样子，然后比这个再进一步了。那可能就是他他不仅说我开出去就是让你知道我有这么一个开源，他他其实是希望让你手把手把这个商业模型做出来的。他就会不仅告诉你说他为什么他做了什么，然后他还要告诉你他他为什么这样做，把它当成一个课程，然后把这个东西手把手包教包会。但这种模型其实一般性能上会比后面那几种要差一些。因为如果这个真的是有一些非常独到的技术的话，那大家可能不会说把这个东西直接告诉自己的竞争对手。",
      "speaker": "发言人4"
    },
    {
      "time": "01:26:47",
      "text": "然后就是比这个比刚才我们说的这种再开放一点，它可能就是一个开放合作的组织。像我们的这个被扣的，其实或者说不论，最早的这个大语言模型就是它是一个group，然后每个人都可以进去，进去之后你可以看到说现在大家在做什么，然后进去掺和一下。比如说你可以做一点数据集的工作，或者说你对这个模型的微调有一些想法，你都可以贡献你的想法。所以那个paper出来之后，你会看到前三页全都是各种的作者，就是基本上参与的都都写在了一个作者栏目。所以我感觉整个开源VSB源其实是一个非常长的光谱。然后每个公司根据自己的一些选择，然后根据自己的实际情况，去决定说他想要在这个光谱的哪一个位置。我觉得我们说不说必然就是只要能能开源的这些，我觉得都是非常优秀，我们也都非常的欢迎。",
      "speaker": "发言人4"
    },
    {
      "time": "01:27:43",
      "text": "然后我们也理解说，很多企业其实出于各种原因。比如说他没有公开数据集，可能是版权上的保护的一些原因。然后，他没有把自己的这个take完全写出来，是出于防止这个竞争。就比如说open a他也基本上也不写他的新的技术它它里面的一些东西，它只是在technical paper里面简单说一下它的这个架构。所以sorry，说完之后它的架构是怎么样，大家到现在还在猜，我们也都是非常理解。然后我觉得这个真的是要根据各个公司的情况，然后它的战略的重心来决定自己的定位。当然你选择不同的开源的这个程度，社区的反馈可能也会相应的不一样。",
      "speaker": "发言人4"
    },
    {
      "time": "01:28:23",
      "text": "对前提到说的确中国的这些中国的这些开源模型有就从数量上有很大的一个一个一一个提升。我好奇你有没有大概这样的数字，从数量之外你觉得其实中国这些模型你觉得还带了哪一些不同其他的一些贡献是值得大家关注的。",
      "speaker": "发言人1"
    },
    {
      "time": "01:28:43",
      "text": "绝对的这个数量我倒没有一个非常好的统计，因为其实这也差不多有广告。就是如果大家传模型到海运face的时候，它是有一个tag，然后在那个tag上你可以写上你的模型是支持中文的，这样方便我们去统计。但现在其实很多模型并没有加上这个tag。所以当你在里面搜这个文本生成类模型，然后能说中文的文本生成类模型，这个数量还是比较少的。我现我我严重怀疑这个数据是被大不大低估然后我。",
      "speaker": "发言人4"
    },
    {
      "time": "01:29:11",
      "text": "我插我插问一句，就是我看你那个report里边其实引用说好像是lotus这个杭杜特的一个一一个报说中国出现了两百好两百多个。我看好像在百度好像也有一篇文章说，去年中国就两百多个这个模型，是不是？但显然跟几十万这个统一口径是有差别。我好奇这个怎么样算，如果算严格定义的话，到底怎么算着算算在这个数字里面。",
      "speaker": "发言人1"
    },
    {
      "time": "01:29:38",
      "text": "明明白明白，工作社的那个报道实际上比较早，那个是二三年5月。然后他说的也不是开源模型，他说的是所有的大元模型。然后说就是到二三年5月，中国已经发布了19个大语言模型，然后美国发布了18个人几百个模型。我觉得那个可说法可能来自于百模大战。就是大家有有非常多的这个模型在在竞争，但这个数字其实都跟那个66万这个量级对不上了。我是这样理解的，就是当我们说一个模型的时候，有可能我们说的是这个模型的系列，也有可能说的是一个具体的模型。",
      "speaker": "发言人4"
    },
    {
      "time": "01:30:14",
      "text": "就拿千问为例，千问就是在这个百模大战里面，千问只能算一个。但是在high face这边，千问及其衍生品我觉得可能得有上百个模型这么多，就包括千问自己发的，从这个千万一然后到千万1.5，每一个大的类下面都有很多很多模型。比如说110B70B然后什么不同的condition ation，那这个就是分支分下去其实他是有非常多的模型。然后除此之外还有社区去做的，比如说dolf他们做的这个千万的微调，这也算是千万基础模型，就是一个衍生品。还有很多比如说大家做这个GGUF或者说做AWAWQ coalification，都会让千万家族发展壮大。",
      "speaker": "发言人4"
    },
    {
      "time": "01:30:58",
      "text": "这其实也就是开源的一个精神，就是把我擅长做的事情做了，然后社区自然有啊别的人，他可以从他们的角度去让这个社区不断的变得更加强大，然后大家去进更深度的去去进行合作。所以怎么理解说一边看到可能只有百这个量级，另一边看到已经有几十万的这个量级。那那中间是就是要考虑说你这个模型的衍生品，还有整个模型的范围大概有多大。",
      "speaker": "发言人4"
    },
    {
      "time": "01:31:23",
      "text": "对，刚才铁证讲的很好，也解释他那个开源模型的数量为什么这么大。然后这里边数字方面的差异，我解释一下千万这边的情况，我其实自己都没有具体去算这个数了。因为我在知乎上面有人说我们是劳模，因为我们那个开源的size非常多。我们现在光1.5的系列的话，其实算下来就有八个size language model，再加上一个code queen就相当于九个size。然后每个size的话，我们基本上会发他的base模型，以及是他的chat模型。因为一般比较资深的人的话可能会去用base模型去做微调，他不希望在chat模型上面去做微调。然后在这个基础上我们会去做量化，一般会去做GPT q和EWQ。这里的话又多出两个模型，然后这几个月的话这个GGUF的用户其实会非常多。",
      "speaker": "发言人3"
    },
    {
      "time": "01:32:19",
      "text": "因为三大框架里边的话，很多人喜欢本地跑模型的时候，最方便的方式就是一个GGUF拉下来，用拉马达CPP去跑。很多人会发现说拉马达CPP其实我是不是还得学C加加？然后大家望而却步。于是就有像LM studio这样的应用，以及像欧拉玛这样的应用，能让大家非常方便的以一行另一行的方式，或者是有一个图形化的界面可以去运行大模型。所以我们又会去给大家去发这个GGUF的模型。然后这个GGUF的模型下面又有不同精度的量化，一般我们会做234568 bit的量化，这样的话就。整个模型数量就会非常多，算上很多，因为现在有很多人在find to我们的模型。刚才杰森其实提到了dolen，every half他们做的非常好，然后跟lucas atkins他们一起去把我们的模型，然后其实还有很多的人在反映我们各式各样的模型。有些人的话去做一些margin又有一些新的模型，我估计现在应该是有几千个。",
      "speaker": "发言人3"
    },
    {
      "time": "01:33:24",
      "text": "对这里想插一句，其实就是有很多做开源模型的公司或者说组织。其实当他们想要去衡量自己的模型畅不畅销的时候，其实很多人会去看这个访问量或者下载量。其实hand face这个下载量没有防止大家作弊或者说过于关注下载量。他看的是过去一个月的下载量，所以之前的很多信息都都丢失了。其实这种麦克就是cut up上这个麦克的数量，一定程度能反映模型的畅销程度。但是其实也很容易被也很容易刷，而且这上面有很多bias。",
      "speaker": "发言人4"
    },
    {
      "time": "01:33:56",
      "text": "我觉得真正靠谱的能够衡量一个模型是不是在社区已经产生足够的社区影响力的。就是去看他有这个模型的家族有多大，他有多少个衍生品，有多少人拿它去微调，有多少人拿它去做各种各样的模。比如说刚才俊阳说在安徽上搜这个千万已经能看到几千个模型。我觉得这是一个非常大的数量。我们可以去再去对比别的这个模型的家族，然后用这个去做一个另外一个角度的这个模型畅销程度的评测，我觉得会是非常有意思的一个事情。",
      "speaker": "发言人4"
    },
    {
      "time": "01:34:27",
      "text": "对，这个角度非常有意思，我觉得很期待看到这个更真实的更真实的一个反馈，谢谢铁子。补充，看你的这个report里边，其实你也会把这个模型分成不同的arch。比如说你是基于某一个模型，比方你这里举的例子是老马你基于某个模型这个权重来做一个fine time，还是说你自己真正retrain form scratch，还是有其他的这种完全不同的这种。就你觉得这三类在整个大语言模型的设计中，它的地位是什么呢？我们在强调说比方说中国的这个混合公司模型能力的时候，是不是还是pre chain这个方式是最能够show muscle的这种方式。",
      "speaker": "发言人1"
    },
    {
      "time": "01:35:12",
      "text": "对我当时大概分了三类。第一类就是在meta发布了这个妈妈模型的基础上，直接去做find team，去增强它某一个领域的能力，或者说它增强它中文的能力。然后第二类就是说我还是用妈妈的这个架构，因为用妈妈架构实际上是非常占便宜的。妈妈还是到目前为止最大的一个march with model的这个family所有的下游的info包括vm其实都为妈妈的这个架构做了非常多的优化。我觉得举一个非常明显的例子，就是现在district的这个MRA出来了。但是我相信楚汉他们可能还没有对这个MRA的架构做一个优化。而且这个优化可能是比如说几个月之后，我们才能看到一个非常成熟的优化出来。",
      "speaker": "发言人4"
    },
    {
      "time": "01:35:54",
      "text": "所以有很多很好的模型，因为他选择了一个独特的架构，这个架构非常先进，导致看社区用起来还是有有也有很多模型会说，其实拉马架构已经足够好，我没必要去探索更好的一个架构。我就直接用拉马的架构，我觉得这是也是一个非常合理的选择。那他们会选择从头预训练，然后去喂一些自己的清洗过的饮料，这是第二类。",
      "speaker": "发言人4"
    },
    {
      "time": "01:36:22",
      "text": "然后第三类，就是说自己创新的一些新的架构，当时大概分了这三类，然后怎么去看待这三类呢？其实是这样，就是这个表格之所以以lama为中心分三类，是因为lama还是目前最大的模型的社区。如果有一天是千万或者是deep seek，或者是这样变成最大的那我觉得我这表格可以重新做对吧？我可以是以是不是用千万这个模型结构来来排这三类。第一类是千万直接用千万来翻听的，第二类是retrain的，从千万的架构pretend那是其他的架构，然后把拉马就分到其他的架构，就大概是我我我是这样的一个分法。",
      "speaker": "发言人4"
    },
    {
      "time": "01:37:01",
      "text": "然后我觉得对于大语言模型来讲，就是看这公司的主要业务或者是诉求什么。我觉得如果是一个业务导向的公司，那其实无所谓用什么样的架构，只要能很快的满足自己的这个业务需求就可以了。比如说现在我看有很多人拿这个妈妈三去简单调一下中文，然后发现那个能力还不错，然后就直接上。然后再再用什么双控妈妈或者是其他的什么药这些技术扩展一下它的contact list，然后就可以直接上线。那那我觉得这个也很好，因为能很快的解决公司业务的一个痛点，没必要在这个地方花费太多的成本。",
      "speaker": "发言人4"
    },
    {
      "time": "01:37:40",
      "text": "也有一种情况就是这个可能是一个基础模型公司，他想的可能更远。他不仅说我要支持中文，我可能还要支持很多东南亚的语言，我要支持非洲的很多这种小语种。这种情况下可能是需要拿更多的语料去进行或者是continue to或者是单群，然后让他学到更多更多的这种专门的这知识，效果会更好。然后也有些公司可能就是从节约成本，或者是探索新的领域的方向，他可能会选择自己创造一个新的架构，然后去往前发展。所以我个人是其实是不是特别支持提这个所谓的妈妈talk。不管你选择哪一条技术的路线，就是适合自己的就是最好的，没必要是因为说这个公司用妈妈去去解决技术技技术问题，花了十块钱就解决了人家10万块钱的问题，就去鄙视这个公司。我觉得怎么样符合这个公司业务要求最好。",
      "speaker": "发言人4"
    },
    {
      "time": "01:38:45",
      "text": "当然就是说宣传的时候可能要提到说自己是基于这个妈妈饭厅。因为你要注意这个meta它的发布的时候，其实它是有一些限制的。然后包括现在比如说你去申请这个啊妈妈三的这个权限，如果你说你的国家是啊就非美国的话，可能也会遇到一些挑战，所以这方面还是要注意一下。",
      "speaker": "发言人4"
    },
    {
      "time": "01:39:06",
      "text": "我正好就想问一问俊阳，就是我看到在铁的这个分类中，其实你千万别分在用这个other architecture，就是其他的这种架构。我好奇当时俊阳你们是怎么考虑这样的一个选择呢？因为就像帖子说的，你如果用这样的一种方式，可能就是面临你需要额外的一些社区的支持，对吧？在原来数据就是你就不能够直接去用了，可以分享当时你们的一个考量。",
      "speaker": "发言人1"
    },
    {
      "time": "01:39:32",
      "text": "这个就讲到我们这个千万发展的故事。其实是这样，我们的模型架构的话跟拉满的差异其实是非常小。比如说我们算上token lizer的话，我们的token ized不太一样。我们自己做的是基于GPT过的token ize的话去做扩展，做的token ized。然后模型上面的话，其实是在transformer上面的话，有一个UKB的这个bias，大致是这个样子。但是我们其实是经历了一些痛苦，这里我其实是想响应一下刚才铁证提到关于拉马以及是说这个资源架构的这类的问题。",
      "speaker": "发言人3"
    },
    {
      "time": "01:40:13",
      "text": "我们最早做这个模型还没有开源的时候，其实探索了很多种可能性。因为今天其实基本上你要做一个大语言模型，无非你还是基于transformer来做。那transformer的话无非就几个板块，一个是attention，一个是MLP。然后内部的话基本上这个mlp的话就是看你的计划函数。其实我们最早的方案其实跟今天解码的是更接近，我们用的是gaw的这个计划函数。但是后来其实多做了一些实验之后的话，会发现说如果用多一个楼的话，其实训练起来会更快，效果的话还能稍微好那么一丢丢。所以反而是做了一圈技术探索之后，回来变成跟这个拉马非常接近的这个样子。",
      "speaker": "发言人3"
    },
    {
      "time": "01:40:59",
      "text": "然后今天大家在谈这个拉玛，这其实也有人非常激烈的反对这个词语。如果大家知道google，原来google现在是rica的创始人一K他其实非常反对这一件事情。因为这个architecture的首创其实反而是pum就是lama。它是主要follow google的成功经验，然后训出非常好的模型。但是拉玛在今天的open source community的话非常的火，所以今天大家叫他这个lama detection。从EK的说法来说，就是今天的这些new be根本就不知道说这个model architecture is invented by google。",
      "speaker": "发言人3"
    },
    {
      "time": "01:41:39",
      "text": "这个是相应的背景。我们其实经历的痛苦是什么呢？是我们以自己的方式去写我们的代码，而没有follow hugin face practice写拉马的方式去写。因为hugger face提供了一种很好的模式，叫做trust remote code。他的意思就是说你可以自己写代码，但是你不需要merge进any face的代码。主库里边你只需要保证你这个东西能跑起来，你解决好你的用户的问题就好了。其他的问题与hunting face无关，hanging face的官方维护不会来考虑你前文的模型。",
      "speaker": "发言人3"
    },
    {
      "time": "01:42:17",
      "text": "这个的话其实对于很多三方框架的适配的话，就会带来非常多的困难。比如说像罗汉这一边，VRM我相信在做我们地板的这个适配的话，VRM还是要做不少模型方面写代码，去理解我们写的代码的相关的工作。这里的话其实就可以看到说你的这种工作的话，反而会给很多人带来一些extra的effort。所以今年年初在复盘2023年的时候，我的观点反而跟国内很多社区的人讲的很多不一样。大家喜欢讲什么自研这一类东西。",
      "speaker": "发言人3"
    },
    {
      "time": "01:42:57",
      "text": "我觉得既然拉玛探索出一个非常好的model architecture，那今天大家在选择使用transformer的时候，拉玛architecture就是一个更好的一个选择的情况下，为什么不基于他去训一个更好的模型？那基础模型的公司其实也是一样的，fink of scratch也好，还是你继续训也好，大家其实最终关心的是你这个模型够不够强，拿出来好不好用。我觉得最终关心的是这个点。所以如果让我再重新做一次的话，我可能会更加积极的去拥抱开源社区和拉马的发展。这样的话我觉得对于快的推广的话，我会能做的更容易一些。其实快1.5其实就是做了这一类的工作。",
      "speaker": "发言人3"
    },
    {
      "time": "01:43:44",
      "text": "因为当时我在调研，为什么困在海外火不起来？就是因为国内的用户我们其实是比较容易拿到的。因为我们开源了比较多的模型，我们的模型质量，然后中文比较好，很多人其实就会用。但是在海外的话，我当时是先去了香港去讲，然后再到新加坡去讲。那就发现很多人就甚至没有听过换这个名字。然后当时跟every harper聊，他其实就提到说我甚至都跑不起来。",
      "speaker": "发言人3"
    },
    {
      "time": "01:44:14",
      "text": "你这个东西又不在哈根face的主库里面，看起来就会像是一个非常山寨的东西，我就不想用。所以铁证这边帮了我非常多的忙，包括跟hand face这边协作做做完我们的快的代码进hand face的主库之后的话，其实就到佐汉这一边VRM，然后把我们的新的模型给适配上。这样的话基本上这个生态的局面的话就相对来打开。所以我得出的经验是说，做这个大模型的公司的话，我觉得最终还是更加关注你的模型质量的本身，以及是你这个模型好不好用这个问题。把这两个事情解决了，我觉得其实就OK了，其他的话我觉得倒没有那么值得关心。",
      "speaker": "发言人3"
    },
    {
      "time": "01:44:59",
      "text": "那正好也问一问这个铁证，你们首先非常感谢你为国人的这个大模型做了这个贡献。这样也提到了就是你在中间的一些一些learnings。那从铁证角度，你觉得对于这个有什么补充的吗？我顺便也可以说一说，那那相比起我觉得像mystery，国际上非常一线的这种也是有一个开源模型的一个家族，我们还可以从他们身上学到什么。我们现在可能看到一些差距。",
      "speaker": "发言人1"
    },
    {
      "time": "01:45:31",
      "text": "还是对这个我其实大体同意俊阳的说法，其实我觉得001的那个战略就很聪明，就是你走妈妈的路，让妈妈无路可走，因为妈妈已经把整个生态建好。但是我感觉现在这个局势其实又有一些变化。我觉得跟我们当年聊的时候还不太一样。现在我感觉社区其实更能够愿意支持一些非拉马架构的那这里面可能有有几个原因，一个可能是说大家在继续用拉拉A架构的时候，看到拉马架构的一些限制。它有它的这个潜力可能不多。比如说最近deep sick他用MLOA的技术能够把这个protocol Price降的非常的低。那这个里面就会给大模型公司一个新的玩法，就是它的开源和他的商业就非常不冲突。",
      "speaker": "发言人4"
    },
    {
      "time": "01:46:19",
      "text": "然后我感觉这里面就是我们未来去探索这个模型新的架构的时候，其实可以有很多新的玩法出来。就包括刚才我们提到的miss o其实他他当年就用了这个MOE的架构。应该是第一个就是得到大规模使用的MOE大元模行。这个其实就创造了一个新的热点，然后大家就有必要去去关注，去了解，然后去去尝试去使用，慢慢就会转化成一个用户。",
      "speaker": "发言人4"
    },
    {
      "time": "01:46:45",
      "text": "比如说现在我们想要说训练一个妈妈架构的模型。其实你很难说我可以在各个方面全面超越妈妈3。但是如果你换一个架构的话，你可能从成本或者说从未来对VR的支持上，可能会有一些不一样的点。然后像比如说现在这个的下游的这些系统，不管是阿玛尼CP还是VOM，其实我觉得都很愿意去支持并且尝试一些新的架构。所以我我感我倒是觉得从今年下半年开始，我们可能会看到多非拉马架构的这个模型出来说。",
      "speaker": "发言人4"
    },
    {
      "time": "01:47:26",
      "text": "你们从这个info角度，你有看到您要靠类似的趋势吗？",
      "speaker": "发言人1"
    },
    {
      "time": "01:47:30",
      "text": "对我觉得非常同意刚刚两位的观点。就是我觉得首先第一个我想说的是即使是沿用lama本身的结构，我觉得也没有什么不好的。因为是因为我觉得还是要从需求出发，并且训练一个大模型，大语言模型本身也不光是这个模型结构的事情，很多是关于你的数据，还有你的这个算力。然后你到训练多久，你在训练过程中需要做什么事情。模型其实模型本身的架构只是大模型整个训练过程当中非常小的一部分。所以我觉得如果你没有什么特殊的需求，你即使是沿用拉玛的结构，我觉得没有什么不好的这是第一点。但是第二点，我觉得大家现在有对lama有很多的改动，我觉得是看到了很多lama本身的限制。",
      "speaker": "发言人2"
    },
    {
      "time": "01:48:07",
      "text": "然后比方说第一个比较大家最近比较关注的一个是MOE。就好像刚刚铁震提到的对，就是可能在这去训练一个dance模型的时候，在训练的效率上本身可能会有一些会有一些低效。然后如果用这个mixture of experts，我能够用，我能够在更快更短的时间内训练一个更大参数的模型。这个可能会对于我在这在有限的计算资源的情况下训练一个更好的模型，能够得到一个这一点会有相当大的帮助，能够最终获得一个更好的模型。然后第二个更多的是可能比方说dance attention本身的限制。",
      "speaker": "发言人2"
    },
    {
      "time": "01:48:43",
      "text": "就比方说我想要做一个非常长的context，比方说one million甚至ten million的一个context。我怎么样？我如果用dance attention本身，它有一个针对于它的计算量，是随着你的输入长度的平方级别上升的。然后那你如果有比方说100万的那100万的平方是一个相当大的数，已经可能很有可能在现实interview的时候已经是没有办法了。那你我们可能就需要改动，作为对这个attention机构结构本身做一些改动。然后我是我觉得这样子的改动是这种必要的改动，我们是非常欢迎的。",
      "speaker": "发言人2"
    },
    {
      "time": "01:49:13",
      "text": "然后对于我们来说。我对于我们作为一个研究者来说，是一个非常让我非常exciting的事情。就是我们能够怎么样能够adapt我们的system，来更好支持这些更复杂的这种attention的这些结构。对然后在于对设计这个系统上来说，对我们也是一个非常有意思的chAllenge。我们怎么样能够保证我们这个VM的system general enough来支持这样各种各样不同的architecture，来让我们让大家更好的在同一个框架下能够使用各种各样不同的模型。同时让不同各种的模型都能够享受到VOM的各种feature所带来的performance的influence的performance的提升。",
      "speaker": "发言人2"
    },
    {
      "time": "01:49:50",
      "text": "沿用之前的model architecture本身没有问题，我觉得这个是一个非常好的事儿。然后有一些改动的话，我觉得如果是出于一个这种功能性本身出发的改动，也是一个非常有意义的改动。我觉得我们我们也会非常高兴来适配这样子的改动。",
      "speaker": "发言人2"
    },
    {
      "time": "01:50:08",
      "text": "对我觉得两位其实讲的非常好。就今今年从去年下半年开始到今年，有很多新的架构在涌现，或者是它在变得更加的流行。像最近捐，之前还有像RWKV，现在整体还有一个复古IN的这个趋势在这里边。",
      "speaker": "发言人3"
    },
    {
      "time": "01:50:29",
      "text": "所以我觉得其实还有挺多新的架构，包括MOE也好，其实像myo他们做的这个探索做的非常好。比如我本人早年是做MOE的，但是当时我们没有定义好说这个MOE应该用多少个expert，然后你激活多少export达到什么样的效果，这个事情没有定义好。所以很很长一段时间大家就觉得不去做这个事情了。但是像mystery的话，其实比较清晰的跟大家传递的一个信息。或者是说开源社区发现，你比如说激活14D的参数，你可能能大致达到训练一个28b dance模型的效果。那这样的话其实是比较划算的一个事情。",
      "speaker": "发言人3"
    },
    {
      "time": "01:51:14",
      "text": "所以我觉得很多新架构的探索，其实是是非常必要的。我觉得他们是不同的目的，就拥抱拉马的架构，往生态方向去走，提供solid的foundation model让大家能够用起来。这个其实对很多开发者，尤其是企业用户，他其实是非常的欢迎的那如果提出新的东西的话，我觉得可能对这个新技术的开发者来说，他主要的挑战是你怎么样让你的技术变得popular。这个是我们此前没有解决好的一个问题。",
      "speaker": "发言人3"
    },
    {
      "time": "01:51:50",
      "text": "但我看现在，因为大家对新技术更加开放，所以大家可以看到说今天有新的像MOE的东西出来，像新的MOE的架构出来。然后这些开发者会在生态方面做很多工作，不断的去做一些适配。我觉得如果新技术的开发者能够去帮助开源生态做好这一些适配的工作的话，然后大家都发现你这个技术有用，那我觉得这个领域能发展的更快。",
      "speaker": "发言人3"
    },
    {
      "time": "01:52:18",
      "text": "铁刚你提到你说这些新的价格在开始提出这个新的架构，你说对于商业化也提供一些新的思路，能够展开说说为什么？",
      "speaker": "发言人1"
    },
    {
      "time": "01:52:29",
      "text": "这可能是就是我个人的一个揣测，不一定准。换一换一个角度说，就是为什么我觉得马尔马架构未来会有可能会被妈妈架构模型出来，是因为马架构在这就是大家知道我可以用拉马架构宣传好的模型。但是大家什么时候会从一个好的模型换到另一个好的模型呢？是因为新的这个模型能给我带来完全不一样的感觉。不然的话我可能觉得就有的这个模型还不错，我知道怎么去用很很好的这个prom去去微调它。如果说所有的模型，阿马架构的模型，他们已经到了一个分水岭，就是我觉得这个模型基本可用了。那这时候出一个新的同样架构的这个模型，可能不会让大家有非常强的迁移的欲望。一旦我们到了这么一个点，现在竞争的就是所有模型都可用。",
      "speaker": "发言人4"
    },
    {
      "time": "01:53:18",
      "text": "那我们现在竞争的是什么？就是模型的成本。就谁能跑得更快，谁成本更低，谁能够部署在更多的设备上。那这时候怎么样降成本？妈妈架构的限制就在这儿，你的成本就是这个样子。你可能说我们有一个什么新的什么什么fast decoding或者是类似的这个技术，能够让成本降低。但这个对于妈妈也是一样，对于你用妈妈的这个架构也是一样，对于其他的这个模型都是一样。",
      "speaker": "发言人4"
    },
    {
      "time": "01:53:44",
      "text": "这是一个平均水位的一个变化，并不是你自己模型的一个能力的变化。你可以去更好的调模型，这是你自己模型的能力的变化。然后如果说大家都可用，那下一步竞争点在哪儿？就是说你有什么差异化？那你能差异化的点可能就是说你你你这个模型知道别人不知道的东西，或者说你这个模型可能对reg的支持特别强对这个指令追随的能力特别强，那这些可能是一个可以竞争的点。但我感觉老马三其实比其实已经很不错。",
      "speaker": "发言人4"
    },
    {
      "time": "01:54:13",
      "text": "那你想要在这个方面做出非常出众的成果，是有点难的那另外一个可以卷的点就是说我在达到妈妈3 70B的效果的同时，我可以做的成本比它更低。并且妈妈的这个成本的降低不会影响就是我我的这一块。那么你能做的点就是创造新的模型架构，让这个新的模型架构它跑出来的对对对，不管是GPU内存的需求，还是说你推理的延迟，都有阿马3追不上的地方。比如说你是沿ME架构，那你可能推理上就是比妈妈要快。那你如果用了MLA，那你可能就是比这个其他的一些架构的成推理成本要更低。从这个角度来讲，如果你想做开源的话，那探索新的模型结构是非常有必要的那这个会让你的模型更加出众。",
      "speaker": "发言人4"
    },
    {
      "time": "01:55:03",
      "text": "然后从另外一个角度就是回到刚才主持人提到这个商业的竞争的这个点。这是我个人的理解。就是说我们过去看这些数据库公司，比如说什么snow flakes database，像比如说Spark，它其实是有开源版本的。但是开源的版本并不影响它闭源的一些商业上的东西，为什么？因为它必然比开源跑得快。然后所有的这些优化只有它必然的版本才有。",
      "speaker": "发言人4"
    },
    {
      "time": "01:55:28",
      "text": "如果说我有一个新的模型结构，那我自己可以让它跑得非常快，我的API这个价格非常低，并且我有一个开源的版本。大家可能说需要慢慢去去研究怎么样让这个开源版本跑得更快。但是大家可以通过我这个开源版本了解到我这个模型的一些实力。",
      "speaker": "发言人4"
    },
    {
      "time": "01:55:44",
      "text": "当大家部署的时候，大家会想说，我就想用这个模型，这个模型太好。但是我到底是用我自己的部署还是买别人的API？发现我用自己的部署，所有开源的这个比如说VM还没有支持，这个东西还没有跟上的时候，我买API更划算。这个时候其实他开源的这个工作就是直接导致他的这个API的收入上涨。所以我觉得这是一个非常好的技术演进的方向。这跟过去开源项目怎么赚钱的这个模式其实也是结合的非常紧密的。",
      "speaker": "发言人4"
    },
    {
      "time": "01:56:17",
      "text": "就是这个LM的这商业化模型的商业化肯定跟以前我们看到这种什么database的数据库的商业化不一样。然后你刚才提到就是说原来我们看到现在这些做大模型的公司，他们商业化都是说我开源几个小的。让你秀一秀muscle对吧？让你感受一下我的实力，然后我再把最大的这个去去闭源。你的意思以后可能是说我这就是我闭园的那个商业化的model。可能它不只是一个performance更好的model，其实它可以其实是在一个通过架构的变化，其实是在效率等等方面都能够有一个更质的一个提升，是吗？",
      "speaker": "发言人1"
    },
    {
      "time": "01:56:54",
      "text": "对，没错，它商业化路径不仅是小的大的，就是这种路线也有可能是在infer层面的优化上，然后决定他这个商业化，info也可以帮助商业化。",
      "speaker": "发言人4"
    },
    {
      "time": "01:57:04",
      "text": "志阳你觉得你怎么思考这个问题？",
      "speaker": "发言人1"
    },
    {
      "time": "01:57:07",
      "text": "我觉得铁证刚才提了一个很好的问题。从我总结的角度来看，我觉得是一个关于开源和研发之间关系的这个问题。因为开闭源其实是一种选择，但是你的研发水平或者是看各个公司的算法以及基础架构能做到什么样的水位。开源的话毫无疑问它肯定是比这个内部是要跑的相对比较慢的。",
      "speaker": "发言人3"
    },
    {
      "time": "01:57:34",
      "text": "因为我我我这么解释，我有一项好的技术，我肯定要把它经过非常充分的验证，并且社区可能会比较欢迎我的情况下，我觉得他有可能火的情况下，再把把它推出去。然后再不遗余力的像我刚才说像跟三方做适配等等，让生态的话去接受我这个新的东西。这是从推广技术的角度上来说。但是我们的算法人员或者是我们的工程人员研发出一项新的技术落到我们的模型上的时候，一般来说我们肯定是优先自用的那像刚才铁证讲到的，比如说今天研发出一项比较能够降本增效的这个技术，那我用到这里边，那这里的话我就能拉开我的技术优势，通过，技术方案的优化来实现降本。那这个的话，其实反而是说，闭源的某一种优势。然后如果这个优势特别大的话，那绝大部分公司的考虑反而是可能就不将这个技术开源到这个社区里面去。",
      "speaker": "发言人3"
    },
    {
      "time": "01:58:37",
      "text": "当然他们也有也一定程度上有可能去开源。但从常规的逻辑上来讲，举个例子，比如说OpenAI。OpenAI开源的可能性大家都会觉得相对比较小我觉得它是一个开源和研发之间的关系。研发的话往往其实是要走的更快的。因为你更solid，更加考虑各方面的因素，都觉得OK的情况下，可能才会将它开源出去，这是我考虑的一个点。所以我觉得说今天商业化可以通过技术来实现降本。然后有一些公司甚至能获得盈利的话，我觉得这个并不是不可能。所以今天大家可能还会更加相信技术一些。",
      "speaker": "发言人3"
    },
    {
      "time": "01:59:22",
      "text": "刚才其实铁砧提到了一个话题，就是说我们先通常大家看要去要去探索一个新的架构。很可能是因为现有的架构，我们可能多多少少看到了一个提升的天花板。现在像拉马这种架构碰到了瓶颈，或者我们什么时候知道说这个碰到了瓶颈，而新的架构不只是一种雕花，而可能是一种真正的新的路径。",
      "speaker": "发言人1"
    },
    {
      "time": "01:59:50",
      "text": "对我觉得这个是很好的问题。就是架构方面的探索的话，其实我们的经验应该是先追上，然后再看到有什么样的问题。包括刚开始我们先追上之后的话，就会发现当时拉马兔还没有出。我们其实就会发现说你在推理部署的时候，其实这个multi had attention的话，其实是会给你的推理部署的成本会带来很大的提升。因为它这里的KP cache会非常大，那这样的话你可能就会选择switch到比如说像MQA、RGQA这样的技术。",
      "speaker": "发言人3"
    },
    {
      "time": "02:00:25",
      "text": "其实很多公司都会去想做这样的一些问题。这就发现原有的拉马架构有这样的问题。那拉马自己自身他们的团队也会去研究看说这方面有没什么问题，但有一些没有什么太大问题的地方，你就可能就不太需要去动它了。就比如说像这个计划函数的这个部分，目前来看的话，如果想做一个计划函数去击败schedule的话，是一个非常难的事情。",
      "speaker": "发言人3"
    },
    {
      "time": "02:00:51",
      "text": "随着这个技术的发展，包括整个开源社区的发展，大家会非常关心长序列这个事情。就比如说我们做ogan devo，我们遇到的序列其实就非常长。当然希望这个模型能支持更长的这个序列。这样的话我们就会发现说，原有的这个方案这么逊这个拉玛可能会有点问题。",
      "speaker": "发言人3"
    },
    {
      "time": "02:01:12",
      "text": "你follow lama，比如说拉玛最早是用2K长度的一个数据去训练，然后你训出来的模型它能扩的长度就非常的有限，大家会去想很多新的方法出来。对。然后我们作为模型开发者的角度上来讲，就会去去改变，说我要用更长的数据去训练。当然这里不是模型架构的这个事情。但是你会发现就是follow别人的这个技术方案的话，你会发现这样那样的问题，然后你会去对他进行解决。",
      "speaker": "发言人3"
    },
    {
      "time": "02:01:42",
      "text": "进行解决之后，包括像刚才田震提到，我们团队提了创lama这样的技术。其实我们现在是做了很多创业clamp开发的东西，包括跟推理框架的接入等等。然后让它真实的在场景当中能支持更长的这个序列。我们现在其实也在突破这一类型的限制。包括你的attention上面的话，可能会一定程度的采用一些space attention。你怎么样让这个space attention不掉点，这些也是非常讲究的事情。有人可能就会像java的话，会把某一些层给换成类似于RNN这样的一些东西。",
      "speaker": "发言人3"
    },
    {
      "time": "02:02:18",
      "text": "大家会去做这样或那样的一些探索，都是去发现说lama这个architecture也好，或者是拉玛这个技术方案也好，没有办法满足我今天的这个需求，所以我我我觉得大家都在做非常有意义的这一个事情。等到做出来比较骚的的这个方案，然后每个公司在自己的厂商也都验证好之后，他就会推动到开源社区这一边。然后开源社区在对你做进一步的event。这个方法如果都经过开源社区检验了，那我觉得基本上就是一个能流行开来，甚至是名垂千古的东西。",
      "speaker": "发言人3"
    },
    {
      "time": "02:02:59",
      "text": "对OK我觉得这个是一个非常好的问题。我也在想就是因为我每天看到这么多paper，看到这么多的工作出来，到底哪些是值得花很多时间很多精力去做。然后哪些可能就是简单了解一下就好啊。我觉得这个其实真的是需要在这个行业可能一线有很多这种上手的经验，才能找到更多的感觉。对我们来讲，我因为我并不是一线的researcher，所以很多时候我其实也是需要听很多专家或者说同事他们的意见，然后我自己也会跑一些测试。但是我觉得这个实际上是一个相辅相成的。",
      "speaker": "发言人4"
    },
    {
      "time": "02:03:39",
      "text": "就比如说创妈妈，刚才志阳也提到，这可能是一个非常好的技术。我去我看到那个paper，我觉得他也非常对。但是他是不是能够得到社区的一个公认，就是大家能不能用起来。",
      "speaker": "发言人4"
    },
    {
      "time": "02:03:53",
      "text": "其实这个就要面临一个技术推广上的困境。比如说如果这个技术大家都不用，那他可能是一个很好的技术，或者不是一个很好的技术，但是没有人用，所以不知道，或者说大家都用起来，然后发现这个技术好像没有想象的那么好，然后浪费了大家的时间。对。所以就是我觉得每一个项目的开发中，大家都会考虑说，到底怎么怎么做对吧？是不是支持这个新技术，可能要花一个星期的开发时间。然后这个效果怎么样，就是需要有一个评估。",
      "speaker": "发言人4"
    },
    {
      "time": "02:04:23",
      "text": "我觉得很多时候都是靠感觉，或者说靠这个行业内KOL的口耳相传，而且很多时候他这种优化并不是就是一个系统去优化就可以了。比如说我看俊阳之前提到说创妈妈想要真正跑起来，需要在flash attention那个层面去做一些支持。这种优化就不是一般人能做的，因为你可能要写很多非常底层的CID的代码，然后对flash tension的实现有一些了解。并且你还就是非要非常小心各种数据精度这这种这种问题。所以我感觉现在这个整体上来讲，应该没有一个特别好的方法论。",
      "speaker": "发言人4"
    },
    {
      "time": "02:05:02",
      "text": "我能想到的更多的是说尽量从各个角度展示，说你这个工作做的非常的结实，并且多跟这些领域的KOL去一起沟通。然后如果是像阿里巴巴前面这样规模比较大的团队，那其实可以自己出一些人。然后不仅仅是写一个paper，并且实际把它贡献到对应的这些库里，比如说VOM这些库里，其实我挺好奇VOM是怎么去评估一个技是不是应该被接入，还是说他就可能是并不是一个特别大的事情，然后就不管了。对。",
      "speaker": "发言人4"
    },
    {
      "time": "02:05:42",
      "text": "所以我觉得这是一个非常好的问题。其实我们每天都在琢磨这个问题，就是到底要哪怎么样的模型我们是要打算介入，怎么哪些模型我们打算不介入。然后我觉得我觉得我们从推理上来说，总的来看还是看这个模型到底是在在这个训练的时候的表现思维，那你最后训出来的模型的效果到底如何。如果是一个效果非常好的模型，那我们肯定会拼尽了全力去支持。但是如果你只是一个效果不太好的模型，模型结构又做了非常大的变化。那我从我们VM角度如果支持一个这样子的模型，我们要改非常多的代码来专门去support的一个这样子的一个如果效果不太好的模型，那我们可能是在engineering上做了这个trade off之后，我们会觉得可能这个事情可能会不太值得。",
      "speaker": "发言人2"
    },
    {
      "time": "02:06:24",
      "text": "那卓涵有没有关于接下来的这你看到的这一些这些模型到底哪一些怎么样的模型，你觉得这种架构是啊是真的能够带来这个质的提升，你觉得现在我们那拉满，我们算是看到了一个11个1个瓶颈。你觉得现在还有哪些值得关注的工作和方向？",
      "speaker": "发言人1"
    },
    {
      "time": "02:06:44",
      "text": "我觉得第一个是首先这个lama本身结构，你我说是说拉马结构看起来好像就一年一个，其实它就是一个transformer结构。对，然后再加上一些小改。然后transformer本身是一篇2017年的论文，所以他也已经久经时间的考验。从2017年开始，大家想要做很多各种各样transformer的modification。但是大家最后发现还是transformer本身的结构能够is good enough，is easy to scale.",
      "speaker": "发言人2"
    },
    {
      "time": "02:07:06",
      "text": "然后你可以在训练上获得非常好的效果。你想要在这个碑上面再提高一步是一件比较难的事情。然后我觉得我们最近看到的一些比较有意思的点，第一个是像刚刚提到的MOE很多的model，像特别是从mr o开始，很多的model都是使用了这个M1的这个架构。我们在推理侧也在想怎么样能够做更多关于MOE相关的优化。比方说引入一些MOE本身MME ted kernel。",
      "speaker": "发言人2"
    },
    {
      "time": "02:07:30",
      "text": "然后第二个还是这个longer test的问题。对，就是普通的attention。在特别是在推理的时候，我们会发现在你在context变长的时候，你的这个模型会显著的变慢。然后怎么样能够把这个在自动context情况cos降下来，然后设计一些新的这种attention的方案。这些都是我们比较感兴趣的一些内容。就比如包括像moba这样的结构，也是能够为了优化在transformer attention在句子长度的平方的这一个计算的复杂度。",
      "speaker": "发言人2"
    },
    {
      "time": "02:08:03",
      "text": "昨天团队其实在之前你也参与了那个维A的项目。其实这样想来，真的这个大模型这个领域的变化真的是非常快。我想比空难应该也是最早的一批，trig b出来了以后做的这个开源的LM的项目。可以，这个主要还要不跟跟大家简单介绍一下这个是什么时候的事儿。然后当时作为空的，包括我记得那一段时间出来了很多驼类项目的开源的这个LM表。当时的一个背景是怎么样的？那个时候其实很多这些项目是学术界主导的，对吧？像stanford alpaca等等的那现在你好像看到这些相对来说少了一些，所以想听听你怎么看待以后学术界对于这种开源ION的一些贡献。",
      "speaker": "发言人1"
    },
    {
      "time": "02:08:45",
      "text": "好的，对我先讲讲维库纳的历史。如果按照刚刚铁证的分类，这个就是属于一个典型的魔改喇叭魔头。二三年的二月份三月份左右。然后我们当时看，首先ChatGPT刚出来，大家都非常的excited，然后看到有这个大元模型能够做这么多这么厉害的事情。当时正好赶上这个lama，meta release lama，但只是release的一个base model。当时大家还meta本身也没有figure out how to do like RHF来怎么样做一个chat model。但是我们可以看到它base model本身在各种benchmark上的performance显著高于它之前的这个model make之前的这个OPT的model。然后我们在看这件事情的同时，所以我们现在想怎么样能够把这个lama本身能够变成一个更加大家easy to use，更加像ChatGPT的一个一个模型。",
      "speaker": "发言人2"
    },
    {
      "time": "02:09:33",
      "text": "然后我们就是在这个同时，我们看到了另外一个网站叫做share GPT。这个是当时大家也是ChatGPT刚出来，大家都感到非常excited，所以有有open source develop开发了一个croom插件可以看可以让这个不同的user能够这个ChatGPT的user能够。如果看到一个他自己觉得非常的非常有意思的一个跟ChatGPT的对话，他可以一键点击分享，然后可以分享到他的网站上。对然后我们发现这个share GPT当时因为GPT本身很popular，share GPT的这个插件也很popular。有很多的这个对话。大概是当时有7万到8万条对话被分享到share GPT的这个网站上。",
      "speaker": "发言人2"
    },
    {
      "time": "02:10:13",
      "text": "然后我们就通过这个share GPT，我们会发现这个share GPT其实他capture很多这种GPT这个模型在在这个3Q上面做出的努力。就比方说我怎么样让他表现跟他那个chatbot能够和人类进行这种交互的能力。然后我们就想能不能直接用这个share GPT的这个数据集来fine to try to一下lama的model，看看会是什么样的效果。我们我们最后很surprising的发现就是如果非常简单就是使用这个share GPT的数据集来find lama，可以迅速获得一个比较一个usable一个conversational fluent的一个chatbot。然后我们我们就非常exciting和大然后和大家share这个发现，然后发然后也对对，我们也就是非常lucky就be become the the first model that can achieve the conversation of fluency as ChatGPT。所以我们当时也收获了很多的attention。",
      "speaker": "发言人2"
    },
    {
      "time": "02:11:10",
      "text": "对，然后我觉得回到刚刚Monica的问题，就是在学术界develop LM首先我觉得现在学术界还是有很多的LM还有很多在LM上各做的各种fine和各种各种各种听的一些一些work，甚至还有在学术界做的一些pretrail ing。就好像omo model它是来自AI to或者说universe washington。还有其实像阿联酋的这个MVZUAI，他们也自己训了一些自己的model，所以学术界的model还是在训。",
      "speaker": "发言人2"
    },
    {
      "time": "02:11:41",
      "text": "但是我觉得可能大家现在觉得学术界的这个学术界的声量比较小是两个原因。我一第一个是我觉得这个大模型本身已经证明了自己有足够的商业价值，所以有很多的商业公司参与进来。然后商业公司的声量也是非常大的，这个导致相比而言学术界声量就比较小。第二个就是也是一个现实的问题，就是学术界拥有的这个计算资源和一个大公司相比是远远有限的。所以学术界其实是比较难做一些这种非常大规模的预训练之类的事情。所以学术界想做的事情就会更加的limit，更加for focus on specific scope。比方说我做一个agent测试或者做一个specific task上，我怎么样翻译一下LM来让它做的更好。学术界的focus可能更多的转移到了那个部分。",
      "speaker": "发言人2"
    },
    {
      "time": "02:12:29",
      "text": "对我特别同意刚才那个主任说的，就是我感觉学术界其实比较叫什么这个资源就缺的比较多。然后你想这个大模型公司现在动辄估值是几个亿，然后他们买卡，然后处理数据，然后去预训练的这些经费，远远超过这个学校。我还是觉得就学校在学校的这些老师和同学还是有蛮多机会去做很多事情的。因为大家的出发点不一样。比如说一个公司如果他想要做一个开源模型，他实际上是希望这个开源模型能够获得一定的影响力。",
      "speaker": "发言人4"
    },
    {
      "time": "02:13:05",
      "text": "在学校里面，其实大家更关注的是在某一个小点上做一个突破，然后获得很多citation。比如说一个公司说我想获得很多location，我发了一个质量非常好的数据集，然后就没了，对吧？他他没有办法把这个东西转换成一个商业的闭环。所以这方面的工作他可能会做的比较少。而且你作为公司去去发一个数据集，然后也有可能会有很多这种IP的风险。所以他可能就有很多事情他是选择不做，他他会把自己定位成说我一定要做一个能够产化，然后能够获得影响力，然后方便我去后面产生这个现金流的生意，然后方便我去做更高的估值的这这些事情他可能更愿意。",
      "speaker": "发言人4"
    },
    {
      "time": "02:13:46",
      "text": "学校里面，比如说大家愿意做一些这种模型架构上面的调研。比如说去去拆解一下说我现在有有这么多技术，到底哪一个才是实际有用的。或者说去尝试一些这种现在不能马上转化的这种未来架构的这种巧思，我觉得这些都是可以理解的。",
      "speaker": "发言人4"
    },
    {
      "time": "02:14:06",
      "text": "如果我没记错的话，像比如说mamba应该也是在学校的时候做出来的一些工作。像flash attention也都是在学校的里面做出来的一些工作。因为这些工作开源出来之后，其实是不能够直接转化成商业的。所以我觉得我们是需要这些商业公司在这里面去做很多很重要的投入，然后去推动这个行业不停的往前发展。但是同时我们也需要很多，不是以有一个商业转化的闭环的这些。比如说在学校里面这些研究员去做一个非常好的工作。",
      "speaker": "发言人4"
    },
    {
      "time": "02:14:37",
      "text": "其实这块儿我还是挺好奇，就是中方不知道后面对这个VOM的这个项目的想法。随着比如说项目里面大家这些博士生毕业，后面这个项目的计划是什么样子？大家会说想要把它做成一个有商业化闭环的这种商业项目？还是说这个实验室会有不同的有有新的同学进来，然后维持，VRM作为一个纯社区向的，然后呃就是非公益的这种项目继续去运转下去。",
      "speaker": "发言人4"
    },
    {
      "time": "02:15:05",
      "text": "非常好的问题。对，然后也是一个我们被问的最多的问题。我觉得VM本身它是一个，我觉得我们是非常幸运，然后我们收到过了很多很多特社区的支持，然后我们现在一个比较大的亮点就是说，其实你看如果VM现在贡献的code，我们其实这两天正好做了一个统计。大概只有百最近只有25%的这个commit是来自于UC birkin，然后剩下75%的commit都是来自于其他的open source community。",
      "speaker": "发言人2"
    },
    {
      "time": "02:15:31",
      "text": "不同的公司。然后我们现在有很已经有和和很多的公司合作。很多公司在有一个dedicated engineering的team来给VOM写code，来给vm加新的feature。然后甚至有很多公司也有很多VM的committed和reviewer，能够来帮忙review VM的这各种新来新的贡献。然后我们目前的计划还是说希望keep the open source mom going，我们希望VM能够成为一个成功的open source project。我们我们在burkey这边我们更想做的事情是能够get大家能够coordinate大家的effort，然后来做更多的，要让保证这个项目能够在能够长期的发展的下去，因为我们有很多新的博士生同学加入。然后对我们也希望在这个项目能在burkey也能够长久的发展下去。",
      "speaker": "发言人2"
    },
    {
      "time": "02:16:19",
      "text": "BLM的话其实让我想到几个月前我其实发起了一个投票，就是说你们会用VLM还是用EGI，还是用tech RD，因为我我想找到一个更好的推广给我的用户的方案，但是大家其实会更多的把票投给那个VRM。当然NVIDIA的人会跟我说，天使RDRM的性能会更好各方面，但其实VRR的话在应用性方面其实做的很好。而且它是基于拍摄的，让大家用起来的话会非常的方便。",
      "speaker": "发言人3"
    },
    {
      "time": "02:16:52",
      "text": "所以其实对于BLM这个框架的发展来说，其实像刚才卓翰提到，其实现在已经变成有很多人在给他提坑爹的，甚至是有小公司大公司的人就在不断的给他提肯定。因为他们在帮助别人的同时，其实就是在帮助他自己。所以我其实相信说VRM这样的一个框架能够获得流量，能够获得流行度的情况下，我觉得他可能真的会是成为大家的标配。这让我想到honey face。其实早年的时候大家会去诟病说any face自己本身的代码有这样那样的问题。但其实整个社区的话，它就这么自然的发展起来。所以我也希望说这样的话，其实也能以这样的方式发展起来。",
      "speaker": "发言人3"
    },
    {
      "time": "02:17:35",
      "text": "关于学术界和工业界的问题，因为刚才铁生谈的比较多这方面的问题，其实我我跟学术界的人接触其实还比较多，我也在想怎么跟学术界合作会比较合适。我先说一下，比如说在大公司或者是说大模型开发的这个团队的现状。就是我们一般做事的话，在技术方案上反而是不那么敢非常激进的去做一些事情。因为。你训练一个大模型真的不容易，而且它是非常烧钱的一件事情。毕竟这么多卡在那烧，也是不能随意的去做做实验的。我们一般会选用更加稳健的方案的话，保证这个模型训出来的效果是足够好。",
      "speaker": "发言人3"
    },
    {
      "time": "02:18:22",
      "text": "但是这个世界是需要很多人去进行探索的。有很多新的技术的话，反而会对这个大模型的技术带来致电，其实有很大的帮助。就像大家在提OpenAI的时候，其实大家会发现说OpenAI在做的事情，他的很多技术似乎其实都是来自于别人。但是他把临门一脚做的特别好，把它做到特别上乘。",
      "speaker": "发言人3"
    },
    {
      "time": "02:18:47",
      "text": "我觉得像学术界的话，其实可以做很多创新的工作。我觉得有一个非常好的例子，就是斯坦福的DPU，我非常喜欢这一个工作。因为在去年的时候，其实大家在做大模型都经历了非常艰苦的事情。就是大家想做LHM，但是成失败的经验积累的越来越多，但是不知道怎么走向成功。因为PTO非常难训，而且往往是有做RL的人才清楚这个事情怎么做。",
      "speaker": "发言人3"
    },
    {
      "time": "02:19:20",
      "text": "但是RL又不太了解NOP，这就导致这件事情很难做。让大家觉得说open I好像有这个神秘的魔法，我们很难追上。但是有了DPO之后的话，这个技术很快他就经过了这个社区的验证。我大致的感觉是去年10月份的时候，这个技术就开始非常火起来。大家开源社区的人的话都会去用DPO去做learning from human preference这一件事情，就是学习人类的偏好，用DPO的方式去做。其实我们现在实际在用的时候也是会用到DPU或者是DPU的变种。这也为我们迭代更好的模型化带来非常大的帮助。",
      "speaker": "发言人3"
    },
    {
      "time": "02:20:06",
      "text": "我觉得学术界如果能做出这种非常有意义的工作，非常创新的工作的话，其实是对于整个行业的发展是非常重要的。所以我觉得不一定是说学术界对于大模型就比较那个的，就是说我没有卡，我就做不了一些事情。其实现在还是能做比较多一些创新工作。尤其是今天开源社区，像honey face，像lamer TCPT以及MLX，不断的在推动大模型的普惠。今天你甚至可以用lamda CPP，甚至用MX就在你的苹果电脑上，就在你的这个笔记本上面的话，就能够搬进一个模型。所以我觉得能做的事情还是非常多。",
      "speaker": "发言人3"
    },
    {
      "time": "02:20:48",
      "text": "对的，我觉得这个分享非常棒。其实学术界现在做了很多工作，很快也会反映到这个开源社区里面。要么就是本身开源了，要么就是很快有开源社区的实现。我觉得整个进化的步伐，我觉得还是让人觉得非常的期待。",
      "speaker": "发言人1"
    },
    {
      "time": "02:21:02",
      "text": "然后另外一个关于开源，我想不得不提的其实就是data对吧？这个数据其实前面铁证也提到了，说看到了很多其实国人驱动项目也是一些一些开源数据的项目。当然我们又是另外一方面数据，其实又是做AI中很重要的一块。很多大众型公司我想肯定也都在花很多的钱在再去创建他们自己的自己的propriety的数据也好，或者说新的这种数据方法。似乎数据也是大家最难以去最难以去明说的一个secret sce。",
      "speaker": "发言人1"
    },
    {
      "time": "02:21:38",
      "text": "所以我好奇说铁证您怎么看待？就是说现在这些开源数据集在整个整个行业里边的一个作用。那谁有动力？我要去把自己辛辛苦苦抓下来的，甚至生产的这个数据把它开源出来。往后随着这高质量数据可能越来越贵，产生越来越贵，我们会看到有一些怎样的挑战和还有一些你就看到的一些可能解决这些挑战的一些机会。",
      "speaker": "发言人1"
    },
    {
      "time": "02:22:08",
      "text": "对我觉得虽然数据上其实现在已经多了很多数据，但其实总体上我们还是非常去高质量的中文语料的。前几天我做了一个测试，就是我让这个各种大模型来跑这个固体式，然后算它的瓶子。因为我自己不会说方言，所以我我我其实分不清哪个是陆生。然后我也不太会知道这个瓶子，所以我希望大大模型能够帮助然后发现基本上就是全军覆没，就是呃4O其实4o deep sik还有零一的那个闭源的模型，其实都都做的还可以。但是其实还有我觉得这个领域其实还是需要大量的数据，才能让模型更好的去理解中文，理解我们这些传统文化。而且现在我们对模型的需求也会不断的变化。以前是说他好像挺聪明就可以，现在我们不仅让他聪明，其实还需要让他知道更多的知识。要知道更多的知识，其实就需要更多高质量的语料。",
      "speaker": "发言人4"
    },
    {
      "time": "02:23:03",
      "text": "然后我看到就是现在其实也发了大家也发了很多这个数据集。大部分的数据集他们的源头其实都是common cross。它是common cross就是一个在互联网上的一个应该是非盈利性的一个机构。然后他从很早以前就是远远早于大模型出现之前，就开始在互联网上爬各种各样的数据，然后把它存起来，然后呃每个月就是发一个版本。然后到现在它应该已经有几个PB的数据。但是所有的这些数据并不是都能被大模型用于训练。比如说它爬下来的这个数据可能是HDL格式。那你要把它写成就把里面的关键的信息找到广告屏蔽掉，然后一些没有用的信息也都屏蔽掉，然后就是把它写成一个文本的格式，然后方便这个大模型去去训练。",
      "speaker": "发言人4"
    },
    {
      "time": "02:23:47",
      "text": "很多人在这里面做了蛮多的工作。但是这个问题在于什么呢？就是common cloud里面就是就算你把所有common cross的能用的数据全都用了，他也只有不到5%的数据是中文数据。为什么呢？就是come cross它是一个君子行为的这种组织。他也为了避免自己一些法律风险，所以他在爬网页的时候，他会看那个logo协议。就如果这个网站禁止搜索引擎或者说其他的一些地方买卡特的网站的话，这个数据是肯定不会出现在common cross里面的。并且common cross也会制定一些规则，把一些网站给屏蔽掉。如果说大家的开源数据集都是从common call起，并且common call不太懂中文的话，那其实就是我们会遇遇到一个巨大的危机，就是开源数据集，就中国的开源数据集实在太少。",
      "speaker": "发言人4"
    },
    {
      "time": "02:24:32",
      "text": "这方面就是有有两派做法。一派是更草根一点的，就比如说MNBBC，实际上他们就是在各种他们的这个愿景非常的高大，大家感兴趣可以去看看一下，让他们在收集各种高质量的中文语料，并且做了非常多清洗的工作。然后另外一个就是可能不是草根的视角，而是更官方的一个视角。比如说支援就在做CCI的这个数据，他联合很多这种单位，然后去希望他们能够贡献一些数据集。我觉得这些工作都都特别的重要。因为我们要找到一些common cloud里面没有的中文数据，然后把这个中文数据集的趋势给补充上去。",
      "speaker": "发言人4"
    },
    {
      "time": "02:25:17",
      "text": "然后后面就是我们其实看到这个中文数据集的缺失，有可能会有另外一个方向的演化。就是在海外york kms不是把那个OpenAI给告了。所以其实就是国内的产权，就知识产权界也有一些讨论说这个大模型的语料到底如何去界定。所以现在很多公司我感觉还是有一些顾虑。就是就算他们自己做了一些比较好的数据集，是不是能够把这个数据集开？我觉得后面这个可能更多的不光是一个技术圈的讨论，更多的是一个法律圈或者是国际关系的这些人讨论的一个一个话题。",
      "speaker": "发言人4"
    },
    {
      "time": "02:25:58",
      "text": "这个我也不清楚后面会怎么走，我还是希望能够走向对开源更友好的一个方向。就是美国的话，其实它包括这个common cross，他们其实打了一个点就是fair use，就是合理使用。他认为大模型的训练什么的，这些数据可能是被可以被定位成合理使用，然后不受知识产权保护的限制。这方面国内并没有这么一个知识，就是合理使用的大规模的判例。所以这块也看国内的一些变化。",
      "speaker": "发言人4"
    },
    {
      "time": "02:26:34",
      "text": "俊阳我好奇，因为你们在自己做大模型的过程中，就是数据这一块接下来主要研究方向是什么？会有还有哪些新的挑战？",
      "speaker": "发言人1"
    },
    {
      "time": "02:26:42",
      "text": "数据方面的话，现在预训练的数据，我整体看下来，可能大家略略都会有一些趋同了。因为都是获取全网的数据，放到这里边去训。但我觉得这个数量上的空间其实还蛮大的。像last three的话，他说他是15T的数据，但是我听mark的博客讲，我感觉他的数据应该是有多个iteration，也就是说他的unique token不一定那么多。",
      "speaker": "发言人3"
    },
    {
      "time": "02:27:13",
      "text": "我目前整体看到大家能获得6到7T会是一个认为比较合适作为训练数据的6到7D会是一个比较合适的数字，但是应该还有空间再进一步往上走，走到比如说10T以上的比较高质量的数据。因为事实上如果你觉得这个质量比较低，你也能纳入进来的话，其实你确实能把这个量能扩得更大。那一般大家还是会有一个自己定义的一个标准的限制。但我觉得预训练这个事情的数据的方案可能会相对来说比较确定一些。",
      "speaker": "发言人3"
    },
    {
      "time": "02:27:49",
      "text": "但是post training这一块的可能性就比较大。大家可以看到有很多不同的research在探索，post training的数据应该怎么构造，会带来什么样的影响。像去年的话，大家会说lessons more，比如说我一天淘数据可能就够。今天的话我们可以看到拉拉three的话，引导的方向又变成了说我要用了1000万条instruction tuning的这一类的数据。那我觉得还有一个很大的空间非常值得探索。",
      "speaker": "发言人3"
    },
    {
      "time": "02:28:19",
      "text": "我也知道现在有美国这边有很多大的公司在做的一个事情，就是要标那种非常难的高质量数据。举一个例子，还是跟open deven相关的，就是跟coding相关的这个数据。像我们现在想获得真实的，我们叫project。就是人在解决这个代码问题的时候，他是怎么做的。这个过程的数据能把它给构造出来。比如说今天我碰到一个英雄，然后我去思考我怎么去做，然后我第一步做了什么，然后得到什么样的反馈，然后第二步又去做什么样的事情。",
      "speaker": "发言人3"
    },
    {
      "time": "02:28:55",
      "text": "这种数据的话其实是非常难构造的，而且审核也非常困难。就是你需要相对比较专家的工程师，他才能够去判别说你这个数据表的质量是好还是不好。所以我觉得之后如果大家更清楚那拉model能做什么样的一些事情，尤其跟agent相关的时候，它发展的更好的话，反而会知道说这个数据应该怎么去标。我觉得更大的空间其实是在这个post training这一块儿。今天大家谈更多关于OpenAI的秘密的话，其实OpenAI在数据方面的话也有很多不人知的一些秘密。也需要我们这个OpenAI以外的人的话去做非常积极的数据方面的探索。",
      "speaker": "发言人3"
    },
    {
      "time": "02:29:42",
      "text": "其实刚刚铁匠还跟我分享了一下这个数字，说其实其实像千问和以前我们提到要看他们的社区发展怎么样，要看他们这个家族有多大。我看到从国产的这些模型来看，千万和这个亿都是在他们整个家族里面都是有上千的社区的上千个model对吧？这个社区里边我想都是一个非常不错的成绩。但的确对比起像myo，还有像lama，像国际顶尖的这些项目，还是有一个数量级的差距。说到底我们跟这些顶尖的这些模型社区他们的主要的差距在哪？然后还有哪些对我们就可以学习的这个地方，包括我们看到说MSO明明在这其实是去年下半年才开始，然后好像一两个magnet这个磁力链接就可以让他们的这个热度马上上来。这个背后有哪一些跟技术产品和社区相关的东西，我们是可以去学习。",
      "speaker": "发言人1"
    },
    {
      "time": "02:30:39",
      "text": "这个问题我很喜欢，因为正好说起我们当时为什么会做快1.5的经历，其实当时就是因为看到了mr o他虽然是后发者，但是会发现他的世界范围的影响力的话，当时是远大于困非常多。像刚才我提到的，甚至当时还有非常多的人没有听过你这一个名字，那我就会去看他究竟做对了哪一些事情。因为从model quality的角度上来讲的话，我承认当时即便但是今天1.5的7B的话，可能跟他的myrle的7B的话在一些能力上还是会有一些差距。是我觉得这个不是完整的原因。所以当时去做的比较多的探索，其实会发现miss o它看似非常随意的扔了一个磁力链接，但是其他方面的事情他其实做的非常多。我觉得海外的人反而会把事情做得更加全面一些。就比如说今天我要去release一个模型，那我以什么样的方式让这个用户能够更好的用起来。然后预判一下用户可能会有哪些问题。现在开源生态大家是怎么用大模型的那怎么样让他们不用拉马而用myself？我觉得这些问题它其实是想的比较多的，所以他把很多事情都都推进下去。",
      "speaker": "发言人3"
    },
    {
      "time": "02:32:07",
      "text": "当时我们多少有点知耻而后勇的意思。就是因为当时内部去做讨论，说我们问题究竟出在哪里，有什么可以跟他学的。能不能有一天别人在提完拉玛mystery之后，想到的就是queen这个系列的模型。",
      "speaker": "发言人3"
    },
    {
      "time": "02:32:26",
      "text": "所以我们当时就去做了很多三方生态适配的这个事情，包括修改我们代码的事情。所以其实很多事事情或者是别人看起来非常轻易的成功，他其实背后做了非常多细节的工作，而不是一些大的战略上就能判断对的一些事情。我觉得今天大方向上大家都整体都差不多，但是更多的事情是藏在细节当中，让他们还有对开发者做很好的关系的维护。",
      "speaker": "发言人3"
    },
    {
      "time": "02:32:55",
      "text": "我觉得在国内的话，我们其实是相对来说做的比较少。这可能也要再进一步的去做，然后去教大家怎么去用你的模型。你的模型可能优势在什么样的地方，怎么样去给大家讲好你自己的故事。我觉得这些的话都是需要我们去学习的这也是为什么我们当时queen 1.5出来之后，还要配上一个类似于专属于快的官方的博客。因为每一次的话我们就能够把这些信息的话以博客的方式去呈现给大家。",
      "speaker": "发言人3"
    },
    {
      "time": "02:33:26",
      "text": "你可以看到之前有很多的国内的开源的模型的话，其实是没有把这个信息去做充分的传递，也没有比较好的教程去做这个事情。我们这边还相对好一些，就是某大社区帮我非常多。某大社区会去推广各个开源的模型，交代要怎么去用。他同时也会去推广会去怎么去做这些事情。我觉得这些的话，这些细节其实往往是你成功的关键，而不是一些大的东西所带来的成功。",
      "speaker": "发言人3"
    },
    {
      "time": "02:33:57",
      "text": "对我觉得这是一个特别好的问题。然后我觉得在国内困应该是算是做的最好的就海外宣传的之一了。大家如果想要做这种很有海外影响力的大模型或者是项目，其实都可以和这个正阳好好学习。然后VRM在海外也有非常大的影响力，我觉得其实特别期待有有更多的分享，就说这个从0到1的一个起步到底做对了什么？然后我特别同意志阳刚才的那个点，就是其实从大的战略上来讲，比如说我们就谈这个宏观的这个战略，其实支持开源，这个其实很多人都是认可的。然后说我们要有更多海外影响力，这谁谁不想要对不对？但是魔鬼都在细节，是不是能够把这个东西做好。比如说这个东西是一个KPI追问的事情，然后上面就只有一个战略，就是说我们要有一些海外的影响力。",
      "speaker": "发言人4"
    },
    {
      "time": "02:34:48",
      "text": "最后可能大家觉得，这到底能做什么？我把模型传到hang face是不是就结束？没有，其实还有好多事情可以做。比如说你是不是可以写一个非常好的中英双语，或者说只有英文的这种把矛头靠就是在大家访问到你这个模型的时候，第一眼就能看到说你模型有什么亮点，然后解决了什么样的问题，然后你的这个评测的效果是怎么样子，有没有一些例子，有没有一些吸引眼球的图片。最后再给一些cos nap的告诉大家怎么去把这个东西用起来。然后你在上传这个模型的时候，你是不是加了合适的meta data，让大家在high face里面搜索这个模型的时候，一下就能把你的模型搜到，也可以比如说在模型发布之前，可以找我，我们可以一起对一下，看看怎么能够帮你们有更多的影响力。",
      "speaker": "发言人4"
    },
    {
      "time": "02:35:37",
      "text": "其实不光是hin face，比如说在推特上的宣传，然后在discard的宣传，然后跟这些开发者的沟通，然后跟比如说是不是能够在模型发布之前，就跟比如说VIM的团队去做一些沟通，然后看看是不是可以一起去release。然后还有什么拉马CVP等等这些就是下游的这些组件是不是都能够把你的模型跑起来。然后这样的模型一出，大家就能把马上把它用起来。因为这个就是大家人的关注的热点，都是说这个热点一出来的时候是是就是关注度最大的。如果那个时候你抓不住用户，那后面用户肯定就流失了。没有人想说，那我过一个月之后再来看看你的模型到底能不能跑。一个月之后这个就已经有新的模型，所以一定要是在发布的时候就把。尽量把所有的这些东西都准备齐。",
      "speaker": "发言人4"
    },
    {
      "time": "02:36:23",
      "text": "然后像刚才也提到这个mysql，它是用磁链接的方式发布。我觉得保持一点神秘感，然后把它做成一个话题，然后让更多的人能够参与进来，有更多的吃瓜群众，我觉得这也其实也是挺挺重要的一种能力。我觉得像沈阳刚才也提到他会在推特上去去发一些破什么的，我觉得这种。Sorry, 这种跟社会跟社区的一些互动也是非常重要的。然后是不是能够让大家哪怕不是因为你模型发布以来，也来关注你的这个公司的一些推特账号，然后跟你有一些互动，我觉得这些都是很重要的。还有就是关注一下，就是用你模型的一些KOL，它可能会已经既然已经用了模型，那不管它的评论是好是坏，其实都都可以进跟他进行可以进行更更深度的互动，然后说不定就可以有一些非常忠实的粉丝。",
      "speaker": "发言人4"
    },
    {
      "time": "02:37:20",
      "text": "然后再就是就是尽量面向这个英语圈创作更多的内容，就像刚才提到创作一些blog，甚至可以考虑做一些youtube video。Youtube上其实有很多非常火的视频，就是教大家怎么去在windows上面装一个stable的fusion。这些博主其实他们也很缺素材。如果你有一个非常好玩的东西，不管是模型，不管是文生图，不管是lara还是怎么样，是不是能找到更多的渠道在海外去去发生。",
      "speaker": "发言人4"
    },
    {
      "time": "02:37:48",
      "text": "回到主持人一开始的那个问题，就是为什么mystery它的衍生模型会比困和01要多出一个数量级？我觉得其实也是和大家主要面向的这个群体有关。我感觉国内其实也有很多人做了非常不错的模型，但是大家可能也不太愿意往上传或者是开源，所以看起来这个模型的数量会会有点少。",
      "speaker": "发言人4"
    },
    {
      "time": "02:38:12",
      "text": "然后在海外的话，这些用户因为整个hang face的全家桶其实是非常方便的。你在那块儿随便就是我就我我就说我有一个什么300条的数据，然后我翻听一下，然后用这个模型翻听一下，那个模型翻听一下，看看哪个能满足我的要求。然后翻听完我就直接传到网上，这就是其实是非常容易的一个事情。上次去dolphin去去翻，请那个千万110D的老哥，我问他说你大概用了多少张卡，然后用多少时间？最后一算他微调一个模型的成本可能也就5000美元。因为海外的这个算力成本可能就是比较低，所以大家觉得我就是这个周末？我玩一下，然后下周我就能看到一个模型训练出来的结果传到海洋face，看看有没有人关注，或者说他去宣传一下他自己的微调模型，间接的为这个基础模型做一些宣传。我觉得这里面其实有非常多特别好玩的东西。可能需要找到一个天天在这个社区里面去积极发言，然后去知道这个社区大家可能会期待什么，然后就慢慢有一个有有就有耐心去去慢慢把这个自己的社区吧。所以我觉得就是现在我们看到说已经有一些模型做的还蛮不错的那我们相信后面会有更多国内内的模型会有非常大的影响力。",
      "speaker": "发言人4"
    },
    {
      "time": "02:39:31",
      "text": "对我觉得刚刚铁证和这个铁镇和这个俊阳分享的这个关于这个到底如何执行上面和这个一些相当于树上的一些这种分享是非常有帮助。我觉得可以帮助大家在发布一个下一个大模型的时候能够做到怎么样，能够有第一波的影响力。但是我觉得到了最后，你的这个模型最终能取得影响力，其实还是取决于你这个模型本身的质量。就是你的这个模型，比方说相比起我用拉马本身能够带来什么怎样的优势，能够表示能够更快，还是说能够得到更高的质量呢？",
      "speaker": "发言人2"
    },
    {
      "time": "02:40:03",
      "text": "对我觉得大家就是我我记得印象比较深刻的一个例子是零一的那个模型E他们第一开始发布的时候，我觉得我记得他们一开始还是有一些风波。因为他们的模型长得非常像llama的结构，大家可能觉得这个就是一个copy拉，你就是什么都是抄的。但是其实在在海外大家还是最后还是发现这个E的这个模型训练其实是比lama本身做的是要好。他有用了更好的数据集，可能训练的更久，它的这个模型的质量是比拉马要高的。最后很多很多人开始把自己的模型从这个lama模型到E这个模型。我觉得EEE这个模型到了最后还是能够收获非常大的成功的。从V我们VM的角度看，我们很多艺术也是来就是大家碰到问题都是在跑E这个模型的时候碰到的，而不是对我觉得这个到了最后还是看这个模型的质量本身。",
      "speaker": "发言人2"
    },
    {
      "time": "02:40:51",
      "text": "我非常同意佐汉的观点，因为讲到E的话，就是一定程度上E是一个学习的对象。因为我当时还专门问过吕强的，怎么去做这个E的运营这个事情。因为E这个模型的话，首先它训出来的效果是比较不错，另一方面的话它无缝衔接了这个拉马，他甚至可以直接用拉马的代码来跑。他把之前的问题给解决了之后的话，在海外其实很快就火起来。而且他还有一个独特的优势，就是其实也是前锋miss掉的一个事情，就是我们有很多的用户在用我们的14B的模型说，然后会发现说能力不是很够。然后用72D觉得还可以，但是72D太贵了。就非常希望我们能有一个中间的模型，所以E的34D这个size的话其实是非常好。所以几方面的因素了，一方面的模型效果很好，另一方面的话它的size的设计的话正好填补社区的空白，所以其实他就很快的做起来。我其实想补充这一个点，意义确实也是一个做的非常好的逻辑。",
      "speaker": "发言人3"
    },
    {
      "time": "02:41:57",
      "text": "我特别同意卓翰说的，最后还是其实实力说话。那时候和妈妈获得那么大的影响力，其实也是因为他的这个模型确实是啊非常不错的。然后刚才给千万打了这么多广告，我也给E打一个广告，就是E的最新的模型也有一个新的亮点，就是说它的license切换成apache 2，基本上就是没有太多的限制。不管是商用，还是说你想用这个模型去做一些更细致的调整什么的，其实都是非常方便。",
      "speaker": "发言人4"
    },
    {
      "time": "02:42:28",
      "text": "对卓涵我好奇就是做像VLN这种没法的项目，为什么能够取得这样的成功？回过头来你觉得有哪些经验可以跟大家分享？",
      "speaker": "发言人1"
    },
    {
      "time": "02:42:39",
      "text": "对我可以分享我们ZN从release之后发生的一些事儿。我觉得我们第一天release的时候，是我们也是有一些有一些一开始的这种bonus。是因为我们我们实验室有好几个项目。第一个是维库纳维库纳，我们当时是需要设置一个VA的demo来给大家用。第二个是为了当时evaluate维库纳的这个结果，我request到底是一个好模型还是坏模型。我们和我一开始也有参与的一个小项目叫做一个项目叫做chatt bolt arena，来通过人投票的方式来比较不同的模型之间的好坏。不管是你的demo还是travel arena都需要我们自己serve很多的模型。然后在学术界我们又需要serve模型，我们又没有那么多GPU，那我们就非常需要一个高效的一个LM serving engine来来能够support我们的这些这样子的项目。",
      "speaker": "发言人2"
    },
    {
      "time": "02:43:27",
      "text": "然后当时VIM在release之前，我们其实我们是6月20号release的。然后我们在大概四月份的时候就已经开始support both a demo以及这个chat board arena的项目。所以在release的时候，我们不光是release项目设了一个我们的比较好的一个performance number，并且我们也其实展示了一些我们的real world case。我们已经设置了大概两个月的traffic，这个可以给大家更多的信息，然后来给开始使用这个项目。",
      "speaker": "发言人2"
    },
    {
      "time": "02:43:56",
      "text": "还有一点就是我觉得对于一个开源的inf项目来说，能够让大家get easy to start a get，就是能够很快速用起来，是一件非常重要的事情。我觉得很多做info的人会低估这件事，特别是大家在大公司里面做。你可能只面对你的一套环境，然后你可能部署一次，你就再也不用部署了。然后你面对的同事也是非常聪明的同事，他可以follow你的一百行的一个instruction来set up一个environment。对我觉得这个但是在开源的世界，很多时候很多人可能试了两三步发现不行，可能就直接放弃了。所以我觉得对于这个project一开始adoption来说，你的这个project本身非常要very easy to get started。这件事情是非常重要的对如果一个人能够跑起来，你的项目他就会对自己做的这件事情有信心，然后他就会更愿意来看你的这个代码，更多的使用，就这样子的事情。",
      "speaker": "发言人2"
    },
    {
      "time": "02:44:47",
      "text": "然后我觉得最长期的发展的话，我觉得还是要多和社区交流。我觉得是这样的，我们我们在未来团队本身，我们想我们的努力目标一直就是为了让大家能够更好的用VM以及更好的加新的feature。能够比方说你有有新的模型，你想要实现新的一些优化，或者是你有一个新的硬件。我们我们想我们从在VM团队本身，我们想要让我们的project能够。的结构上来说，能够非常容易的来增加这些新的这种不同的优化。",
      "speaker": "发言人2"
    },
    {
      "time": "02:45:19",
      "text": "然后我觉得这个是我们一直在努力，一直在做的一个事情。最终以及还有我们要be more inclusive，就比方说像我们有committed给我们来commit新的代码，我们也会很积极的去review，然后来提供我们的feedback。如果一个贡献者如果能够他们如果有经常的贡献，那我们也非常愿意让他能够多参与进来。来比方说成为一个reviewer能够来review一些code，然后能够merge，然后有有merge的权限。然后相当于对我们对这个项目有更多的拥有权，或者说一个更多的参与感。对我觉得这个也是非常在运营我们这个开源项目当中非常重要的事情。",
      "speaker": "发言人2"
    },
    {
      "time": "02:45:57",
      "text": "有一开始有一点无心插柳的这个成分，但其实我觉得这个是挺常见的。其实有很多我们看到的一些开发者工具什么的。其实他们的诞生都是因为这开发者自己在做另外一个应用，在过程中很自然而然的发现了这个需求。主要还正好提到了一个项目，不知道刚大家听有没有听清楚，就是这个travel arena，其实这个正好就是正好完美切换到我下一个想要讨论的这个话题，就是LM的这个评估讨论非常热烈的一个话题。",
      "speaker": "发言人1"
    },
    {
      "time": "02:46:28",
      "text": "虽然我们有各种各样的leader boy，但是都没有一个共识。然后such a arena其实也是一个特别，我觉得很有特点的一个lia boy。而且如果大家去到这个hugger face上面去看的话，跟hugg face也host的这个榜单，你会发现在这个highflier这个open LM leader board的的模型的结果，其实跟这个travel arena结果还挺不一样的。要不卓涵跟大家简单介绍一下这个travel rena是怎么样的一个是是怎么样的一个评估这个方式。你们对于大模型评估这件这个事情有怎么样的一些观察和思考。",
      "speaker": "发言人1"
    },
    {
      "time": "02:47:05",
      "text": "对，chat ble arena是我们实验室的几个同学他们发起的一个项目。就是说这当时一开始的motivation也是像我刚刚说的，就是为了解决这个A的evaluation的问题。我们到底怎么样判断到底OK是ChatGPT也好，还是好到它的BTB比丰田好多少。然后我们当时我们很难找到一个自动的量化的评价指标。所以我们到了最后，我们想的方案就是OK我们给给一个给一个人，给一个random user两个模型。然后那个random user可以问这两个模型任何各种各样的问题，然后我们就可以问，然后他可以模型会给出相应的输出。然后这个人可以在两个模型当中投票为这两个人。",
      "speaker": "发言人2"
    },
    {
      "time": "02:47:46",
      "text": "每个用户可以在两个模型当中投票，然后可以选择一个相对选择他认为相对更好的模型。我们可以最终通过这些投票的结果。然后每一次投票就相当于是两个模型打了一场比赛，然后有谁赢有谁输。然后我们可以最后可以通过把这些模型的输赢的情况做一个加权的平均。",
      "speaker": "发言人2"
    },
    {
      "time": "02:48:06",
      "text": "就好像大家比方说网球比赛有很多我各种网球选手有一个世界排名，他们的一个算，然后有一个积分。然后他们这个算就是通过这种每每不同的网球选手之间的对决，然后来来得到一个来最后推算出一个积分。这个积分我们用的一个积分叫做要用的一个积分叫做ello school。然后也是一个大家在各种各样的这种比赛当中经常常见使用的一个积分。然后我们最终通过这个yellow score来给这个普通的模型来进行一个排名。这个就是chat board arena背后的故事。",
      "speaker": "发言人2"
    },
    {
      "time": "02:48:40",
      "text": "Chat arena比如说跟其他leader board，像open on leader boy其实这个效这个排名上还是差别还是挺大。这个背后说明了什么？",
      "speaker": "发言人1"
    },
    {
      "time": "02:48:51",
      "text": "对我我的我个人的理解，我的感觉是check table。反映的也是一种特殊的一个人的集体的bias。到底是怎么样？是一个人人更加喜欢的一个模型。然后我觉得是一个比较unique的一点是说，他大家可以自由的问各种各样的问题，大家可能都会问比较chAllenging的问题。然后为了来区分出两个模型的差异，所以这个本身能够能够这个问题的难度本身可以让我们更好的来tell the difference between the two models。所以能够可以让这个模型本身的可以能够更好的看出这个不同模型之间的区别。",
      "speaker": "发言人2"
    },
    {
      "time": "02:49:31",
      "text": "而我觉得我看到的其他的benching mark本身更多的是使用一组fixed set of data，然后来测试这些模型，然后fix data可能有这样子的两个问题。第一个问题就是说就是这些data本身可能最终会进入到这个pre training data里面去，这个是一个非常难避免的事情。在这种情况下，你可能你的模型，你最终的一般认识的结果就会出问题，就会要有点leaking。然后第二点第二点就是说大家可能如果你有一个fix的这个testing data，大家可能会overfeed到你的这个feed testing data面去。而不能最后看出你这个模型真正的一个好坏的结果。所以我觉得就是从从因为这两点原因，可能其他的一些如果用fix data的data的一个batcha可能多少会有些bias。而transport的arena本身它可能会更加的robust to against this看这些这样子的这种robust against这些这样子的这种这种这些因素，所以能够得到一个更加相对稳定一些的结果。",
      "speaker": "发言人2"
    },
    {
      "time": "02:50:33",
      "text": "对的，就是我觉得模型的评测实际上是很重要的。然后我觉得就发展到现在这个阶段，其实对于我们应该对每一个不同的场景可能有自己细致的评测。每一个榜单它并不是说非常全面，然后都都需要不同的这个榜单结合起来一起看，然后才能让你对这个模型的能力有一个大概的认知。然后可能你自己也会就是用户自己也会准备一些题，然后去去问不同的这个模型，然后来来有一些感受。我们上面是有一个榜单叫open origin with model d的board，他测的就是所有的开源的模型，他没有测闭源的模型，他测的就是这些模型。",
      "speaker": "发言人4"
    },
    {
      "time": "02:51:09",
      "text": "它的这个题目其实在网上也是知道的，所以有的时候会有一些数据污染的问题，但如果有这样的问题被发现，比如说有一个模型它特别小，然后它分数特别高，然后大家就很怀疑它是有数据污染。然后后面不管正式还是没正式，大家通过在社区的这个讨论，后面管理员会给这个模型加上一个标志，说这个模型可能还有现在的数据的污染问题。所以这作为就是一个防止数据污染的办法。然后我们也其实也推荐大家同时去看，就是很多不同的lead bard，比如说MS这个lead bard，我觉得也是能能给大家提供很多这方面的详细信息。",
      "speaker": "发言人4"
    },
    {
      "time": "02:51:52",
      "text": "真的要做好这样的一个评测。现在我们的它的难点在哪？对于很多用户来说，我可能要看的只是LN本身的这个评测，还要看它在我具体场景中的效果。这个中间会有一些gap，有什么best practice可以大家去分享一下。",
      "speaker": "发言人1"
    },
    {
      "time": "02:52:12",
      "text": "对，如果是开源的一个评测，其实比较容易有数据污染的问题。因为你的所有的数据，还有它的答都是透明的吗？透明的意思就是说，如果有一天这个东西被爬虫爬了，跑到你的数据集上，那你需要主动的把这个数据从数据里面删去，不然你就污染了这个模型，你的评测的结果就不准的。",
      "speaker": "发言人4"
    },
    {
      "time": "02:52:35",
      "text": "地缘的评测，如果说我这考题不告诉你那这种评测就会遇到一个公平性的问题。大家会觉得我都不知道你在考的是什么，你这个题是不是答案错了，还是我的模型错。就好像大家去参加高考，然后得到了一个不公正的分数。然后你去找人说，我想看一下原卷，人家不给你。就是会有这样的一些顾虑。所以我觉得最理想的就是最终的一个解决方案，可能就是说大家每个人都有自己的一套品测的题目。那除了看到网上已经发的这些评测之外，还要把你把这个评测，把这个模型在自己的这个场景上跑一下，然后给他一个分数来决定说要用这个模型还是用其他的一个模型。",
      "speaker": "发言人4"
    },
    {
      "time": "02:53:14",
      "text": "这样也听听你们做大模型的怎么解决评测的这个问题。",
      "speaker": "发言人1"
    },
    {
      "time": "02:53:18",
      "text": "首先像刚才提到open a little board和travel reina他们的评测的目标其实是不太一样。就风险LMD board的话，其实是有六个比较经典的数据集组成起来之后，大家经常讨论的MLU，然后像出口QA还有阅读理解等等。PSN8K和数学这些的话其实是考察这个模型的基础能力的。我们一般内部用这种数据集其实是评测我们的base language model。就刚才提到平时大家用的其实是我们常说的那个chat language model，那个是很难评的。但是base language model的话，你其实是可以用这些数据集去评它的一些基础的能力。",
      "speaker": "发言人3"
    },
    {
      "time": "02:54:04",
      "text": "我们内部去做评测的时候，其实构建了比较长的时间。因为如果你评测体系不做起来的话，你是没有什么可能选出好的模型出来的。我特别记得greg rossman当时讲了一句话叫evaluation for you need。",
      "speaker": "发言人3"
    },
    {
      "time": "02:54:19",
      "text": "有的评测的这个重要性，我们其实就是把这种评测给建设起来。我知道。这些数据集可以用来评base with model，另一些的话它不能用来评，那我就要去找一些新的。然后这个评base with model这个数据集它评的分越高，它跟后面的这个评测的分数的提升是否是正相关，这件事情也非常关键。这样的话对不同的阶段的话进行分层的评测之后，这样这个team和team之间的话就不会出现打架的问题。就是说今天我建了一个check模型，最后效果不是很好。",
      "speaker": "发言人3"
    },
    {
      "time": "02:54:55",
      "text": "然后这个时候我们开始追责，究竟是谁出了问题，这个post training出了问题呢？是不是training出了问题？这很难解决。所以站在我的角度，刚才提我可能要lead board这些数据其实是比较适合betraying model的人。所以包括我们以及是mixture的话，其实在宣传的时候，尤其mixture最新的那个8乘22B我们在宣传的时候其实是在B上会把其他的选项给点掉，只选那个绿色的，然后去看看我们的水位大概是到哪个份上。像之前我们72B排到这个第一名，然后后来的话mixo把72D超过我们后来出了110D这个分数。",
      "speaker": "发言人3"
    },
    {
      "time": "02:55:33",
      "text": "更好，这个是可以去看这个base model quality，但是check model就非常难评了。但model难评就是你用很多自动的数据很难去体现它有多好。这就是刚才昨天提到的这个问题，就是你可能需要的是人工评测。但是人工评测其实是非常昂贵的，而且它非常的耗时。然后对于学校来说，想做这个事情，你总得有一个好的机制。我觉得这种让人你去盲评二选一，真的是一个很好的一个方法。",
      "speaker": "发言人3"
    },
    {
      "time": "02:56:03",
      "text": "现在开放的rena绝对不是一个小的项目，这是一个全球都在关注的项目。像google release他们新那么开心的时候，我觉得他们都是会讲我们in上的一个份上，然后包括sam去推一个OpenAI的模型，他非常关心IM also跟GPT two和IM跟GPT two的表现。因为人品其实是更加公平公正，以及是更加准一些的方式。除了这种，因为这种其实是比较慢而且比较昂贵的方式。",
      "speaker": "发言人3"
    },
    {
      "time": "02:56:36",
      "text": "在这之前其实还可以有一些中间一些的方式。比如说我们可以自己构建很多自动评测的数据集，去考验它不同的能力，那这个事情就得做的比较全了。像我们自己做的话，就会做非常多的子任务的评测数据集，然后去评他在这些任务上面表现怎么样。我们会去综合看他的一个平均分，然后再去看它每一个具体的分项表现怎么样。",
      "speaker": "发言人3"
    },
    {
      "time": "02:57:03",
      "text": "比如说我们迭代了一个新的模型，然后这个时候它的instruction following能力发生了显著的下降。那这个模型肯定是不太OK，我们就要去看我们的方法出了哪些问题，这是自动评测。自动评测完了之后，我后面其实也做了很多很好的工作，像MP bench。其实你用这个GPT去评，你也能大概看出来说，我这个模型一直大概到哪个水位。有可能人的表人的这个表现的话，其实跟GDP的那个评判，其实还是有比较强的这个观点的。我觉得还算是比较接近的那这个时候的话，在进入check winner阶段之前，可以去做类似这样的事情。",
      "speaker": "发言人3"
    },
    {
      "time": "02:57:45",
      "text": "评测这件事情确实是非常的难。但我觉得不同的阶段的模型的话，用不同的评测数据集去评，我觉得是比较合理的方式。今天的话其实还有很多，实际在用这些模型的开发者以及是企业客户，他们自己会有自己的评测机，那就是发给他们领域的这个任务。我觉得这一块的话反而是相对比较的确的就没有比较明确的说我这个行业应该用哪些数据集去做做一些相应的这个评测。然后大家比较公认的，这也给很多企业用户在选择大模型，使用大模型上的话，造成一些困难。我觉得还需要时间再去对评测做更多的投入。",
      "speaker": "发言人3"
    },
    {
      "time": "02:58:28",
      "text": "多模态要怎么这个评论是不是会相对说更难一些？",
      "speaker": "发言人1"
    },
    {
      "time": "02:58:33",
      "text": "多模态的话其实也是类似的一些问题。但是多模态有比较多比较不错的一些数据集可以供再去评，比如说再去评他一些数据的能力，有lisa然后还有一些OCR的能力。因为今天其实大家已经发现OCR的能力对于这个BR模型的能力非常重要。这些能力的这些数据集放到这里面来。然后现在比较有代表性的就是对标这个MMLU的话，就是这个MMMU的一个数据集。它其实一定程度上能反映这个模型不仅仅是自然语言理解和生成能力，同时还包含它对图像信息的理解的能力，这是自动的一些独特的一些数据集。我觉得它相对来说，这些自动的数据集没有那么多像last every model的这种评测数据集所产生的一些争议。就是你你的模型真的实力好，然后他其实分数就会越高。",
      "speaker": "发言人3"
    },
    {
      "time": "02:59:33",
      "text": "但是其实多模型它也需要人去评的，这种方式就是类似于travel arena的方式。现在的话其实check lino以前在做这个，据我所知当时跟伟林在聊，他现在已经在做这个评审的这个版本了，就是可以去通过上传图片去衡量这个fusion model的表现。所以我觉得很快也会有非常nice的travel rena流行开来。但是我觉得还多模态其实还有更大的空间。因为今天一个大的趋势是训练出来一些unify的模型，就是统一的模型，将多种模态统一在一起。",
      "speaker": "发言人3"
    },
    {
      "time": "03:00:14",
      "text": "只是能做到这个水平的，可能当前世界上可能也就google和AI上会做的比较好吧。但我觉得比如说明年后年肯定很快这个开源社区雨后春笋，会有很多类似这样的一些模型。那接下来的话，可能我们又需要一个针对全模块全方面同类的一个那宝贝，那怎么去兼顾他的评测的公平性，也是这个效率，我觉得都是需要考虑一些问题。所以刚才提这个多么他的评测这个问题其实非常好。我觉得他也是一个开放的问题，需要整个学界的业界去解答。",
      "speaker": "发言人3"
    },
    {
      "time": "03:00:50",
      "text": "卓翰他们这个lab真是做了很多非常非常重要的工作。如果大家不知道的话，其实中央政府代表也是以生产开源商业化的独角兽著称，对吧？在这个lab中诞生了像da bricks，还有ray相对应的any是关于开源开源项目的商业化。比如说on board之前也做过一些讨论，就是开源省的这些项目的商业化的中间的一些机会和挑战。我想周涵我跟大家分享一下这些项目它可能以后商业化的一个潜力和挑战。",
      "speaker": "发言人1"
    },
    {
      "time": "03:01:27",
      "text": "对我可以我可以从VM角度说说这个问题。首先对你看我们这个VM项目是来自于这个burkey的skylab。然后之前前身是rice lab以及m lab。我们实验室有诞生很多像刚刚莫妮卡说的很多很成功的创业项目，就比方说像Spark，最后到诞生到后来的data bricks，以及ray到后来的any skill。所以我们肯定有在想，就是有没有办法把我们的这个项目商业化。然后能够看看能够有一个创业，能够根据这个项目来开一个创业公司。然后这个第一一方面能够是这个是一个很有趣的经历。第二个是也能够保证这个项目的更长期的发展。",
      "speaker": "发言人2"
    },
    {
      "time": "03:02:01",
      "text": "然后我们目前的想法还是觉得，我们还是想要先focus on这个open source本身。我们希望能够在open source上的成功，成功更加重要。然后我们在商业化这样的角度，我们还是在思考当中。因为我觉得我们还是我们实验室还是受data bricks的这一套方法论的影响是非比较深远的。",
      "speaker": "发言人2"
    },
    {
      "time": "03:02:21",
      "text": "我觉得data break成功其实不光是这个open source成功，还有一方面它其实有两次成功。第一次成功是Spark本身它非常快，它的在开展世界影响力非常大我觉得第二个成功是他data break这个产品本身在这个确实是一个非常好的产品。在基于Spark上面做出了足够多的增量的一个创新。并且他给的是一个不同的用户群体，然后他能够获得更多，然后他才能够获得这个商业上的成功。",
      "speaker": "发言人2"
    },
    {
      "time": "03:02:49",
      "text": "然后从VM角度来说，如果我们很简单的做一个like a managed VM service which就是说如果我帮你起这个VM的这class，那这个生意本身其实和一个卖end point的生意是没有什么本质的区别。然后如果你是如果是只是在这个open source的这这个模，如果我们就serve一些open shoes model，然后就是在LEDR的GPU上，如果直接做这样子的一个模式，我们觉得可能是一个打价格战的一个模式，可能是并不是一个特别让我们满意的一个商业模式。所以我们也可能在想，现在有没有什么一些其他的事情可以，其他的这个商业化的方向可以做对对，这个是我们可能目前的一些在商业化上的思考。我们觉得可能暂时还是想要focus on开源的成功，以及开源项目的保证。开源项目的这个问题上来说。",
      "speaker": "发言人2"
    },
    {
      "time": "03:03:40",
      "text": "前面有提到就关于inference的这个成本下降药，让这个大模型能够大规模的使用。我想大家都很关注这个推理成本的下降。卓涵你觉得说你怎么看待到底未来推理成本还有哪些下降空间？哪一些可能是我们期待的模型架构层面去做的，哪一些又是。不要M这样的框架会去贡献的。",
      "speaker": "发言人1"
    },
    {
      "time": "03:04:03",
      "text": "对对对，这是一个非常好的问题。然后我可能最一开始我想把这个问题分成两两部分。第一部分是硬件上的问题，第二部分是软件上的问题。然后硬从硬件上的问题来说，我觉得我们在VM角度出发，我们可能想要做的一件事情是支持更多的这种不同的各种各样硬件，而不是只支持英伟达的这个GPU。然后现在感觉现在首先大家知道英伟达GPU是非常贵，并且在这个很贵的情况下也是非常的短缺的。并且在中国有很多的各种各种贸易禁运的一些受到贸易禁运的影响，所以导致英伟达获取英伟达GPU的成本很高，导致这个音粉各种部署大语言模型推理的成本很高。然后从我们运营角度，我们会希望有更多的硬件厂商加入进来，然后加入进这个能够支持比方说更高效的这个单元模型的推理。然后在这样子的情况下，如果不同硬件厂商本身的之间的竞争，能够让这个大模型推理的成本快速的降下来。",
      "speaker": "发言人2"
    },
    {
      "time": "03:05:02",
      "text": "这是从硬件角度来说，然后从软件角度来说，有我觉得有两种的不同的优化。第一种是我们叫做model ignostic，就是和这个模型无关的优化。就比方说我们之前做的像是这个VM本身最开始做的这个优化上配置的case来管理，以一个更高效的方式来管理这个KV case的memory，然后再加上以及其他的一些technical，像是continue fetching。然后像是最近大家可能比较关心的一些technical，比方说像specular decoding，就是我用一个小模型先来预快速的预测几个可能的输出，然后再用大模型来验证这些小的输出小模型的输出是正确的还是错误的。然后这样子能够加速一个大模型推这个推理的速度。Special另外一些像其他的technique，就比方说我们刚刚一直提到的这个Price cache，我们能够catch一些之前算的算过的一些句子，然后来生成接下来的这个新的句子。",
      "speaker": "发言人2"
    },
    {
      "time": "03:05:58",
      "text": "以及在我们还有一些更多的像这个scheduling上的优化。比方说创新preview，每次就是说我们每次在如果一个用户的输入过长的情况下，我们可以把一个用户的输入切成好几段。然后再每次然后再让整个计算的过程更加的均匀，然后让每次计算的GPU的利用率提上去。以及像是preview disagreed ation，就是说我们可以把这个大语言模型推理处理用户输入的部分以及输出的部分，放到两块不同的机器上，然后来做一个更好的更好的平衡。以及这样子一系列优化，我觉得还是有很多提升的空间的。我刚刚提到的这些优化，很多已经被集成在VRM里面，但是都还是一个刚刚被集成，还没有被性能调优的阶段。我觉得把这些优化全部调优之后，才能够期待我能够有个2到4倍的一个提升。",
      "speaker": "发言人2"
    },
    {
      "time": "03:06:47",
      "text": "我觉得在不同的influences的output层面来说，然后在未来的话，我觉得还有一部分就是我更期待的是这个模型本身的一个变化。就包括我们刚刚一直提到的像是更高效的attention的科技，像是MOE本身这个MOE model的influence的一些优化。然后再以及像更激进的一些模型，像是mamba之类的模型。如果mama他们这样子的这种非attention base的模型，到底能不能给我们带来一个这种比较好的performance。然后能够通过这些模型本身，我们也可以让我们的这个推理加速不少。所以我觉得就是这一系列的从硬件以及model agnostic这个软件层面，以及model本身这优化都能够都都能够还能够让我们的这个LM influence的效率能够提升一大截。",
      "speaker": "发言人2"
    },
    {
      "time": "03:07:34",
      "text": "你说这个VIN角度可以再提升2到4倍。如果执行的好的话，你觉得相应的会对应到我们看到的这个推理成本有多大的一个下降。",
      "speaker": "发言人1"
    },
    {
      "time": "03:07:43",
      "text": "对我觉得首先速度的提升，你可以直接翻译成这个定价的提升，定价的下降，对吧？就是如果如果一个东西本来需要跑2秒，那现在只能跑一秒，那么它的价格理论上来说是可以除以二的对对，但然后我觉得定价本身是一个比速度更加复杂的问题，因为价格是一个供需关系决定的。就比方说以后因为我们能够更容易的获取不同的GPU，那我们的成本也就能够相应的下降下来。我们的电费变得更便宜了，我们的成本也可能能够下降下来。所以我觉得成本下降的空间是比这个速度提升的空间还是要更大的。",
      "speaker": "发言人2"
    },
    {
      "time": "03:08:17",
      "text": "非常感谢大家那么有耐心的跟我们聊了这么长时间。最后我们进入我最期待的一个快问快答的环节。反正准备了几个小问题，大家可以就很快的说说你们的这个想法。我觉得第一个问题，因为今我们录制的这一周，正好正好是open I和google IO的重磅的发布会。大家可以简单分享一下，在这两场发布会上，你们觉得让你们印象最深刻的是什么？你觉得比起你原来预期来说，哪一些是超出预期，哪些是不及预期的地方。",
      "speaker": "发言人1"
    },
    {
      "time": "03:08:48",
      "text": "欧美原来那个的话，我觉得他那个TDS的话确实做的非常的惊人。因为他做的非常的自然，而且涉及打断的功能。我觉得OK还是投入了非常多的功夫在做这件事情。看他的博客的话，甚至有interpret的这个组，我不太确定是不是在干这个事情。所以我觉得OpenAI我觉得你做出来的东西很很让人震撼，但三号他可能给我的产品的感觉可能会更强一些。可能我还是更期待GPT five的出现，毕竟对OpenAI的要求比较高，这是OpenAI。",
      "speaker": "发言人3"
    },
    {
      "time": "03:09:27",
      "text": "然后google这一边的话就是没有等到germany nine two，有些遗憾。但是context能提到two two million token的话也非常好。可能比较impressed的是那个product s，我觉得如果google能够把它的长序列的大模型跟A准结合的比较好的话，反而是有可能通过google看到大模型落地比较硬核的场景的可能性。",
      "speaker": "发言人3"
    },
    {
      "time": "03:09:57",
      "text": "你觉得说他这个把我打断，还有这个语音的这种自然程度模型本身能力的事情，有多少是可能一些微调的东西。你觉得别人要追上这样的一个程度，他的最大的考验的是什么？",
      "speaker": "发言人1"
    },
    {
      "time": "03:10:13",
      "text": "对我觉得跟欧洲人还是有一定的差距的。因为他其实把多种模态的话都做到n to n我不能完全保证它生成方面也是NDA。如果它生成方面也是基于transformer整个连头N去做的话，那是非常惊人。但从他的表现出来的水平来说的话，不太像是一个pipeline的东西。所以在这个技术层面上的话，我可能还是至少在多模态这个方面，我觉得是非常领先。",
      "speaker": "发言人3"
    },
    {
      "time": "03:10:43",
      "text": "我特别同意正阳刚才说的那些观点，然后我想加一个就是OpenAI它做了一个桌面的应用。我觉得这个其实如果这个东西做得好的话，可能算是一个里程碑。因为之前大家需要打开一个网页，然后在里面去去跟这个open来说话。现在它变成一个桌面的应用，有可能它就会长期的驻留在用户的桌面上，变成他以一个无时不在的一个助理，随时有遇到什么样的问题，就把共享屏幕的这个权限交给OpenAI，然后让他帮你去处理。甚至未来OpenAI也可以比如说开放这个接口，然后设计一个agent，能够帮你在屏幕上点点点去完成很多工作。就把你自己这个工作跟大模型通过这么一个桌面的应用，再加上视频里视觉理解的这么一个多模态的模型，然后就无缝的衔接起来了。我觉得这里面想象空间会非常大对。",
      "speaker": "发言人4"
    },
    {
      "time": "03:11:36",
      "text": "我印象最深的是real time，就是open I的demo。所有的demo都是在表示在我说完话之后能够立刻的开始说，并且我也能够随时的打断，并且能够做出很多real time interaction。我觉得这个东西是我最关注的，因为我本身是做这个推理框架的。这个推理框架做到极致，就是做到这个time。我觉得OpenAI做的这件事情让我非常的respect。对我觉得这个是我非常的关非常觉得非常impressive的事情。我觉得在google这边我还是觉得这个project extra，特别是他的那个demo，就是能够看到很长的一段视频之后，并且能够recap之前视频当中出现的某一帧里面的某一个object。我觉得这个还是一个非常impressive.",
      "speaker": "发言人2"
    },
    {
      "time": "03:12:19",
      "text": "的一个result。今年一年的话，我觉得能够畅想的或者是今年之内，开源的language model的话，是真正意义的全方位的超越了GP服务，真的能让大家用起来。因为今天其实还是可以看到，即便是arma three它的travel reno表现很好。但是其实可以看到它在实际使用的时候，还是会跟GPT过会有一些差距。在地缘领域的话，肯定是希望有像GPT five这种理解能力要完全超越他们。我觉得结合open demo可能会更直接一点，就是能让我们这个open demo能够帮用户解决真实问题。微电池化我提到50分以上，这可能是一年然后三年的畅想的话，那就是真正意义的多模态的大模型。对于物理世界都有非常好的理解，同时具有非常好的知识储备和非常强的推理能力。",
      "speaker": "发言人3"
    },
    {
      "time": "03:13:17",
      "text": "对我觉得未来一年我最大的期待就是能够open source model能够赶上还有GPT4这个performance。对我觉得未来三年我已经不敢想了。我觉得三年之前的这个世界是是是和现在是已经完全不一样的一个世界的。",
      "speaker": "发言人2"
    },
    {
      "time": "03:13:33",
      "text": "我觉得未来一年我更希望说我们进一步能够降低推理的成本，甚至说比如说M4奥创这个芯片出来之后，我们能够在端上非常快的去跑这个啊妈妈370B甚至是千万110B这个级别的模型。未来三年我的愿景就是希望这个大模型能够跑在能够无处不在，跑在各种比如说嵌入式系统，然后跑到我们生活当中的每一个地方。然后甚至有可能会出现，我们就买一个简单的那种小的模组或者一个小的芯片，然后它里面就已经带一个足够够用的大模型，然后把它放到那，然后就把原来的设备升级成有大模型智能的这么一个产品。这个可能就是不知道三年能不能做到，但是这是我的一个比如说三五年的这么一个愿景和期待。",
      "speaker": "发言人4"
    },
    {
      "time": "03:14:21",
      "text": "几位在分别就你们所负责的这个项目，这个濯涵的VRM，然后俊阳的这个queen还有这个铁证在哈根费。接下来的工作就下一个阶段你们自己的项目上最值得期待的事情。",
      "speaker": "发言人1"
    },
    {
      "time": "03:14:37",
      "text": "快点，在发完快two之后，我们是期待今年看能不能把块free给做出来。目前我们现在能做到水平的话是语音、图像和文本都能统一起来去做理解相关的工作。如果这个模型能推出来的话，我觉得快应该能帮到不少人。可能今年比较重点是瞄准靶快速给做好这件事情。如果是open driven的话，真的让他能够被某些人真正的用起来。我觉得这个比较直白，就是能够固定的解决某一些代码方面的问题。有人拿去解决真实场景的问题，比如fake issue或者是像我想的帮助滴滴学习工程师能够使用open devon去训练模型，部署模型。",
      "speaker": "发言人3"
    },
    {
      "time": "03:15:32",
      "text": "我还是希望有更多非常不错的国内的开源领域的一些工作出来。然后甚至刷一些中国发起的一些组织，然后这些组织能够也是非常大的影响力。",
      "speaker": "发言人4"
    },
    {
      "time": "03:15:48",
      "text": "对我觉得从这个VM角度来说，我们的目标还是是成为一个open force LM serving的一个standard。就希望大家想要做这个LM退役的时候就是用我们。然后我们总的来说，从我们就伯克利的这个核心的team的角度来看，我们想希望enable两件事情。第一件事情是更多的hardware support，更具体来说就是我们怎么样能够design我们的系统，使得加一个新的硬件厂商想要添加它的这个硬件支持是一个是一使得使得这件事情是成为一个相对比较容易的事情。然后第二个事情是还是针对于这种新的这种各种各样新的优化新的功能来说，以及一些新的这种模型上面的优化来说。我们怎么样能够让VM这个软件的结构本身，使得它非常容易能够加新的这个feature，能够加新的功能，然后使得VM能够更加rose，能够更加能够经受得住未来的这些模型的考验。",
      "speaker": "发言人2"
    },
    {
      "time": "03:16:47",
      "text": "其实就像我之前就卓翰之前说的，虽然说我们都觉得这个领域变化太快，都不知如何预期未来做一个预言。但是我想真正的这个未来其实在这个创造者的手里去出现。非常感谢几位时间，也希望我们今天的这个讨论能够对大家也都有所帮助。也希望更多的人来参与到这个真正的开源社区可的建设中。也欢迎大家去关注几位嘉宾接下来的很多项目，还有活动和进展。",
      "speaker": "发言人1"
    },
    {
      "time": "03:17:16",
      "text": "好，再次感谢大家。恭喜你终于听完了，是不是跟莫妮卡一样，感觉有点疲惫又有点信息轰炸的开心。希望你跟我们一起追寻最前沿的技术研究，做一线的实战思考。最后我们呼吁，如果你认识具有行业影响力作品的顶尖研究员，或者有深度思考和阶段性成果的创业者，都欢迎通过公众号或者邮箱等方式推荐给我们我们的邮箱是on board点forecast点2024 at g mail dot com，on board点podcast点2024 at g mail点com。我们会给推荐嘉宾的你一些小小的福利和惊喜。希望我们卧虎藏龙的听众们跟我们一起探索这个令人兴奋的AI和软件的未来，连接更多未来的缔造者。期待你的来信。",
      "speaker": "发言人1"
    },
    {
      "time": "03:18:09",
      "text": "以上就是本次播客的全部内容，感谢大家的收听，希望对你有所启发。如果你喜欢我们播客的内容，欢迎你点赞分享，在评论区写下你的心得。另外onboard也有听众群了，添加小助手微信ID on board 666。再说一次非常好记。666加入听众群了解更多互动机会。另外如果有喜欢两位主播用爱发电，也可以在小宇宙给我们打赏，请我们喝个咖啡。如果你在用apple podcast收听，也希望你能花几秒钟给我们打个分，打个五星好评，让更多人可以了解到我们我们下期再见，继续更多干货。",
      "speaker": "发言人1"
    }
  ],
  "lab_info": {
    "summary": "在这次讨论中，几位技术专家聚首，聚焦于大模型技术的开源生态、架构创新、以及商业应用等多个维度。他们深入探讨了大模型技术的进展，特别是开源模型如阿里通义千问在国际舞台上的影响，强调了开源在推动AI发展中的不可或缺性。讨论涵盖了模型架构的演进，如拉马架构（LAMA）的应用，以及开源模型面临的挑战与机遇，指出通过社区协作可加速技术进步。同时，探讨了模型开发中的技术难题，如序列长度处理和推理效率提升，凸显了这些挑战对架构创新的驱动作用。此外，对话还涉及了学术界与工业界在大模型研究上的合作，以及模型在不同应用场景下的优化和商业价值实现。整体而言，这次讨论展示了开源社区在技术创新中的关键作用，以及平衡学术研究与工业应用的重要性，共同勾勒出大模型技术发展的未来图景。",
    "qa_pairs": [
      {
        "question": "上周日我们在北京举办了on board第一次线下听友会，活动内容有哪些重点部分？",
        "answer": "听友会上，我们讨论了从机器人到AI创业投资再到软件出海等多个主题，干货满满。一百多个人的场地座无虚席，直到活动结束大家仍不愿离去。",
        "time": "00:00:15"
      },
      {
        "question": "通义千问模型的最新进展是什么？今天我们讨论的主题是什么？",
        "answer": "就在今天凌晨，阿里发布了通义千问模型的72B开源模型，这个模型的表现相当出色。今天我们要讨论的是大模型的开源生态，特别是在生成式AI领域，开源的发展突飞猛进，包括模型能力的提升、生态系统的完善以及来自中国的开源模型崛起。",
        "time": "00:01:06"
      },
      {
        "question": "对于开源大模型生态，有哪些值得关注的话题？",
        "answer": "我们深入探讨了开源大模型生态的多个方面，包括底层基础大模型的开源闭源演进、开源模型商业化与过去大数据时代的开源商业模式异同，以及如何做一个有国际影响力的开源项目等。",
        "time": "00:03:49"
      },
      {
        "question": "参加这次讨论的嘉宾有哪些背景？",
        "answer": "王铁镇是hugging face的工程师，他作为连接中国和世界开源AI生态的关键人物，对整个AI开源生态有深入观察；俊扬来自通义千问团队，在开源社区里主要负责开源相关事务；卓涵是大模型推理框架BLM的作者，也是开源计算框架ray等知名开源项目的贡献者，他从大模型周边生态和国际视角分享了很多技术理想的干货。",
        "time": "00:02:19"
      },
      {
        "question": "是否在节目中提到的开源社区和项目有商业赞助？",
        "answer": "在节目中重点提到的开源社区如hugging face以及开源项目如阿里千问、open devin等，我们都没有收取任何广告费，全程无广，都是基于嘉宾的走心分享。",
        "time": "00:04:57"
      },
      {
        "question": "王铁震可以分享一下自己进入AI和开源领域的工作经历吗？",
        "answer": "我叫王铁震，是hugging face的工程师。我之前在谷歌做TensorFlow相关工作，后因对开源的热爱跳槽到了hugging face，致力于推动开源行业的发展并在国际上提升影响力。",
        "time": "00:06:00"
      },
      {
        "question": "俊阳能否介绍一下自己和当前的工作重心？",
        "answer": "大家好，我是林峻阳，通义千问团队的一名算法工程师，专注于大语言模型和多模态大模型的研究，并负责开源相关事务。目前的主要精力集中在通义千问系列以及open UI等项目上。",
        "time": "00:11:34"
      },
      {
        "question": "你们项目的初衷是什么？",
        "answer": "项目的初衷是希望构建一个使用local large language model实现的、模型不是闭源的demo，以便未来能够应用于实际场景并解决真实问题。",
        "time": "00:13:53"
      },
      {
        "question": "目前项目的进展如何？",
        "answer": "项目发展迅速，UI和基本功能已经齐全，并且我们有相应的agent在sweet dash like上运行。最近的测试结果显示通过率为25%，效果尚可，下一步目标是将其应用到真实场景中解决实际问题。",
        "time": "00:14:44"
      },
      {
        "question": "开源项目VLM是做什么的？VLM中配置attention技术的作用是什么？",
        "answer": "VLM是一个开源的大语言模型推理和部署引擎，它包含了一系列优化技术，如配置attention算法、continue batching CUDA graph模型、量化模型、并行prev decoding等，能显著提高大语言模型推理速度和吞吐量，目前支持一键部署hugging face上的主流模型。配置attention技术用于管理transformer里的attention操作中的KV缓存内存，通过操作系统层面的 paging和virtual memory 技术，可以提高4至5倍的内存利用率，并带来约四倍的吞吐量提升。",
        "time": "00:16:02"
      },
      {
        "question": "VLM在哪些方面得到了应用？",
        "answer": "VLM被多家云厂商如AWS、Google Cloud、Oracle Cloud以及微软Azure等采用作为默认推理引擎，同时在苹果、IBM、Snowflake等多个公司中都有部署使用。",
        "time": "00:17:01"
      },
      {
        "question": "您关注的开源项目open David有何特点？",
        "answer": "open David是一个复杂且有用的agent框架，它能够多次调用大语言模型，为推理引擎优化提供更多可能性和研究机会，特别是在针对特定应用的优化方面。",
        "time": "00:17:52"
      },
      {
        "question": "VLM是如何想到并创建的？",
        "answer": "在2022年底，为了演示一个大语言模型demo，我们发现市面上缺乏专门做大模型优化的开源系统，因此决定自行创建VLM来解决大语言模型推理效率低和GPU利用率低的问题。",
        "time": "00:18:42"
      },
      {
        "question": "VLM相比普通推理框架有何优势？",
        "answer": "VLM整合了一系列针对大模型优化的方法，如连续 batching 和配置attention等，能将推理成本降低1至2个数量级，比直接使用hugging face transformer更为高效。",
        "time": "00:23:18"
      },
      {
        "question": "VLM解决了大家在模型部署上的哪些问题？之前大家是如何处理这些问题的？",
        "answer": "VLM显著提升了模型的推理速度，用户反馈之前在使用BR（可能指BLM）时经常出现速度慢、不方便的问题。而选择DRM、any face的TGI等推理部署框架，如DRM因其易用性和较低的学习成本受到青睐，许多用户和企业都采用VLM进行部署，并取得不错的效果。",
        "time": "00:28:31"
      },
      {
        "question": "国内和国外在使用模型部署方面有何不同？",
        "answer": "国内主要使用VLM，也有部分用户采用英伟达的TSRTLM和其他本地部署方案，但VLM的用户量相对更大。",
        "time": "00:30:00"
      },
      {
        "question": "open dev d这类应用为何对你们有帮助，它有什么特点？",
        "answer": "对于学术界来说，open dev d是一个非常有研究价值的use case。它对LM有一个复杂调用，不仅限于简单的问答，还能进行多轮对话并接收环境反馈以优化LM推理效率。通过将对话信息整合到推理引擎中，可以利用上一轮的结果加速下一轮预测，降低成本。",
        "time": "00:31:58"
      },
      {
        "question": "与open GPT等早期agent框架相比，open devin的需求和优势有何不同？",
        "answer": "open devin是一个更为成熟且具有复杂环境交互能力的agent系统，相比open GPT，它能更好地处理多轮交互和特定应用场景，背后有活跃且认真的开源社区支持，能有效降低开发成本并带来新的挑战与机遇。",
        "time": "00:32:53"
      },
      {
        "question": "对于GPT4 API提供的多轮对话支持，以及Context Caching技术，你们有何了解和看法？",
        "answer": "Context Caching技术有助于解决长序列处理问题，避免每次对话都重新计算整个历史上下文。目前，社区关注如何结合部署推理与agent类型应用，以实现更高效、经济的多轮对话场景。推特上有关GPT4可能支持Context Caching的讨论引起了注意，但具体实现细节尚未明确。",
        "time": "00:39:52"
      },
      {
        "question": "站在open dev的角度看，当前agent在哪些场景表现得比较好？",
        "answer": "站在open dev的角度，agent在一些基础任务上表现得还行，例如summerization（摘要）、写report等简单应用。比如酷EAI的项目，通过文达课程教授大家如何制作这类agent，并应用于实际场景中，如摘要、翻译和文字创作生成。此外，在推特上也有专门帮人写作的agent，通过编程实现生动文章或小说的撰写。",
        "time": "00:41:20"
      },
      {
        "question": "在coding场景中，open dev有何重要作用？",
        "answer": "在coding场景中，open dev扮演了重要角色。它可以帮助解决实际问题，比如机器学习工程师利用BRM部署模型时，遇到不清楚如何部署某个大型模型的问题，open dev能提供解决方案。随着GPT-4与open demo结合的进展，未来几个月内，前端工程师可以通过与open dev交互来完成相关工作，比如找到并部署所需模型。",
        "time": "00:42:51"
      },
      {
        "question": "目前open dev面临哪些挑战及社区需要补充哪些工具？",
        "answer": "目前open dev需要解决的问题包括缺乏高效的评测工具，目前可用的如sweet bench较为复杂，难以快速准确地评估模型性能。此外，期望有更多工具和服务提供良好的评测环境，让研究人员能更专注于创新，同时降低构建复杂agent的成本。数据环境的优化也很关键，需要构造合适的数据集以提升模型在特定场景下的表现和稳定性。",
        "time": "00:46:07"
      },
      {
        "question": "多模态agent的发展现状和挑战是什么？",
        "answer": "多模态agent正成为一个发展趋势，因为它能更好地理解物理世界的信息，从而做出更优决策。例如，一个能通过自然语言与手机屏幕交互的多模态agent，需要理解视觉信息以实现直观操作。然而，现有的大语言模型对于多模态理解仍有局限性，幻觉问题较为突出，尤其是在实时性和细节检测方面。因此，提升多媒体foundation model的能力是当前的重要挑战。",
        "time": "00:48:30"
      },
      {
        "question": "A类（agent）其实分为哪两部分？",
        "answer": "A类分为两部分：一是大语言模型本身，需要更好地适应agent workload；二是为agent提供一个适用的环境，这更适合开源社区来做。",
        "time": "00:58:41"
      },
      {
        "question": "未来会形成一个怎样的生态形态？",
        "answer": "未来可能是更开放的生态，每个企业专注于自己最擅长的部分，例如模型公司专注于模型研究，开源社区负责设计各种agent并帮助它们更快地创造内容。整个生态分层合作，每一层拥有标准接口并与下一层紧密联动。",
        "time": "00:59:41"
      },
      {
        "question": "是否会出现一个从头到尾做所有事情的巨无霸公司？",
        "answer": "不太可能出现这样的巨无霸公司，而是倾向于开放的合作模式，各个公司发挥各自优势，比如模型公司和应用层公司各自做好自己的部分，同时相互学习和借鉴对方的know-how。",
        "time": "01:01:58"
      },
      {
        "question": "为什么会出现这么多做agent framework的公司，以及为何需要它们？",
        "answer": "出现很多做agent framework的公司是因为开发者需求多样，审美和应用场景各不相同，每个框架设计者对agent的理解和封装方式也有所差异。同时，很多开发者可能并不依赖框架，而是选择手写agent逻辑，这导致了多种框架的涌现。目前没有一个主导的agent framework，是因为众口难调和技术创新的氛围。",
        "time": "01:04:32"
      },
      {
        "question": "在猫咪agent mod age的背景下，为什么使用多个大语言模型进行协作会比单个模型表现得更好？",
        "answer": "这是因为真实的场景中，任务的复杂性很高，单个模型可能无法处理这种复杂性。通过将任务分解给不同的agent，如产品经理、运营、前后端开发等角色，可以模拟现实世界的公司运作模式，让每个模型专注于不同的任务，从而实现更好的效果。",
        "time": "01:09:18"
      },
      {
        "question": "目前的技术水平下，是否确实需要使用multi-agent框架来解决真实任务？",
        "answer": "是的，从当前的技术水平来看，对于解决真实任务，确实需要利用multi-agent框架来封装和分配任务，因为单个模型可能无法应对复杂多变的需求。",
        "time": "01:10:31"
      },
      {
        "question": "多agent框架是否意味着我们在利用人类先验知识进行任务拆解，而模型自身也能更好地完成这些任务？",
        "answer": "是的，使用multi-agent框架相当于将部分人类先验知识融入其中，预先设定好任务分工。但长远来看，如果未来的大语言模型能够自行高效拆解任务，那么可能就不需要依赖于人类预设的框架了。",
        "time": "01:11:28"
      },
      {
        "question": "开源社区在过去一年中关于L模型的发展有哪些重要变化和进展？",
        "answer": "在过去一年多的时间里，Hugging Face上的开源模型数量从15万个增长到了66万个，数据集的数量也大幅度增加。微调技术、模型合并、量化以及不同格式之间的转换等发展趋势为开源开发者提供了更多机会，同时，中国社区在开源模型和数据集方面取得了显著进步，出现了多个优秀的社区项目和国际合作组织，推动了整个社区的快速发展。",
        "time": "01:19:13"
      },
      {
        "question": "在海外，大家对于中文大语言模型的认知情况如何？",
        "answer": "韩国等海外对中文大语言模型的认知比预想的要好一些，他们知道像百川、千问等模型，但对DeepSik的认知较少。在海外分享和推广中文模型及其技术的重要性较高。",
        "time": "01:22:41"
      },
      {
        "question": "目前在海外的影响力如何？以及对于开源模型的看法和推崇的开源方式是什么？对于开源模型的数量增长和质量对比问题，能否具体说明？",
        "answer": "我们意识到在国内的开源模型影响力还不够大，特别是在海外。关于开源模型，存在不同层次的开源方式，例如只开放模型权重（open service）、开放模型数据集和训练技术信息、提供脚本以供了解模型构建过程，甚至还有开放合作组织形式，如被扣项目，鼓励社区成员参与贡献和微调工作。开源模型的数量增长主要是由于衍生品的增多和模型范围的扩大。以千问为例，一个基础模型可以衍生出上百个不同大小和特定条件的模型。此外，社区也在不断对模型进行微调和扩展，形成了庞大的模型家族，这使得开源模型的数量看起来非常庞大。",
        "time": "01:24:52"
      },
      {
        "question": "衡量开源模型受欢迎程度的有效指标是什么？",
        "answer": "除了参考模型的访问量或下载量外，更重要的是观察该模型是否有大量衍生品、被广泛应用和微调，以及在社区中有足够的影响力。例如，在海面上搜索特定开源模型，看到的数量越多，反映出其在社区中的影响力越强。",
        "time": "01:33:56"
      },
      {
        "question": "关于开源模型的数量统计问题，如何准确计算？",
        "answer": "统计开源模型数量时，需要注意区分模型系列和单个模型。比如千问模型就有多个系列和分支模型。同时，考虑到模型的多种量化版本和不同架构的模型，总数会非常庞大。而且，由于存在大量的模型衍生品，实际数量可能远超过已公开的数据。",
        "time": "01:29:38"
      },
      {
        "question": "模型的训练方式（如fine-tuning或retraining from scratch）在评估模型能力时的地位有何不同？",
        "answer": "模型训练方式分为基于现有模型权重进行fine-tuning增强特定领域能力，以及使用同一架构但重新训练模型参数（retraining from scratch）。这两种方式各有侧重，fine-tuning更侧重于快速适应特定任务，而retraining from scratch则从零开始训练，可能带来更深层次的改变和优化。",
        "time": "01:34:27"
      },
      {
        "question": "在大语言模型领域，为什么有公司选择直接使用拉马架构，而不是探索新的架构？对于不同类型的公司，应如何选择适合它们需求的大语言模型架构？",
        "answer": "这是因为拉马架构是目前社区中最大的模型社区，很多模型都基于此架构。对于一些业务导向的公司来说，他们更关注快速满足自身业务需求，采用拉马架构可以迅速解决痛点，无需在架构上花费过多成本。业务导向的公司主要看能否迅速满足自身需求，选择能够快速实现业务目标的架构；而基础模型公司可能需要支持多种语言和小语种，因此会选择更多的语料进行训练或创新新的架构以适应更多场景；同时，有些公司出于节约成本或探索新领域的目的，也会创建自己的新架构。",
        "time": "01:35:54"
      },
      {
        "question": "使用其他架构而非拉马架构时，面临哪些挑战和社区支持方面的问题？当初你们为何不采用拉马架构，而是选择了自研架构？",
        "answer": "如果选择其他架构，可能会面临需要额外社区支持的问题，例如数据无法直接使用，需要自行处理。此外，在宣传和应用过程中需明确指出基于何种架构，同时要注意某些开源项目如meta发布的限制，以及国家和地区对模型使用的法规要求。当初在研发过程中，我们发现拉马架构与我们自研架构的差异较小，但通过内部实验发现使用特定技术（如UKB bias）后训练效率和效果有所提升。尽管存在对拉马架构的争议，但我们认为应该拥抱开源社区和拉马的发展，因为关注点在于模型质量和实际效果，而非架构本身。",
        "time": "01:35:54"
      },
      {
        "question": "对于未来模型架构的发展趋势，有何看法？",
        "answer": "未来大模型公司应关注模型质量和易用性，同时可以探索非拉马架构的新玩法，比如利用MLOA技术降低训练成本，或者尝试MOE架构等新型结构来应对大规模训练和长文本处理等挑战。随着社区对不同架构接受度的提高，未来可能会看到更多非拉马架构的大模型出现。",
        "time": "01:35:54"
      },
      {
        "question": "沿用之前的model architecture本身是否存在问题？",
        "answer": "沿用之前的model architecture本身没有问题，这是一个非常好的做法。如果出于功能性改动，也是有意义的。",
        "time": "01:49:50"
      },
      {
        "question": "是否有新的架构涌现并且变得流行？",
        "answer": "是的，从去年下半年开始至今，许多新架构不断涌现并变得流行，例如RWKV和复古IN等。",
        "time": "01:50:08"
      },
      {
        "question": "对于MOE架构的发展情况如何？",
        "answer": "MOE架构早年有探索，但当时没有明确定义如何配置expert数量和激活条件以达到最佳效果。而mystery团队清晰地传递了激活14D参数可大致达到训练28B dance模型的效果，这使得MOE架构更具吸引力。",
        "time": "01:50:29"
      },
      {
        "question": "新架构的探索有何必要性？",
        "answer": "新架构的探索非常必要，它们有助于提供更solid的基础模型，对开发者和企业用户都非常有益。新技术开发者面临的挑战是如何让技术变得流行起来。",
        "time": "01:51:14"
      },
      {
        "question": "新架构在开源生态中的作用是什么？",
        "answer": "新架构的开发者会在生态中做适配工作，若技术能帮助生态发展，被大家认可，整个领域将发展得更快。",
        "time": "01:51:50"
      },
      {
        "question": "对于商业化角度来说，新的架构有何意义？",
        "answer": "新的架构可能提供不同的感受，当现有模型达到一个分水岭后，新的架构可以带来差异化竞争，如更低的成本、更好的部署性能或更强的指令追随能力。",
        "time": "01:53:44"
      },
      {
        "question": "开源模型与闭源模型在商业化上的差异是什么？",
        "answer": "开源模型可以展示实力，让企业在部署时考虑自有部署或购买API。当开源版本无法满足需求时，企业可能会转向购买API，从而带动开源项目的商业化收入增长。",
        "time": "01:55:44"
      },
      {
        "question": "如何在成本降低的同时保持模型效果？",
        "answer": "可以探索新的模型架构，使其在推理延迟、GPU内存需求等方面优于现有架构，例如通过改进解码技术、使用更高效的架构设计等。",
        "time": "01:54:13"
      },
      {
        "question": "技术演进对开源项目商业化的影响是什么？",
        "answer": "技术演进可通过开源版本吸引企业关注，当企业发现自有部署不如API划算时，开源工作推动了API收入上涨，实现了开源与商业化的结合。",
        "time": "01:55:44"
      },
      {
        "question": "开源和研发之间的关系如何？",
        "answer": "开源和研发是选择问题，研发水平和算法基础架构决定开源的速度。通常情况下，公司会优先自用新技术以保持技术优势，而非急于开源。当开源技术经过充分验证并得到社区欢迎时，才会将其推出去。",
        "time": "01:57:07"
      },
      {
        "question": "何时知道现有架构遇到瓶颈，需要探索新架构？",
        "answer": "当现有架构在推理部署、长序列处理等方面出现问题或达到性能天花板时，业界会开始探索新的架构，如针对Lama架构存在的问题，团队会进行技术创新并推动至开源社区。",
        "time": "01:59:22"
      },
      {
        "question": "在项目开发中，对于新技术的采用存在哪些挑战和困境？",
        "answer": "新技术在推广时会面临一个困境，即如果所有人都不使用，我们无法判断这项技术的好坏。一旦大家开始尝试后发现效果并不理想，可能会浪费大量时间和开发资源。因此，在决定是否支持新技术时，项目团队会考虑投入一周的开发时间去评估其效果。",
        "time": "02:03:53"
      },
      {
        "question": "当前是否有针对新技术优化的方法论？",
        "answer": "目前并没有一个特别成熟的方法论来系统性地优化新技术，很多时候依赖于业内KOL的感觉和口耳相传。例如，针对某些技术的底层代码编写和实现细节，如flash attention层面的支持，需要有深厚专业知识的团队才能完成。",
        "time": "02:04:23"
      },
      {
        "question": "阿里巴巴等大型团队如何评估并决定是否接入新技术？",
        "answer": "大型团队会通过展示扎实的工作成果、与领域内KOL沟通交流，并实际将优秀技术贡献至对应仓库（如VOM）。至于VOM如何评估是否接入新技术，这是一个很好的问题，实际操作中可能是根据模型在训练阶段的表现和最终效果来决定是否支持。",
        "time": "02:05:02"
      },
      {
        "question": "接下来哪些模型或架构有望带来质的提升，以及还有哪些值得关注的工作和方向？",
        "answer": "lama结构因其良好的通用性和可扩展性，尽管已经较为成熟，但仍有优化空间，比如引入MOE架构的相关优化以及针对长文本处理时注意力机制的改进，例如moba结构，以降低计算复杂度。",
        "time": "02:06:44"
      },
      {
        "question": "学术界对于开源LAM模型的贡献现状及未来如何看待？",
        "answer": "学术界在LAM模型上有许多fine-tuning和预训练工作，但由于大模型已证明商业价值，商业公司投入更多资源并占据较大声量。此外，学术界受限于计算资源，更倾向于聚焦特定领域进行深入研究，如agent测试或特定任务上的优化。尽管学术界在模型架构上的探索很重要，但商业化闭环方面的投入相对较少。",
        "time": "02:11:41"
      },
      {
        "question": "VOM项目的未来计划是将其发展成具有商业化闭环的商业项目还是维持作为社区向的开放项目？",
        "answer": "VOM项目的长远计划是保持作为一个成功的开源项目发展，目前已有来自不同公司的贡献者积极参与并推动项目进步。项目组致力于协调各方努力，保证VOM能在长期发展中不断壮大，同时也会关注用户需求，如通过投票了解用户偏好，以便更好地推广和发展该项目。",
        "time": "02:14:37"
      },
      {
        "question": "在大公司或大模型开发团队中，为何在技术方案上不倾向于过于激进地尝试新事物？",
        "answer": "因为训练一个大型模型非常耗时且成本高昂，所以通常会选择更为稳健的方案来确保模型训练出的效果足够好。",
        "time": "02:17:35"
      },
      {
        "question": "学术界在大模型领域可以做出哪些有意义且创新的工作？开源社区如何促进大模型普惠化？",
        "answer": "学术界可以进行很多创新工作，例如斯坦福大学的DPU就是一个很好的例子。它在去年开源后，迅速被社区验证并广泛应用，帮助研究人员通过学习人类偏好来优化模型，大大推动了整个行业的发展。开源社区通过项目如honey face、lamer TCPT以及MLX等不断推动大模型的普惠化，使得像lambada和MX这样的开源工具可以在个人电脑上方便地使用，为迭代更好的模型带来很大帮助。",
        "time": "02:20:06"
      },
      {
        "question": "开源数据集在整个行业中的作用是什么？对于数据的开源，有哪些动力来源以及面临哪些挑战和机会？",
        "answer": "开源数据集为大模型训练提供了重要资源，但高质量中文语料仍较为稀缺。目前大部分数据来源于Common Crawl，但受限于其筛选规则，中文数据占比不足5%。解决这一问题需要草根力量如MNBBC收集清洗高质量中文语料，以及官方视角如CCI联合单位贡献数据集。此外，随着知识产权讨论的深入，未来开源数据集的合法性和界定将成为关注点，期待向更友好的开源方向发展。",
        "time": "02:24:32"
      },
      {
        "question": "在做大模型过程中，数据方面的研究方向和新的挑战有哪些？",
        "answer": "预训练阶段的数据获取已有一定共识，但数量上仍有上升空间，尤其是达到10T以上的高质量数据。而在post training阶段，构造不同类型的高质量数据（如1000万条instruction tuning的数据）以及更具体场景下的数据标注是未来的探索重点。同时，如何明确模型能力边界并构建相关数据以支持发展，也是数据研究的重要方向。",
        "time": "02:26:42"
      },
      {
        "question": "当时你们在内部讨论中发现了问题出在哪里，以及如何向他人学习以提升自己？",
        "answer": "我们当时通过讨论发现，我们的问题主要在于对细节的忽视。为了改进这一点，我们做了许多三方生态适配的工作，包括修改代码以适应不同场景。看似轻易成功的部分背后，实则蕴含着大量的细节工作。",
        "time": "02:32:26"
      },
      {
        "question": "在国内对于开源模型的推广和使用上，你们认为有哪些方面需要加强？",
        "answer": "在国内，我们相对较少地进行开源模型的推广和教育工作，比如如何利用模型的优势、如何讲好自己的故事等。我们通过例如queen 1.5版本后配套官方博客的方式，将这些信息以博客形式传递给开发者。",
        "time": "02:32:55"
      },
      {
        "question": "在国内和海外宣传方面，你们有哪些值得分享的经验？",
        "answer": "在国内，我们算是做得比较好的海外宣传之一。对于想要打造有海外影响力的大型模型或项目的团队，可以从我们正阳这里学习经验。关键在于不仅要关注宏观战略，更要注重实现这些战略的细节，例如通过高质量的内容、多渠道推广、社区互动等方式来提升模型的曝光率和使用率。",
        "time": "02:33:57"
      },
      {
        "question": "如何在模型发布时确保最大程度地吸引用户并保持热度？",
        "answer": "在模型发布时，应考虑制作吸引人的中英双语或英文介绍，突出模型亮点、解决的问题以及评测效果，并配以有吸引力的图片和教程。同时，添加合适的meta data方便模型在平台上的搜索。此外，提前与社区团队沟通协调，确保模型能够在各大平台同步发布，利用热点效应迅速抓住用户注意力。",
        "time": "02:34:48"
      },
      {
        "question": "对于开源模型的成功，模型本身的质量有多重要？",
        "answer": "尽管宣传和易用性很重要，但最终决定一个模型能否取得成功的关键还是其本身的质量。比如E模型，虽然一开始因其结构类似LLAMA而引发争议，但经过验证，其训练效果更好，质量更高，因此得到了用户的广泛认可和采纳。同时，模型的商业化许可也非常关键，例如E模型最新推出的Apache 2.0许可，大大促进了其在社区中的应用和推广。",
        "time": "02:40:03"
      },
      {
        "question": "在VM团队的发展方向上，你们的核心目标是什么？",
        "answer": "我们希望团队的努力能够让大家更好地使用VM并引入新的feature，比如新的模型、优化或者硬件。我们致力于让项目结构上能够方便地增加这些新优化。",
        "time": "02:44:47"
      },
      {
        "question": "在开源项目中，如何实现社区的广泛参与和包容性？",
        "answer": "我们欢迎并积极审查贡献者的代码，并鼓励有经常贡献的用户成为项目中的reviewer和merge代码的贡献者，以增强他们的参与感和对项目的拥有权。",
        "time": "02:45:19"
      },
      {
        "question": "travel arena项目是什么，它的目的是什么？Chat table arena与open LM leader board等其他榜单相比有何不同？",
        "answer": "travel arena是我们实验室同学发起的一个项目，旨在解决模型评估问题。通过让随机用户向两个模型提问并投票决定哪个模型表现更好，从而得到一个基于用户投票结果的模型排名。Chat table arena反映了用户集体的偏好，允许自由提问并能区分模型差异，使用类似网球比赛积分计算方式的yellow score对模型进行排名，相比固定数据集的评测更能体现模型的真实表现。",
        "time": "02:47:05"
      },
      {
        "question": "为什么模型评测很重要，以及目前存在的挑战是什么？",
        "answer": "模型评测对于理解模型能力至关重要。目前存在数据污染、公平性及评测与实际场景效果之间的gap等问题。理想的解决方案是构建个人定制的评测题目，并结合多种榜单和自定义测试来全面评估模型性能。",
        "time": "02:52:35"
      },
      {
        "question": "如何解决开源模型评测中的数据污染问题？",
        "answer": "开源模型评测容易受到数据污染影响，需要有主动删除爬虫爬取数据等措施确保数据集的纯净性。同时，建立一套完善的内部评测体系对于选出好的模型也非常重要。",
        "time": "02:52:12"
      },
      {
        "question": "对于闭源模型的评测如何处理公平性和数据安全问题？",
        "answer": "闭源模型评测面临公平性和数据安全问题，理想的办法是构建自己的评测体系并在特定场景下使用，同时参考已有的公开评测结果，综合判断模型优劣。",
        "time": "02:52:35"
      },
      {
        "question": "大模型评估中，如何平衡自动评测和人工评测的优缺点？",
        "answer": "基础模型的质量可以通过自动评测数据集来初步评估，但更复杂的检查模型则难以自动化评价，需要人工评测。人工评测虽然昂贵耗时，但能提供更准确公正的评价结果。在实际应用中，可以结合多种评测方法，在模型迭代的不同阶段进行适应性的评测。",
        "time": "02:55:33"
      },
      {
        "question": "在多模态评估中，有哪些数据集是代表性较强的，并且能够反映模型对图像信息的理解能力？",
        "answer": "比较有代表性的数据集是针对MMLU对标的一个MMMU数据集，它在一定程度上能体现模型在自然语言理解和生成能力之外，还包含了对图像信息的理解能力。",
        "time": "02:58:33"
      },
      {
        "question": "多模态模型是否也需要人工评测，以及目前是否存在类似travel arena的方式用于衡量模型表现？目前多模态模型的发展趋势是什么？未来可能面临哪些挑战？",
        "answer": "是的，多模态模型确实需要人工评审，类似于travel arena的方式。据我所知，check lino正在做这方面的评审版本，可以通过上传图片来评估fusion model的表现。当前多模态模型的发展趋势是训练统一模型，将多种模态融合在一起。未来可能面临的挑战包括如何兼顾评测的公平性和效率，以及开源社区中出现更多类似模型后，如何进行公平评测。",
        "time": "02:59:33"
      },
      {
        "question": "中央政府实验室（卓翰lab）在开源项目商业化方面做了哪些工作？",
        "answer": "卓翰lab诞生了如da bricks、ray等开源商业化项目，并讨论了开源项目的商业化机会和挑战。此外，实验室内的VM项目也考虑将研究成果商业化，但目前更关注open source的成功。",
        "time": "03:00:50"
      },
      {
        "question": "对于VM项目的商业化路径，实验室有何打算？",
        "answer": "实验室目前的想法是先专注于open source的成功，暂时优先考虑开源项目的保持续发展，对于商业化路径还在思考中，受data bricks成功经验的影响颇深。",
        "time": "03:02:01"
      },
      {
        "question": "如何看待未来推理成本的下降空间，哪些可能是硬件和模型架构层面的优化贡献？",
        "answer": "推理成本的下降可以从硬件和软件两方面考虑。硬件上，希望支持更多不同类型的硬件以降低成本；软件上，可以通过modelagnostic优化（例如内存管理、持续取样等）以及模型本身的优化（如更高效的注意力机制、新型模型架构等）来提升效率并降低推理成本。",
        "time": "03:04:03"
      },
      {
        "question": "OpenAI和Google IO发布会上，有哪些让说话人印象深刻的内容，以及它们超出或不及预期的地方？",
        "answer": "OpenAI的TDS表现令人震撼，尤其是自然且强大的打断功能；Google虽然未推出GPT-4的预期产品，但其长序列处理能力和product s的展示很出色。",
        "time": "03:08:48"
      },
      {
        "question": "OpenAI演示的桌面应用和实时交互功能，其技术突破点主要是什么？",
        "answer": "OpenAI的桌面应用是一个里程碑式的产品，实现了无缝、实时的自然语言交互，这一技术突破点在于将大模型无缝嵌入到用户的日常工作中，提供即时解决问题的能力。",
        "time": "03:10:43"
      },
      {
        "question": "下一个阶段，各自项目上最值得期待的事情是什么？",
        "answer": "希望尽快看到VRM项目能够实现跨模态统一处理，并帮助解决实际问题；期待open driven能够解决特定代码问题，推动模型部署；希望国内开源领域有更多的优秀成果产出；同时，VM项目期待在硬件支持和新功能添加方面取得突破，努力成为LM服务的标准选择。",
        "time": "03:14:37"
      }
    ],
    "chapters": [
      {
        "time": "00:00:00",
        "title": "探讨大模型的开源生态与生成式AI的未来",
        "summary": "在最近的on board播客中，Monica和高宁讨论了软件如何改变世界，特别是大模型的开源生态和生成式AI的快速发展。他们提到了阿里最新发布的通义千问模型，强调了开源模型在AI发展中不可忽视的角色。讨论还涉及了中国开源模型的国际影响力，以及开源生态中从推理到开发工具的各个方面。节目中，他们邀请了来自不同背景的嘉宾，包括开源AI生态的关键人物、通义千问团队成员和学术界专家，深入探讨了开源大模型的技术、生态以及未来可能的演进方向。此外，他们还讨论了如何创建有国际影响力的开源项目，以及开源模型商业化的可能性，为听众提供了全面而深入的视角。"
      },
      {
        "time": "00:05:22",
        "title": "开源和AI领域的创新项目分享",
        "summary": "王铁震，来自high face的工程师，分享了其在开源和AI领域的经历以及对社区的贡献。特别提到了一个令他印象深刻的项目，该项目致力于使用AI技术进行星际探索，强调了AI在保障极端环境下的稳定运行的重要性。此项目展示了大语言模型从实验室走向实际应用的可能，特别是在生命维持系统、宇航以及无人驾驶车辆等关键领域的潜在价值。"
      },
      {
        "time": "00:09:48",
        "title": "开源项目和生成式AI的发展与影响",
        "summary": "讨论重点在于开源公司哈根脸（Hugging Face）的早期吸引点、其对开发者社区的贡献，以及生成式AI的快速发展。哈根脸作为一个支持多种框架的中立第三方，通过提供基础服务和标准化接口，大大简化了模型使用的流程，从而受到社区的欢迎。此外，讨论也涉及个人在开源项目和大模型研究方面的经验分享，包括对生成式AI领域，如ChatGPT的关注，以及对特定开源项目的兴趣。"
      },
      {
        "time": "00:13:07",
        "title": "开源项目Open Devin的发展及未来展望",
        "summary": "Open Devin项目因Demo发布后迅速获得关注，初衷是希望使用开源的大型语言模型实现更多功能，而非依赖闭源模型。项目吸引了众多开源社区成员参与，目前已具备基本的UI和功能，并在Sweet Dash Like上达到25%的通过率。未来，项目将聚焦于解决实际场景中的问题，而不仅仅是进行基准测试。同时，探讨了千问的英文名字，确认为“困了”。"
      },
      {
        "time": "00:15:38",
        "title": "李卓翰介绍VLM项目及对开源社区的贡献",
        "summary": "李卓翰，加州大学伯克利分校学生，专注于机器学习系统研究。在过去一年半中，他和实验室团队共同开发了VLM，一个针对大语言模型推理和部署的开源引擎。VLM通过一系列优化技术，如配置attention算法和并行处理等，显著提升了大语言模型的推理速度和吞吐量。此外，VLM与Hugging Face社区紧密合作，支持一键部署主流模型。许多公司和云服务提供商，如AWS、Google Cloud和微软Azure，都采用了VLM作为其背后的推理引擎。李卓翰还提到了对Open Devin项目的兴趣，认为特定应用的优化将是未来研究的重点。"
      },
      {
        "time": "00:18:35",
        "title": "大语言模型优化系统的开发历程",
        "summary": "在ChatGPT发布前，一个团队在学校内部启动了一个基于大语言模型（当时使用的是Facebook的OPT175B模型）的demo项目，初衷是为推广他们的开源项目ALPA，该项目专注于模型的自动并行推理和训练。在实际操作中，团队发现模型推理速度慢且GPU利用率低，由此意识到大模型推理优化的重要性。面对市场上缺乏大模型优化的开源系统，团队决定自行开发。开发过程中，团队遇到GPU内存瓶颈问题，并通过迭代提出了新的attention计算方法，利用操作系统中的paging和virtual memory技术来提升内存利用率和吞吐量。经过一系列研究和开发，团队在2023年2月将page attention作为研究点进行深入，并在4月底提交论文，6月底开源了他们的项目，受到了社区的欢迎和广泛使用。"
      },
      {
        "time": "00:20:45",
        "title": "大模型推理优化与成本降低",
        "summary": "训练大模型成本高昂，而部署阶段的成本更大，主要因为大模型需持续处理外部请求。为降低成本，采用推理优化措施，如模型量化、使用低精度计算、为推理优化的内核等。特别地，批量处理（batching）是优化中的重要一环，能够提高GPU利用率。然而，大语言模型的batching更为复杂，因为其输出特性导致必须进行持续批量处理（continue batching）以避免过长等待，并且不同请求的输入输出长度不一导致内存使用效率低下。通过使用类似操作系统的页式管理技术（page attention），可以解决内存浪费问题。VOM整合了这些优化方法，相较于直接使用标准transformer，显著降低了推理成本。"
      },
      {
        "time": "00:23:57",
        "title": "VRM技术及性能提升方法探讨",
        "summary": "从去年初开始，GPT技术的成本大幅下降，引发了对VRM（推断为某项技术或方法）性能提升背后的原动力的探讨。特别地，讨论集中在如何从使用Python推理升级到利用VRM的专用推理引擎上，以及这种转换带来的性能提升。通过连续批处理（continuous batching）技术，可实现约8倍的效率提升；而通过配置attention技术，进一步提高了三倍性能。此外，通过优化推理模型的内核和其他系统层面的优化，最终实现了相较于原始Python推理100倍的性能提升。这种技术的进步使得深度学习模型的应用更加广泛，包括在降低成本的基础上实现各种应用的普及。"
      },
      {
        "time": "00:27:26",
        "title": "开源模型部署中的BLM框架应用与体验",
        "summary": "在开源模型部署领域，BLM框架因其高效性和易用性而被广泛推荐和使用，特别是在处理大语言模型时。用户反馈指出，在采用BLM后，模型推理的效率和便捷性得到了显著提升。无论是对于个体开发者还是大型企业，BLM都成为了推理部署的优选方案。在国内和国际的使用情况比较中，虽然存在其他框架的竞争，如英伟达的TSRTLM和上海浦江的框架，但BLM的用户基数似乎更大，显示出其在全球范围内的受欢迎程度。"
      },
      {
        "time": "00:30:22",
        "title": "探讨OpenDev在提升LM推理性能中的应用",
        "summary": "OpenDev作为一个复杂的应用案例，展示了如何通过与环境的交互反馈以及复杂对话，优化语言模型（LM）的推理效率。通过对特定应用案例的深入分析，可以更好地理解用户需求，进而针对性地优化推理引擎，减少推理成本并提升效率。此外，OpenDev背后的开源社区的积极参与和贡献也是其吸引力的一部分，反映出对构建优秀产品的共同承诺。在学术界，对于如何有效整合现有优化策略并探索新的优化方向，OpenDev提供了一个值得研究的实践案例，特别是在面对general workload时，如何利用特定应用的特性来进行更有效的优化。"
      },
      {
        "time": "00:33:27",
        "title": "Auto GPT与新兴框架的发展与挑战",
        "summary": "Auto GPT作为一种能够响应用户需求并推动项目开发的框架，最初受到了广泛关注和社区活跃度。然而，随着时间的推移，由于A准框架未能达到预期的实用性，其使用率和热度逐渐下降。新兴框架，如David，显示出解决特定问题，如编码任务，的潜力，并通过多轮交互过程提供解决方案，从而引起人们兴趣。特别地，讨论集中在如何通过多轮对话的优化，如结果缓存和可控的对话流程，来降低成本和提高效率。面对昂贵的运行成本，如使用GPT-4模型的高费用，社区期待开源模型能够达到类似GPT-4的水平，以促进框架的快速发展。"
      },
      {
        "time": "00:37:45",
        "title": "探讨语言模型API优化及长序列处理能力",
        "summary": "对话集中在讨论当前语言模型API的限制，特别是每次请求都要重新计算的问题，以及未来技术如context caching的潜在应用。讨论者对GPD4的API机制表达了关切，因为它要求即使在重用token的情况下也必须重新计算，导致效率低下。此外，还提及了对于长序列处理能力的需求，尤其是在多轮对话和医诊应用中，需要模型能够高效地管理和利用历史对话信息。讨论中还提到了一些技术解决方案，包括context caching，以及对如何优化预处理（preface cashing）进行的深入探讨。最后，讨论转向了生态系统的完善，特别是在支持复杂agent项目方面的不足，表达了对未来API更新和生态发展的期待。"
      },
      {
        "time": "00:41:18",
        "title": "Open Dev和Agent开发的未来趋势与挑战",
        "summary": "Open Dev在Agent开发领域展现出巨大的潜力，尤其是在任务自动化、摘要生成、翻译和创意写作等方面。随着GPT-4和开源模型的推进，开发更为复杂和实用的Agent变得可能。然而，要实现这一目标，还需要克服多方面的挑战，如评测工具的不足、稳定的数据环境和成本控制。此外，社区和企业对于开发高效、低成本的Agent有着强烈需求，特别是在代码生成和模型部署等领域。未来的发展方向可能包括更紧密的社区合作、更好的评测服务以及对开源大模型的优化，以推动Agent技术的进一步创新和应用。"
      },
      {
        "time": "00:48:29",
        "title": "多模态Agent的发展趋势与挑战",
        "summary": "多模态Agent作为未来趋势，需要整合视觉、语言等多模态信息以做出更优决策。面临挑战包括模态融合、理解复杂代码和适应界面变化等。通过提升多模态理解能力，可以为视障人士等提供更直观的操作支持，如通过自然语言交互操作手机屏幕。同时，多模态能力在UI设计、代码生成质量评估等方面具有巨大潜力，能够帮助创建更符合用户需求的产品。整体上，多模态Agent的发展趋势是将各种模态和任务统一到同一个模型中，实现更高效的信息处理和决策制定。"
      },
      {
        "time": "00:52:58",
        "title": "多模态模型的挑战与应用前景",
        "summary": "多模态模型在实现具体应用场景中遇到诸多挑战，如模型识别错误可能导致客户损失，对图片细节信息理解不足，以及模型的幻觉问题比大语言模型更为严重。尽管GPT-4B等模型展现出强大的能力，但多模态模型的实际应用仍存在距离，特别是在动态场景下的应用。去年开始，有大量关于agent框架的讨论，探讨了大模型公司与应用开发者之间的角色定位，强调了开源社区和创业公司在推动模型应用创新中的重要作用。同时，指出了将模型能力提升转化为具体应用存在多环节的挑战，包括产品、工程、UI/UX设计等。总体来看，虽然多模态模型能力提升迅速，但其在具体应用落地方面仍有较大发展空间，需要社区和企业共同努力。"
      },
      {
        "time": "00:58:40",
        "title": "大模型与Agent开发的分层合作模式",
        "summary": "对话集中在大语言模型和Agent开发的分层合作模式上，强调了大模型公司与开源社区在提供模型应用环境方面的不同专长。讨论了如何使大模型更好地适应Agent工作负载，以及开源社区在创建安全、开放的环境方面的潜力。提出未来应朝着更开放的生态发展，各企业专注于自身最擅长的领域，通过标准接口实现层次间的紧密合作，以促进创新和满足特定需求。同时指出，即便有些公司可能试图覆盖从大模型到应用的全过程，但分层合作模式可能更为可行和有益。"
      },
      {
        "time": "01:02:58",
        "title": "Agent Frameworks的发展与现状",
        "summary": "讨论集中在为什么需要Agent Frameworks及其多样性原因。指出，尽管当前模型在某些Agent场景下表现可接受，如使用React进行简单搜索，但Agent Frameworks的多样性和发展源于开发者诉求和审美的多样性。讨论强调了创建Agent Frameworks相对于大型工程（如PyTorch）的相对简单性，并指出由于从工程角度出发，很难有一个框架占据主导地位，因为众口难调，不同开发者和企业有不同的需求和偏好。此外，通过实际开发经验分享，强调了即便一些框架可能外表粗糙，但实际效果良好，最终用户更关心的是效果而非框架的设计细节。"
      },
      {
        "time": "01:07:46",
        "title": "探讨Cruel AI项目及其在多智能体架构中的应用",
        "summary": "Cruel AI是一个引人注目的项目，它采用了多智能体框架，旨在解决实际问题。项目通过改善先前存在的问题，提升了实用性，并得到了社区的广泛关注。主讲人指出，Cruel AI本质上是一个智能体的集合，通过分解任务，让不同智能体执行不同功能，类似公司内部分工。这种多智能体架构在处理复杂问题时表现更佳，特别是在当前技术水平下，多智能体架构对于解决真实任务至关重要。此外，讨论还涉及了未来技术发展趋势，包括可能不需要多智能体架构的情况。最后，通过前端框架的类比，强调了从现有框架中学习重要经验的重要性。"
      },
      {
        "time": "01:13:51",
        "title": "开源框架成功的关键因素分析",
        "summary": "讨论强调了开源框架成功背后的几个关键因素。首先，快速让用户通过现成的项目快速上手对于新用户的吸引非常重要，这能够极大地促进技术的采用。其次，框架的可扩展性是满足用户随着时间增加新功能需求的基础，对于框架的长期发展和社区的活跃度至关重要。再者，一个成功的框架需要有丰富的第三方资源和社区讨论，这不仅增加了框架的可信度，也促进了框架的不断改进和优化。最后，设计框架时应将开发者和用户放在首位，确保他们能容易地贡献和使用，这是开源精神的体现，也是框架能否获得广泛接受的关键。"
      },
      {
        "time": "01:17:54",
        "title": "开源大模型社区的快速发展与挑战",
        "summary": "在过去的几年里，开源大模型社区经历了显著的增长和发展，尤其是在中文语言模型领域。社区内的模型数量从15万增长到66万，数据集也实现了大幅度的增加。这种发展得益于微调技术、模型合并、量化技术及不同格式之间的转换等技术的进步，为开源开发者提供了更多机会。特别是在中国，开源中文模型及其对应的数据集得到了快速的发展，质量也得到了显著提高。社区还见证了多个国人驱动的国际合作项目和组织的兴起，展示了中文大模型社区的活跃度和创新力。尽管取得了显著进展，但社区成员意识到，在海外宣传和影响方面，中文模型社区仍有较大的提升空间。"
      },
      {
        "time": "01:23:50",
        "title": "开源模型的层次与社区贡献",
        "summary": "讨论强调了开源模型在数量上的显著增加以及它们在社区中的重要作用。特别指出，开源与闭源模型之间存在一个光谱，从完全闭源到完全开源有不同的层次，每种开源方式都有其独特的价值和挑战。此外，讨论还触及了开源模型对于中国社区的特别贡献，以及这些模型如何促进了技术的透明度和合作。特别提到了Hugging Face平台上的模型数量和开源程度，以及企业根据自身情况选择不同的开源策略。"
      },
      {
        "time": "01:28:41",
        "title": "中文文本生成模型的数量和统计挑战",
        "summary": "在讨论中提到，对于中文支持的文本生成类模型数量没有精确的统计，因为许多模型没有加上相应的标签。尽管有报告指出中国发布了数量众多的模型，实际统计面临困难，因为模型可能包括系列和具体版本，如千问及其衍生品可能涵盖上百个模型。此外，社区的贡献使得模型数量大幅增加，开源精神促进了模型的多样化发展。在讨论中也解释了开源模型数量庞大的原因，提及了不同大小、量化精度的模型，以及社区对模型的微调和改进，这些因素共同导致了模型数量的增加。"
      },
      {
        "time": "01:33:24",
        "title": "开源模型的影响力评估及架构选择",
        "summary": "在开源模型领域，模型的受欢迎程度通常通过下载量或访问量来衡量，但这存在易被操纵的缺陷。真正能反映模型社区影响力的应该是其衍生模型的数量及微调应用的广泛度。讨论强调了基于不同架构的模型（如基于现有模型微调、完全重新训练和创新架构）在大语言模型设计中的地位，指出选择最适合自身需求的架构最为关键，同时要注意到使用某些模型可能面临的版权和地域限制。"
      },
      {
        "time": "01:39:05",
        "title": "大模型开发中的技术选择与社区合作经验分享",
        "summary": "对话中讨论了在开发大语言模型时，如何在技术架构和社区支持之间做出选择。开发团队最初在模型架构上与主流做法略有不同，采用了基于GPT的tokenize扩展，但在transformer上加入了一个UKB的bias。通过技术探索后，发现遵循拉马（lama）架构可以更有效地训练模型，并得到了社区的广泛支持。尽管面临额外社区支持的需求，但通过将代码整合进Hugging Face的主库，成功解决了这一问题。此次经验强调了在大模型开发中，模型的质量和实用性是最重要的，同时开源社区的参与和合作也极为关键。"
      },
      {
        "time": "01:44:57",
        "title": "大模型架构创新与社区支持趋势",
        "summary": "讨论集中在大模型，特别是语言模型的架构创新及其对社区的影响。一方面，沿用现有架构（如拉玛架构）在没有特殊需求时是合理的，强调了数据、算力、训练时长等因素的重要性超过模型架构本身。另一方面，针对现有架构的限制（如训练效率和长context处理能力），社区对非拉玛架构模型展现出更大的支持意愿。技术如MOE（mixture of experts）和对attention机制的改良被认为是必要的，能够提高模型的训练效率和性能。讨论也指出，对于研究者而言，如何适应和优化系统以支持这些复杂的架构变动是一项挑战和机遇，强调了对各种不同模型架构通用支持的重要性。"
      },
      {
        "time": "01:50:06",
        "title": "探索新模型架构与商业化策略",
        "summary": "从去年下半年开始，许多新的模型架构开始涌现或变得更受欢迎，如RWKV和复古IN趋势，以及MOE架构的进一步探索。早年对MOE架构的尝试未能明确其最佳使用方式，但目前有更加清晰的方向，如激活14D参数以实现类似28B模型的效果。这些新架构的探索对于技术的发展和开源社区是必要的，可以帮助技术变得更为普及。同时，新架构也为商业化提供了新的思路，通过提供高效、低成本的模型，以及通过开源版本展示模型实力，从而推动API服务的销售。这种技术演进和商业化策略的结合，代表了一个重要的发展方向，不同于以往的开源项目盈利模式。"
      },
      {
        "time": "01:57:04",
        "title": "开源与闭源技术的策略与影响",
        "summary": "讨论集中在开源与闭源技术之间的关系，以及它们如何影响公司的研发策略和市场竞争优势。一方面，开源技术能够促进技术的快速传播和社区的共同进步，但同时也可能让公司在技术上失去一定的控制权和领先优势。另一方面，闭源技术则能够让公司在特定技术上保持独占地位，利用技术方案的优化实现成本降低和竞争优势。同时，探讨了如何在开源社区中推广新技术，以及技术开源后的接受度和实际应用情况。强调了在选择是否将一项技术开源时，需要综合考虑技术的成熟度、市场需求、社区反馈等多个因素。"
      },
      {
        "time": "02:06:23",
        "title": "大模型领域的快速变化及学术界的角色",
        "summary": "讨论集中于大模型结构，特别是Transformer的改良与应用，及其在处理长文本时的效率问题。随着大模型在商业领域的成功，学术界的声音相对减小，但仍在探索特定领域的应用。维库纳项目作为开源LM模型的先锋，通过使用ShareGPT数据集来提升模型的对话流畅性，展示学术和开源社区在模型创新方面的潜力。同时，指出了计算资源的限制是学术界在大模型研究上的主要挑战。"
      },
      {
        "time": "02:12:28",
        "title": "学术界与商业公司在AI研发中的不同角色与合作",
        "summary": "讨论强调了学术界在资源有限的情况下，仍能在AI研究领域作出重要贡献。商业公司因其资源丰富，能够执行大规模的模型训练和数据处理，但其主要目标是商业化和获取利润。相比之下，学术界更注重在特定领域的研究突破和知识积累，追求的是科学发现和知识传播。此外，还提到了开源项目VM，它作为一个成功的社区驱动项目，吸引了众多外部贡献，展示了学术界和产业界合作的潜力。项目持续发展依赖于社区的支持和参与，反映出开放源代码项目在推动技术进步中的重要作用。"
      },
      {
        "time": "02:17:35",
        "title": "学术界与工业界在大模型开发中的合作与挑战",
        "summary": "在大模型开发领域，工业界倾向于采用稳健的技术方案以确保模型效果，对激进的实验持谨慎态度。学术界在技术创新和探索方面发挥着重要作用，如斯坦福的DPU项目展示了学术研究对大模型技术进步的推动作用。开源社区的发展和对高质量中文语料的需求，凸显了开源数据集对于AI领域发展的重要性，同时指出了中文数据资源的缺乏问题。此外，讨论还触及了数据产权问题和国际间的法律挑战，强调了对开源友好方向的期望。"
      },
      {
        "time": "02:26:32",
        "title": "大模型训练数据的现状与挑战",
        "summary": "在大模型的开发过程中，数据的获取和处理是研究的重点。当前，大多数团队倾向于使用全网数据进行预训练，尽管数据量看似庞大，但有效、独特的token数量可能并不如预期。一般认为，6到7T的数据是一个较为合适的训练数据量，但仍有空间探索更大的数据量，如10T以上，来提升模型性能。同时，关于post training的数据构造及其对模型性能的影响，存在较多的研究和探索空间。特别是在标注高质量数据方面，如与编码相关的数据，需要专家级工程师进行审核，这为数据的准备提出了更高的要求。此外，对于如何标记数据以更好地引导模型学习，尤其是在与agent相关的任务中，是未来研究的一个重要方向。OpenAI等公司在数据探索方面也隐藏着不少秘密，为外界留下了广阔的探索空间。"
      },
      {
        "time": "02:29:42",
        "title": "探讨国产模型与国际顶尖模型的差距及学习方向",
        "summary": "在讨论中，铁匠分享了国产模型如千问在社区发展上的成绩，尽管与国际顶尖项目如myo和lama相比存在数量级差距。讨论重点在于探索这些差距的原因以及国产模型可以从顶尖模型中学到的方面。特别提到了通过观察myo的成功，了解到海外项目在用户使用、问题预判、三方生态适配等方面的细致工作。强调了在开源生态中，细节的处理、开发者关系的维护、以及如何有效传达模型优势的重要性。"
      },
      {
        "time": "02:33:57",
        "title": "海外影响力提升策略",
        "summary": "讨论重点在于如何提升项目或模型在海外的影响力。指出成功的案例并强调开源支持的重要性，同时指出实现这一目标需关注细节，如模型的海外宣传、正确的meta data使用、与社区的互动、利用社交媒体和视频平台推广等。此外，还提到与下游组件团队合作、利用热点时机发布模型、以及针对英语圈创作更多内容的重要性。整体上，强调了在海外获得影响力需要全方位的策略和细致的执行。"
      },
      {
        "time": "02:38:12",
        "title": "海外模型微调与社区影响力提升策略",
        "summary": "在海外，利用特定平台如Hang Face进行模型微调变得十分便捷，用户可以轻松尝试不同模型以满足需求，并将结果快速分享到网络上。一位千万110D的用户分享了他微调模型的经历，指出在海外较低的算力成本使得模型微调变得经济实惠，整个过程简单快速。此外，通过积极参与社区，了解社区期待，有助于提高模型发布的影响力。特别强调了模型的质量是最终获得成功的关键，如“E”模型通过优化数据集和延长训练时间，获得了比原模型更好的性能，从而获得了社区的广泛认可。此外，“E”的模型大小设计满足了社区需求，Apache 2许可的灵活性也为其广泛应用和商业用途提供了便利。"
      },
      {
        "time": "02:42:28",
        "title": "开源项目成功的关键因素分享",
        "summary": "讨论了VLN项目成功的原因，强调了提供易上手的开始体验、快速应用的重要性，以及与社区保持紧密交流、及时反馈的重要性。分享了项目在发布前的准备、社区参与度提升以及开源项目应该易于添加新特性或优化的结构设计。通过实验室多个项目的实施，展示了高效模型服务引擎的必要性，以及开源项目应易于启动、适应新硬件或模型的优化，并且需要更加包容，鼓励社区贡献。"
      },
      {
        "time": "02:46:28",
        "title": "探讨模型评估方法及其在ChatBot Arena中的应用",
        "summary": "对话中讨论了在模型评估方面存在的问题，以及ChatBot Arena项目如何通过比较不同模型的回答来解决这些评估难题。ChatBot Arena通过让随机用户对两个模型的回答进行投票，以此来判断模型的优劣，并采用Elo积分系统对模型进行排名。这种方法不仅能有效避免数据污染问题，还能提供更为公正和全面的模型评估结果。同时，讨论还提到了其他评估榜单和模型评测中的挑战，强调了对不同场景下模型能力细致评测的重要性。"
      },
      {
        "time": "02:52:11",
        "title": "大模型评测的挑战与解决方案",
        "summary": "对话中讨论了开源评测中数据污染的问题以及地缘评测的公平性问题，提出理想的解决方案是让每个人有自己的评测题目，并在自己的场景上运行模型来决定使用哪个模型。提及了不同的评测目标和数据集用于评测基础语言模型和聊天语言模型的能力。强调了评测体系建立的重要性，并讨论了自动评测和人工评测的优缺点。最后，指出了企业用户在选择和使用大模型时的困难，强调了对评测工作进一步投入的必要性。"
      },
      {
        "time": "02:58:27",
        "title": "多模态模型的评测与挑战",
        "summary": "多模态模型在当前人工智能领域中展示了巨大的潜力，特别是它们能够综合处理文本和图像信息的能力。随着像MMMU这样的数据集的出现，这些模型不仅展现了对自然语言的理解和生成能力，同时也体现了对图像信息理解的能力。尽管多模态模型展现出巨大的潜力，但其评估方法仍面临诸多挑战。评测数据集的缺乏和评估过程的主观性，如旅行领域中的评审版本，暴露了当前评测体系的局限。随着统一模型的兴起，即一种能处理多种模态信息的模型，对其评估的公平性和效率提出了新的要求。未来，随着开源社区的活跃，预计将有更多的多模态模型出现，因此开发一个全面、公平的评估框架变得尤为重要。"
      },
      {
        "time": "03:00:49",
        "title": "开源项目商业化潜力与挑战",
        "summary": "讨论集中在开源项目商业化的机会与挑战上，特别是基于实验室诞生的成功案例，如Spark和Data Bricks。一方面，开源项目的成功为商业化提供了基础，另一方面，如何在保证开源精神的同时实现商业价值是一个值得探讨的问题。VM项目作为讨论的重点，其商业化路径还在探索中，目前更倾向于专注于开源的成功，同时考虑如何能够通过创新和提供增值服务于不同用户群体来实现商业上的成功。"
      },
      {
        "time": "03:03:39",
        "title": "大模型推理成本下降的多维度探讨",
        "summary": "探讨了大模型推理成本下降的潜力，主要从硬件和软件两方面进行分析。硬件方面，提出支持更多种类的硬件以降低成本，特别是考虑到英伟达GPU的成本和获取难度。软件方面，讨论了与模型无关的优化（如memory管理、continue fetching、specular decoding和Price cache等）和模型本身的优化（如更高效的attention技术、MOE模型优化和非attention base模型如mamba）。这些优化措施有望显著提升大模型推理的效率。"
      },
      {
        "time": "03:07:33",
        "title": "探讨技术升级对成本和定价的影响及发布会印象",
        "summary": "对话中讨论了VIN角度提升对推理成本下降的可能，认为成本和定价的调整不仅与速度相关，还受到供需关系、GPU获取难度和电费等多重因素的影响。此外，还分享了对OpenAI和Google最新发布会的看法，特别提到了对OpenAI的TDS功能印象深刻，以及对Google未能发布期待产品的小失望，但对Google在长序列处理方面的能力表示认可。"
      },
      {
        "time": "03:09:55",
        "title": "多模态大模型的技术领先与未来展望",
        "summary": "讨论集中在多模态大模型的技术优势和未来发展方向上。特别强调了OpenAI在多模态处理方面的领先地位，以及其桌面应用可能带来的变革，预示着AI在实时交互和日常应用中的潜力。此外，对于未来一年内开源语言模型的期待，特别是在地缘领域的应用，以及未来三年内大模型技术在嵌入式系统和日常生活中的广泛应用进行了展望。"
      },
      {
        "time": "03:14:19",
        "title": "探讨VRM、Queen项目及开源社区贡献",
        "summary": "对话集中在几个核心项目上，包括濯涵负责的VRM、俊阳的Queen以及在哈根费的铁证。讨论强调了将语音、图像和文本统一理解的工作进展，并展望了将这些模型推广，解决实际问题的潜力。特别提到了对开源社区的贡献，以及伯克利团队希望实现的两大目标：更多的硬件支持和新功能的易于添加。同时，也呼吁更多人参与到开源社区的建设中，探索AI和软件的未来。"
      }
    ],
    "mindmap": {
      "children": [
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "hugging face上有66万个模型"
                },
                {
                  "children": [],
                  "content": "存在各种语言模型和数据集"
                }
              ],
              "content": "开源模型数量剧增"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "API的开放程度"
                },
                {
                  "children": [],
                  "content": "开源模型的多样性和质量"
                }
              ],
              "content": "开源与闭源的平衡"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "多模态理解能力待提升"
                },
                {
                  "children": [],
                  "content": "多模态模型应用的场景扩展"
                }
              ],
              "content": "多模态模型的挑战"
            }
          ],
          "content": "开源大模型的现状与挑战"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "通过共享模型、数据集和工具加速AI研究"
                }
              ],
              "content": "开源社区对模型发展的贡献"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "开源项目促进技术创新和应用落地"
                }
              ],
              "content": "开源项目对企业与学术界的影响"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "国内开源模型的快速增长"
                },
                {
                  "children": [],
                  "content": "开源模型的质量与国际影响力"
                }
              ],
              "content": "中国开源社区的崛起"
            }
          ],
          "content": "开源社区的贡献与价值"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "从文本生成到代码理解和生成"
                }
              ],
              "content": "语言模型的多样化应用"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "图像理解、视频分析等"
                }
              ],
              "content": "多模态模型的应用场景"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "强化理解和生成能力"
                }
              ],
              "content": "语言模型与多模态模型的结合"
            }
          ],
          "content": "开源模型的应用与实践"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "通过API服务实现模型的商业化"
                },
                {
                  "children": [],
                  "content": "开源模型的性能优化带来商业价值"
                }
              ],
              "content": "开源模型的商业化路径"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "企业如何通过开源模型提升品牌形象和技术影响力"
                }
              ],
              "content": "开源生态与企业策略"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "持续探索新的模型架构和技术优化"
                },
                {
                  "children": [],
                  "content": "开源模型在多模态和应用落地的深化"
                }
              ],
              "content": "开源模型的未来发展方向"
            }
          ],
          "content": "开源与商业化的平衡"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "对现有模型架构的改进和创新"
                }
              ],
              "content": "新的模型架构探索"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "提升模型在多模态理解上的性能"
                }
              ],
              "content": "多模态模型的技术挑战"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "通过技术创新降低推理成本，提升效率"
                }
              ],
              "content": "开源模型的效率与性能优化"
            }
          ],
          "content": "开源模型技术发展与创新"
        }
      ],
      "content": "开源大模型生态讨论脑图摘要"
    }
  }
}