{
  "pid": "61cbaac48bb4cd867fcabe22",
  "eid": "64403383a79cc81470f62a88",
  "title": "EP 30.【生成式AI专题3】深度探讨大语言模型生态链：芯片，基建，工具，开源，应用",
  "task_id": "mloynmwmbv62qagp",
  "transcription": [
    {
      "time": "00:00:06",
      "text": "欢迎来到onboard，真实的一线经验，走心的投资思考。我是Monica.",
      "speaker": "发言人1"
    },
    {
      "time": "00:00:11",
      "text": "我是高宁，我们一起聊聊软件如何改变世界。",
      "speaker": "发言人2"
    },
    {
      "time": "00:00:18",
      "text": "大家好，欢迎来到Amber，我是Monica。好久没有上新了，大家有没有想念我们？最近AI的进展实在是太惊人了，似乎隔几天没有个王炸的产品问世就有点不习惯。新闻看多了，自然就要有更多来自一线经验的深入思考讨论，为迎接未来的AI变化做好准备。这一期也是我们AI系列的第三期，未来还会有很多有意思的AI专题讨论会陆续为大家呈现，请大家关注omber，不要错过这些上新。新闻中大多数的讨论都是针对大语言模型本身的能力。其实这段时间大模型周边的生态系统，包括硬件和软件的工具链，都随着基础模型的发展而迅速迭代，相辅相成。",
      "speaker": "发言人1"
    },
    {
      "time": "00:01:04",
      "text": "这一期我们就非常荣幸的凑齐了一个精彩的组合，邀请到了在核心生态位的嘉宾来。包括来自NVIDIA、google cloud hacking、face info从业者等等。聊一聊他们的视角看到的AI发展的机会、挑战与未来。这一切讨论同样非常硬核，从芯片架构、GPU集群管理到开发工具，甚至还聊到了AI的社会影响。有好几个即兴的精彩话题，术语和英文有些多，还请多包涵。在shown note中我们会尽量为大家做好笔记。话不多说，enjoy.",
      "speaker": "发言人1"
    },
    {
      "time": "00:01:41",
      "text": "之前我们很多深入讨论，也包括之前一期在微博上最受欢迎的一篇一期泼卡。其实讨论的可能都是针对这个large language model的能力本身。但其实我们越来越深入下去就会发现它周边的生态系统，其实对于整个大语言模型的发展，其实都起了至关重要的作用。所以这一期我们非常的荣幸请到了几位在业内各个生态位中的非常重要的嘉宾来给大家好好聊一聊。从他们的视角我们就开始了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:02:13",
      "text": "一开始还是老规矩，请大家做一个简单的自我介绍。你的背景。我们通常会有一个fun fact，这次的fun fact就是你自己personally觉得非常有意思的一个最近看到的跟大语言模型相关的一个应用，为什么你很喜欢？要不我们就从今天的为一位女嘉宾来开始，佳佳可以从你开始介绍。",
      "speaker": "发言人1"
    },
    {
      "time": "00:02:35",
      "text": "好的，谢谢莫妮卡，我是佳佳，我现在是在NVIDIA做这个产品管理。我现在目前主要其实是负责我们的一款产品叫做on mivers。我是负责对外对客户的产品管理。在之前我主要是在AI infrastructure那边，在负责我们整个automated vehicle的所有的data platform的搭建。加入NVIDIA之前，我在cisco retake了不同的职能。",
      "speaker": "发言人3"
    },
    {
      "time": "00:03:07",
      "text": "从这个product marketing organization design，sales Operation, 我最近发现有一个插件，就是这个GPT four的一个插件，是在experience上面的。我觉得这个东西特别的好用。就原先我们比如说要搜一个机票的话，肯定就是跑到这个网页上去，或者你在APP上面。有这个插件之后，我就等于就跟他说话了，我就说这是我的要求，你去帮我把这些东西都找出来，然后来西亚，我还要比如说在那我飞到那之后的第二天，我就要去周边的哪一个城市要去办一个事情。所以你还要帮我考虑一下，周边有些什么酒店什么的。整一个使用下来，这个interface对我来说是一种有种革命的感觉，就像是从原先要用鼠标到后来touch screen。现在是从原先需要去搜索，亲自的要去打一些keyword，现在是这种对话式的，然后就马上就能够得到我们。我想要这个可能要花半个小时，1个小时才能够做做的筛选的一些信息，非常的方便。",
      "speaker": "发言人3"
    },
    {
      "time": "00:04:11",
      "text": "对我觉得尤其是看到了这个ChatGPT plugin之后，我觉得以后我们我们很多这种to c的使用工具的方法都要发生彻底的变化。可能我们再也不需要我们用的是expedia还是booking还是什么，这个我觉得会是很有意思的趋势。",
      "speaker": "发言人1"
    },
    {
      "time": "00:04:27",
      "text": "我其实对这个挺有兴趣，我想了解一下，不知道佳佳有没有跟如果用传统的google测试或者google的其他的。大家好，我叫赵涵，现在在google做google vertex的online serving。我在google大概五年多，然后我的背景在google一直是做产品，做就是在cloud AI这个department做online serving。然后现在我是tech lead，并且也带一个团队在做，主要是online serving的data plan部分，加上我们还有一些产品，有一个产品叫ANN search，就是叫matching engine。这matching engine也挺有意思的，就是去做对，去text，去matching engine区搜索一些做一些产品推荐之类的，一个比较高效或者性能比较好的这么一个manager的service。这是我在google主要做的工作。在加入google之前，我在call com也就是高通做过researcher，做过一段high performance computing。主要是集中在手机上的高性能计算这一块。",
      "speaker": "发言人4"
    },
    {
      "time": "00:05:43",
      "text": "最后一位的铁证。",
      "speaker": "发言人1"
    },
    {
      "time": "00:05:46",
      "text": "好，感谢主持人。就快速介绍一下我自己。其实我跟张涵的经历非常像，但是我的我的这个经历是反过来的对我的研究生是做分布式计算的，然后后来加入谷歌，也是在谷歌云做一些SEN，就是软件先定义网络，然后做谷歌内部的一些高性能的网络，然后做一些分布式的一些处理，然后这个集群的一些管理，就是这方面的工作。后面我加入bring，然后在探测flow team其实做的是深度学习的框架，做这个端上的部署就是time flow light。还有我们当时做的在mcu上面跑的一些框架，就time for light for mac control，就在单片机上我们可以把这个AI抛弃，离开google去年11月加入的hanging things，继续做开源社区。然后目前也是在希望跟国内更多的开源生态的合作伙伴们，大家一起有更多的合作，去推动这个开源领域的发展，一起进一步推动AI的民主化进程，这是我的一个简单介绍。",
      "speaker": "发言人2"
    },
    {
      "time": "00:06:47",
      "text": "然后每天我在high face space上面去看大家做的这些APP，都都非常有创意，都非常棒。我说一个就是这个curse，它实际上是chat g皮套壳，然后去帮你写代码的一个APP。这样那就非常符合我的这个使用哲学，就非常简单。你进去之后什么都没有，就是一个门框，然后你就按一个快捷键，然后跟这个机器人聊天，说我要写一个什么样的程序，然后看哪块不对，你就选中，然后再跟他说你给我怎么怎么改一下。",
      "speaker": "发言人2"
    },
    {
      "time": "00:07:12",
      "text": "但是这个APP让我就是特别excited的点是说，它实际上改变了整个软件的供给的模式。就是以前这个软件，大家都是说我要做一个什么通用性的中间件，然后我要支持各种各样的东西。然后这样大家只要写一套代码，然后我们可以把这个代码复用或者怎么样就可出来之后，或者说在这个体系代码能力出来之后，大家这个观点可能现在还没有完全转过来了。但是我认为未来会有这样一个转变。就是说为什么写代码是要写通用性的代码？我写一次性的代码不行吗？我写一次性的代码，这代码非常简单，非常直观，非常的易懂，是非常容易改。我写这一个一次性代码，根本不会花我任何时间，因为我只要跟chat PT说就行。",
      "speaker": "发言人2"
    },
    {
      "time": "00:07:55",
      "text": "所以未来就是大家写代码的这个模式可能会变化，那就导致一个什么后果？就是说代码生产的效率会极大的提高。因为之前我们在谷歌其实写一个代码非常难的。就是你要做code review，你要写united，你要消耗所有的场景。为什么？因为我们写这一行代码可能就要被几万几十万几百万的机器去跑，然后去去在各种场景下去跑各种各样的input。",
      "speaker": "发言人2"
    },
    {
      "time": "00:08:17",
      "text": "未来可能不是这样，未来我就是要解决我这个领域的一个具体的问题。比如说我要去查询机票的信息，然后去阿玛尼上面去去下单，然后去把我整个的整个出口定下来。我就非常知道我这个具体的需求是什么。我不要在give up上面去找所有中间件怎么样，我就直接让chah d帮我把这个代码生成出来。",
      "speaker": "发言人2"
    },
    {
      "time": "00:08:37",
      "text": "现在它肯定生成的代码还不够完美。但是我相信随着未来，包括像函数的这个知识库，然后还有微调，还有一些新的技术出来。Chah BT能学到更多写代码的知识，能学能知道说这个代码它怎么去接入新的API，然后把这个代码的正确性保证好。那那其实未来写代码的风格就会变成面对需求生成代码。没有什么需求，就他给我生成代码，我也不考虑这个代码复用，就是一次性就有点像说未来就不是说做一个通用性的这个产品或者平台，未来更多的是啊去做一个个的这种项目，然后去去解决具体的问题。我觉得这一点实际上是非常有意思的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:09:16",
      "text": "我好喜欢这个分享，我觉得好有意思。Monica也邀请来一位khost，这位cohoes不仅自己是一位MOOS领域的创业者，也是一位年轻的陌生人的老兵。高策可以跟大家。",
      "speaker": "发言人1"
    },
    {
      "time": "00:09:28",
      "text": "自我介绍一下谢谢莫妮卡的邀请。各位好，我是高策。然后我跟那个铁证的经历又有一点像，我也是在交大读书的时候，一直的方向就是分布式计算。比如说集群调度，然后资源调度，这TensorFlow这样的distributed training的一个分布式训练的这样一个场景里面，直到现在我也是keep flow这样一个开源项目，全球贡献排名前三的贡献者。",
      "speaker": "发言人5"
    },
    {
      "time": "00:10:02",
      "text": "一直到去年的四月份，我们看到其实foundation model慢慢会成为一个新的新的AI的形态。它会让整个AI的user用户基础会变得更大。我们目前在做一个service list的influence的server list的推理的平台。然后希望能够大家能够用我们这样的平台能够去比如说尝试一些最新的开源模型，或者是啊自己翻一些开源的模型，然后再把它很方便的serve起来。对我也觉得在code领域的编程领域，其实large anal model有非常大的潜力。但是我有一个不太相同的观点是我觉得copilot是一个很好的形态，然后copilot会有很大的发展潜力。",
      "speaker": "发言人5"
    },
    {
      "time": "00:11:00",
      "text": "现在其实copilot我们都知道有已经有很多产品了，比如说像get hub的copilot，这个应该是使用用户最多的。然后他的这个付费大概是月费订阅制每月10美刀。但是就算这样，哪怕很多国内的用户都在给他付费。那么就算是现在做的最好的get up的copilot，我们其实我自己当然也是从一开始他的这个preview版本就开始一直在使用，从它的preview版本就一直在使用。不过我们其实应该能如果是在编程的话，其实就能感受到它其实它的补全效果没有那么好。大概也就是能补全10%到20%的代码。但是比如说如果有未来有了GPT什么56，那在那个时候那个copilot能够满足我们，比如说补全50%或者60的代码。",
      "speaker": "发言人5"
    },
    {
      "time": "00:11:55",
      "text": "也就是说当我去写代码的时候，我可能只需要写前面的50%，后面剩下的50%是有可怕的帮我写的。或者甚至就是更夸张的时候，比如说百分之七八十都是他来写，那我觉得这个体验可能就完全不一样。当然可能最最发展到最后，可能他最理想的体验就像铁砧说的，比如说给一个description，然后你就给我生成就完事儿了。",
      "speaker": "发言人5"
    },
    {
      "time": "00:12:21",
      "text": "但从这个技术发展的角度，无论是你的这个代码质量的把控，还是你这个架构的设计，我们觉得copilot会是一个相对比较长期的时间内的一个一个一个比较优比较比较优秀的选择。所以我自己是挺看好copilot这个领域，copilot for code这个领域的所有application的发展的那现在其实他的补全效果还不是特别好，但是我对这个方向我是挺看好，我也觉得挺有信心的。而且这个方向就是不只有github这样的大公司的机会，他还有很多创业公司的新机会。比如说一些数据合规性的要求，会导致你需要self post copilot，或者你需要一个更小的模型。但是它也需要在你的这个公司所在的domain里面，它的补全效果又比较好。其实我觉得这样的像pose的方向，我我我自己思考之后，我也觉得也是一个非常好的，很适合创业者的方向。对，大概是这样。",
      "speaker": "发言人5"
    },
    {
      "time": "00:13:23",
      "text": "虽然这个话题不在我们原本是预设的范围内，但是我又觉得是个挺有意思的topic。对于刚才这个铁证和高策聊的这个copy verses，完全大于national language出动的coding方式。其他几位有什么comments吗？",
      "speaker": "发言人1"
    },
    {
      "time": "00:13:38",
      "text": "我觉得两个不矛盾，两个可以同时走。因为它适用不同的user segment。那么对于一部分他可能他他要求的code重复使用性并不要求很高。就像其实很多这个dataset写的一些code，你说有这个严谨性，肯定是没有办法跟传统的c plus press来写口的。这些NG也写的好，但是他解决了一个问题，他一个project做完就老师的很少要重复的去使用同样的这个口，很很严肃的一些软件。比如说用于航天航空的，像copilot还有这个AWS前天还是昨天刚刚发布的这样子的一些拓来来帮助可能写一些更加高质量的，更加能够经得起这个code review的这样子。我觉得两个是完全不矛盾的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:14:26",
      "text": "我觉得将来这个code的生成速度肯定会大大加快，就是随着这个写code的这个工具的不断的发展，但是这个对code review就提出了更高的要求，你想如果我们产生越来越多的code，那么怎么把这些code，这个code到底能不能应用到production environment里面？那可能很多code也不需要放到production environment里面。如果你就是想要完成一个很具体的一个东西，或者作为一个prototype，那你完全不需要放到production environment里面。但是如果放到production environ environment里面的话，那我觉得可能对于AI assisted的code review可能也是一个很有趣的方向。",
      "speaker": "发言人4"
    },
    {
      "time": "00:15:04",
      "text": "其实我觉得这是两个不一样的领域。像比如google，其实我们每个工程师大家。可能都会考虑scale，都会考虑稳定性，考虑安全性等等。所有这些问题，我觉得这种style在未来肯定也是会坚持的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:15:18",
      "text": "比如说我们去写相对来说什么火箭，或者是一些非常底层像K8S这些代码，我们确实是需要他们有非常高的质量。但是未来随着这个大语言模型的发展，我相信需要写这种相当底层代码的人是越来越有越来越少的。比如说我们去看这个软件的开发历史，大家一开始都是写什么汇编，然后后面写这个CC加加，然后后面写这个python。我们会发现这个两级分化的非常严重。现在还在坚持写汇编的人是非常少的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:15:48",
      "text": "我们未来需要写这种非常严谨的代码的人可能也是比较少的。因为这种严谨的架构一旦做出来，他可能生命周期是几年，甚至十年为单位。而且这种代码其实一旦你写出来就非常难改。比如说现在我们去改这个就是某些大厂的什么的实现什么，就基本上不太可能。因为整个上线的周期，还有这个投入产出比是是不是那么高的。就是我觉得就是写写严谨代码的这个场景肯定是会存在的。但是他可能肺癌症站在整个软件生产的角度来看，它可能只是1%甚至0.1%的量。未来大量的都应该是基于20万轨制model来做一些事情的这种应用性的application这种情况。",
      "speaker": "发言人2"
    },
    {
      "time": "00:16:30",
      "text": "其实如果我们能够直接让20万规则model来生成一个代码，其实这个是我我个人觉得这个可能市场空间会更大一点。然后包括刚才刚才说的这个对于某一个领域去微调，然后去去deploy代码。我其实最近正在看一些这方面的工作，hang face上也有，我们前几天帮这个tab ML也这也是一个谷歌的前同事出来创业，然后做的一个代码生成工具。它只要一个T4就可以跑，然后也可以支持微调。其实我觉得这个方向就是会发展蛮快的。然后这些模型它可以去根据你自己公司的这个代码库，根据你新的这些API，然后去去学习到新的这个能力。",
      "speaker": "发言人2"
    },
    {
      "time": "00:17:12",
      "text": "对，就是我觉得未来一次性代码生成好，然后能满足你的需求。或者说它不是项目级别的生成，还是比如说一个函数级别的生成，或者说是它是一个什么unit test级别的生成。我觉得这种可能一两年就不需要等特别久，就是一两年它就可能会发展到比较成熟的地步。我未来大家写成应用层代码，可能只要花现在10%的精力就可以做出来一个还不错的。如果说比如说我写一份代码，我只要10%的金币就可以做到我拿这个10%的这种生成，就是用生成方式去写代码的经历和我现在我先花四五天时间去网上找一个对应的写的比较严谨的这种开源项目。",
      "speaker": "发言人2"
    },
    {
      "time": "00:17:54",
      "text": "然后再花四五天时间把他的所有API dog都看完，然后把它里面的所有坑坑都看完，了解它怎么去实现的，然后再去很快的去去调用它，解决我的这个需求。这两个比的话，我可能就是我个人更倾向于花10%的时间，我什么都不需要学。然后我就直接让整个震撼鬼畜猫的给我生成一个最简单，然后最切入正题的这么一个代码。对我我觉得可能就是就我个人的观点，可能我更倾向于直接让语言生成这一个项目，或者说我们换一个角度去考去去考虑我们为什么需要代码，是因为我们需要让机器帮我们执行一些事情。",
      "speaker": "发言人2"
    },
    {
      "time": "00:18:29",
      "text": "我们让机器帮我们执行事情的方式一定是写代码吗？其实不一定。我比如说我如果告诉这个机器说我用我的语言描述说你给我做一个什么事情，他只要能做得好。那我不管说他写的是代码，还是说他里面是一堆矩阵相乘什么，其实我是无所谓。那么从这个角度去看，其实我们的需求需从需求的角度去看，我们是实际上是希望能够让机器直接满足我们的这个使用。而不是说去一步步指导他，然后让他去做，这样我们就会显得这机器不是那么的聪明。所以我觉得从这个角度来说，我也更倾向于机器直接针对某一个我的需求，然后直接帮我生成代码。或者说他就跳过生成代码的这个过程，然后直接满足这个诉求。",
      "speaker": "发言人2"
    },
    {
      "time": "00:19:15",
      "text": "关于这点其实我没有那么乐观。因为我觉得我们讲到这个生成代码这一块的话，其实我们可能因为这里大家很多可能是做开开源社区，或者说做类似于从搭建生态，往往都是可能有一定的bias在里面。所谓bias是说我们做很多generic task。比如说像github这样的社区，我们可以在github上找到很多做同样的，比如说我们要完成一个任务的，github上有大量现成的可以找到的examples。然后我们可以利用自动代码生成的这样的工具来帮我们完成很多的功能。",
      "speaker": "发言人4"
    },
    {
      "time": "00:19:56",
      "text": "但是其实大量的软件公司，其实他们一个是有自己的IP一个是property soft ware。然后每个其实很大量的公司他们都是啊比如说有自己的特别非常独特的business logic。这样的话带来的一个很大问题就是说我们的模型往往没有办法有足够的数据去产生足够高质量的推荐或者生成。对于这样的这样的情况来讲，我不知道大家有什么想法。所以我我我想的就是说可能对于比如说像google或者像meta这个规模的公司里面，是一个single repository，里面有大量的代码。有足够的数据去生成高质量的代码生成，或者说像github这样的社区。那么对于大量的有自己独特的商业逻辑的小公司，或者有自己独特IP的要这样的公司来讲的话，我觉得我对代码自动生成没有这么乐观。",
      "speaker": "发言人4"
    },
    {
      "time": "00:21:04",
      "text": "刚刚听到那个韩的观点，我也某种程度上挺赞同的。我觉得在比如说我们把它叫做硬核编程的领域，可能确实或者很多涉及到知识产权的问题，他可能那现在可能现在的法律还没有办法解决。比如说你用我们用AA公司的copilot生成的这个代码，能不能知识产权归属于我们自己的公司B公司。但我觉得这个问题肯定是能解决。",
      "speaker": "发言人5"
    },
    {
      "time": "00:21:35",
      "text": "然后从我觉得从很多绝大多数不是那么硬核的需求来看，我其实是某种程度上赞同铁砧的观点。就是你其实这个完全的代码生成，或者其实那些不是那么硬核的领域，可能未来都不是都不一定是以代码的形态在产生了。比如说最近很火的auto GPT或者是哈根GPT这样项目。那你你给他描述一个，你用自然语言可以给他描述一个任务，那当然这个任务它是其实有不同的不同，它需要执行不同的步骤，然后它有一定的编程属性在里面。其实你就是用自然语言来描述一个编程的任务，完事儿他就可以给你用用多轮对话的方式，用HIGPT帮你去完成这个任务。虽然他现在这些工作都很早期，但是我们能看到这样的形态，包括OpenAI的plugin，就是能看到有可能会实现这样的未来。我们只需要用自然语言描述一些，可能之前用编程才比较好解决的问题。",
      "speaker": "发言人5"
    },
    {
      "time": "00:22:38",
      "text": "这个大语言模型可以帮你一步一步的分解这个问题，帮你去不管它是生成了代码然后再去执行，还是说走不同的open I的plugin，还是走更其他开源的long chain过来给你这样的方式。但它可以帮你去解决这个问题。那其实就像铁砧说的，你只需要解决这个问题就行了，你不一定非要通过编程这样的方式。",
      "speaker": "发言人5"
    },
    {
      "time": "00:22:59",
      "text": "所以我觉得这样的未来也是我比较期待的。当然他可能会确实就是比较远，可能要等比较久的时间。这个是我对于他的时间我也是不太乐观的。因为看现在的比如说发明GPT或者是auto GPT这样工作，其实前段时间昨天还是前天济南AI的那个CEO博韩博士刚发了一篇文章。我看了以后也觉得我的观点跟他也比较类似。就是auto GPT这样的项目其实才刚开始，他有非常多的问题。当当他能够真正production ready或者真正可用的时候，可能已经很是很久之后了。对。",
      "speaker": "发言人5"
    },
    {
      "time": "00:23:36",
      "text": "非常感谢大家在这个自我介绍环节就衍生出了一个讨论，而且我觉得非常棒的一个讨论。好，我们接下来开始我们所谓的正式的讨论OK作为我们一个影子的一个问题，正好应该是上个月NVIDIA GTC2023发布了很多跟生成式AI相关的新技术新产品，可以大家聊一聊在这次的发布上，你们有哪一些你们自己个人觉得非常exciting的一些新的产品。",
      "speaker": "发言人1"
    },
    {
      "time": "00:24:04",
      "text": "我personally我觉得最exciting的应该是库里总，因为这个就是一个典型的卷到天际的例子。为什么说已经卷到天际呢？这个库栗总首先是一个library，它涵盖了一些工具，还有一些算法。他是在帮助优化一个过程，这个过程是在生产芯片当中非常的compute heavy的一个过程。这个是computational computational dog phy。这个东西的话如果没有cool leader这个library，而且也不不配用我们的这个hop GPU的话。",
      "speaker": "发言人3"
    },
    {
      "time": "00:24:43",
      "text": "现在比如说台积电他要干这件事情，他大概需要两个星期，那么这个他需要4万个CPU才能够把这个计算做完。那么这个为啥要这么长时间？因为这是一个非常的复杂的一个competition process。它涉及到很多很多的方面，包括像这个elector elector electro magnetic physics，还有包括这个photo chemistry，computational geometry，肯定有很多很多轮的optimization，还有distributed computing。这么一个复杂的过程。然后对于台积电来说，其实每年他他要经历好多好多次。因为一个waffle上面的话，他要在同一个过程他要走好多遍，有了这个库里走的话，大概他只需要花8个小时，而不是两个星期的时间能够做完。",
      "speaker": "发言人3"
    },
    {
      "time": "00:25:37",
      "text": "作为一个绿党来说，这个也意味着他能够节约很多的比如说电力，对于我来说这个比较环保，节能，而且对于你的这个data center里面，与其放4万个CPU，你可以放只能放500个hop GPU，也节约空间。所以，现在是一个的一个极大的一个提高，对于整一个process来说，而且这也是属于用GPU来优化制造GPU的过程，或者制造其他芯片的过程，所以是卷到天际。但是我觉得比较比较兴奋的这样子的一个产品。",
      "speaker": "发言人3"
    },
    {
      "time": "00:26:12",
      "text": "我也不清楚说这个是不是GDC的一个发布，就我观测到这个H100就是media最新的这个GPU，它带了一个光口，就是我我对这个实际上是非常感兴趣的。因为传统上来讲就是这个电口它其实有非常多的线性，就是一个电口它可能传输距离不是特别远，所以你这个能耗就会，sorry，就是这个好，散热就会比较难做。然后你这个光这个电口，它的这个效速度也做不上去，所以我看到这H100加光口。其实我在就是想，老黄眼中的未来超级计算机的一个模式，会不会就是传说中大家想象的这种就是有很多弱电的芯片，然后我们用这个光纤把它们都连起来，这样我们可以，远远超过目前比如说256个H100的组网上线的，比如说到H200或者是H300。我不知道是为什么这么命名的。就是下一代这个芯片的时候，可以把几万个几十万个芯片全都连在一起。用这个光纤有个什么好处呢？就是芯片和芯片之间的联通的信道，它的带宽非常大，然后它的这个延迟也会相对来说比较低。",
      "speaker": "发言人2"
    },
    {
      "time": "00:27:23",
      "text": "那么可以做到一个什么呢？就是说现在比如说这个A100还是H100，它其实没有办法放下一个大语言模型。大语言模型就是千亿参数，它需要很多的GPU才能放得下，那么他怎么办呢？他是做这个派并行，然后在一个GPU上面放一层，然后在下一个GPU上面放第二层。那就导致说如果你的传输速度就是从上一个GPU把这一层的结果输出输入到下一个GPU的时候，这个传输速度赶不上GPU计算的话，就GPU在算这一层的结果，那么你就会有非常多的算力的浪费。就是你会发现就是说你这个规模大之后，你的这个GPU的使用率可能只有20%到30%。",
      "speaker": "发言人2"
    },
    {
      "time": "00:28:03",
      "text": "其实你就看这个H100和H800的区别，其实就是它卡的也就是这个芯片之间传输的带宽。其实这个带宽相对于算力来讲，某种程度上对于某种模型，尤其是现在大语言模型是非常重要的一个参数。未来我们能够把芯片全都用光口直连的话，这个带宽可能是非常高的那在这么一个非常高的带宽的时候，我们可能对整个网络的设计或者说模型最大的模型能有多大，然后这个模型计算的效率，整个这些都会有非常大的颠覆性的想法。其实这方面可以参考GPU最近发的一些论文。就是他们用这个光模块或者是光交换机，然后做了很多这方面的工作。我我我个人其实也是非常感兴趣这个领域。我认为说H100这次加光口其实是非常好的一个展现了一个非常好的前景。未来我会看到这方面有更多革命性的创新，这我是非常感兴趣。",
      "speaker": "发言人2"
    },
    {
      "time": "00:29:04",
      "text": "对，你讲我刚想说PPUV four就是也刚刚发布的。它其实是用了这个optical circuit switches的，确实能够对这个data transfer上面的速度是可以加快很多，而且也很节能对吧？用光跑比用电跑要节能得多。那在这整个data center作为制冷，还有这个更不要说连接这个几颗几几万颗几千颗的GPU或者GPU。In that case, 这确实是一个一个一个极大的进步。你还挺关注这个事情的，看得出来。",
      "speaker": "发言人3"
    },
    {
      "time": "00:29:38",
      "text": "因为刚才正好聊到这个H100，其实包括最近这个AWS，包括这TPU，然后包括最近AWS发布的新的EC two instance，大家都号称自己的这个芯片，针对这个生成式AI做了一些优化。在家客服给大家做一个简单的一科普，就从芯片角度来说，或者说什么是一个对这个的生成，谁来的优化？",
      "speaker": "发言人1"
    },
    {
      "time": "00:30:03",
      "text": "对，是就我们最新的这个应该是h one hundred。如果是在某一些国家的话，我们有HA hundred。那么这个区别就是刚才铁成指出来的那其实就是这个data transfer速度上面减到原先的，比如说3分之1、3分之2或者一半，这个相比上一代是2020年推出的A汪恒嘴来说，在对针对这个大语言模型，我们就先说针对大语言模型有一些什么样的一些个提高？其实我们这次是加了一个东西，叫做transfer from transformers engine。这个transformer engine并不是一个硬件的模块，它其实是是一还是一层软件。但是加在我们的这个ten pencil core上面，它干的一件事情就是原先就比如说代语言模型的话，其实现在对于这个精度的要求是越来越低了。最开始可能要这个SP thirty two，后来大家发现其实这个精度对于这单元模型来说重要性不是那么大，就降到这个FFP sixteen也挺好，也没什么问题。后来大家就继续再挑战一下极限，降到FP eight也可以。",
      "speaker": "发言人3"
    },
    {
      "time": "00:31:09",
      "text": "所以的话，如果你的你的这个data format是可以这样子，可能有比如说有两种或者三种，那么这个transformer engine就是在cancel call上面这一层的这个软件，他刚才已经这件事情就是他能够很自动的去管理，还有去scaling up scaling down。那么来那那根据你的这个模型的特性，他来选择，而且我是想要用这个FPH还是F还是这个sixteen bit，然后再在每一层他来他他进行这样子的一个优化。就这一点，他其实对于这个速度的提升，就是大概是2倍到3倍的一个速度。这一个优化相对于A光焊嘴来说，就已经到了2倍到3倍的一个速度。",
      "speaker": "发言人3"
    },
    {
      "time": "00:31:53",
      "text": "那么H光汉准其他的一些的优化方面的话，可能更多的还是体现在于硬件本身，就比如说这个A含水上面只有108个screaming，streaming，multiprocessor，我们去叫他SM了。这个在H王恒嘴上面，是提高了22%，就是132个，135 12个。这是硬件上的一个个数增大了，每一个的速度处理的这个数据的速度也高了高了，有大概两倍的样子。因为现在我们用的这个tsl call是第四代的，a one hundred用的是第三代探索孔。那么这个代际的迭代就已经意味着大概是两倍的这样子的一个提速。还有一个大的一个变化，就是这个h one hundred，就是它的这个crock frequency也增高了，增高了大概3分之1的样子。这些的话都非常有助于大语言模型的无论是计算也好，还是influence也好。",
      "speaker": "发言人3"
    },
    {
      "time": "00:32:54",
      "text": "另外还有一个其实比较新的一个变化，在这个h one country里面，其实是有控dental computing的能力。这个能力的话，原先只在一些CPU上面有。这是我们的这个hopper是第一款有这种confidential computing能力的GPU。",
      "speaker": "发言人3"
    },
    {
      "time": "00:33:13",
      "text": "这个其实很重要，因为现在这些大元模型，我们现在看到的，其实没有哪一个是真正开源的。所以说这些个。各各大厂家，他在他的这个模型训练，或者说甚至做influence的时候，他都不希望他的这个数据是能够哪怕放在云端，是能够被其他人，不管是有意还是无意的能够看得到。做这样子的confidential computing的这样一层保护的话，就是他就可以他很放心。他这个东西的话，是只有他自己能够看得到的。而且这也能够帮助到这个h one country能够做到这个mote teNancy。",
      "speaker": "发言人3"
    },
    {
      "time": "00:33:49",
      "text": "原先h one很准，只能够有一个用户。现在的话一个h one很准，可以同时供七个up to seven这样子的一个探测器在上面。所以这些都是相当于是这个h one hundred的一个一个一个一个待机的一个进步。比起a one汉字来说。",
      "speaker": "发言人3"
    },
    {
      "time": "00:34:08",
      "text": "刚才佳佳提到了这几种优化的方式。就你所知比如说业内这个是业内几种比较常用的优化方式吗？还是说你会发现不同的厂商，他们其实优化的路径会表明NVIDIA不一样的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:34:23",
      "text": "我觉得NVIDIA可能整个eco system加全一点。可能比起其他的做这种AI accelerator的这些其他的厂商来说，也就是我们做起优化来，除了这个硬件，还有比如说软件，甚至是一些个networking communication的这样子的一些个辅助的一些设备。比如说刚才提到如果要串联几百个、几千个、几万个GPU，那么NVIDIA是有这个NV link switch，还有这个infected band这样子的technology的。那么这些的话都是可能其他的如果是啊某一个单打独斗的做AI chips的这个start up，他没有办法去照顾得到的。因为整个echo system搭建，他他很花时间很花力气。",
      "speaker": "发言人3"
    },
    {
      "time": "00:35:09",
      "text": "明白高策正好我们回到刚才我们第一个问题，你在GTC上有没看到什么有意思的产品？",
      "speaker": "发言人1"
    },
    {
      "time": "00:35:17",
      "text": "对，其实刚刚已经聊了蛮多了，我额外关注的一个点是关于media，第一块CPU的发布有NVIDIA在应该是在GTC今年的GTC发布了Grace CPU，然后其实越来越感觉也没点有点像这个AI领域的苹果，或者说服务器领域的苹果。它除了有自己的GPU，也有了自己的网卡，通过无论通过收购或者其他的方式，也有了自己的自己的DPU。反正搞了一圈下来，现在又把这个又基于ARM做了自己的CPU。这一整套生态，包括刚刚那个提到的软件的生态，其实结合在一起感觉也没点一个非常有潜力的。这个在服务器领域未来会非常有潜力。虽然我还没有没有办法试试一下这个Grace，当然他现在应该也还没有没有不知道有没有流片，但我是觉得它的CPU非常有潜力，它它里面做了非常多很有意思的功能。比如说它能够去跟GPU做一个很好的instagram，并且它还支持NV link这些feature都蛮有吸引力的。",
      "speaker": "发言人5"
    },
    {
      "time": "00:36:35",
      "text": "刚才其实佳佳也提到了一个问题，就是怎么去管理这个大规模的GPU集群，到底有哪一些看到我们看到一些难度，尤其是在训练大语言模型这种超大参数数的这种环境下。",
      "speaker": "发言人1"
    },
    {
      "time": "00:36:48",
      "text": "我有限的了解是在这些，因为其实他们还真的不太open了。很多的数据的话，大家可能也都很难找得到一些比较官方的，就是OpenAI并没有谈的太多，反倒是microsoft再有几次跟他的这个AI inference infection跟他们聊起来，他们有一些官方的一些数据。",
      "speaker": "发言人3"
    },
    {
      "time": "00:37:08",
      "text": "比如说这个GPT three最开始的时候其实是用的这个v one hundred做的train。微观嘴的话现在是从我们现在看起来是两代以前的这个GPU了。它的这个GPT three point fy的话，是用的是这个five，也是用的v one hundred。然后GPT four是用的这个a one hundred trend。那么a one a这个a one hundred他当时是用了1万个，据这个action自己说的用了1万个。这个action的话他他们当时其实是花了很大的力气去打造这样子的一个。你可以说他是一个他其实是一个一个巨型的一个super computer了。他当时他们其实还是主要依靠的是这个message passing interface来做这些个这几这这这个几千颗一万颗的GPU之间的这样子的一个相当于个还有data transfer。那么MV link当时还在这个研发过程当中，所以当时是没有用上这个MV link。现在在用了，现在做influence的时候是在用MV link。",
      "speaker": "发言人3"
    },
    {
      "time": "00:38:12",
      "text": "那么in feed back就是刚才有有提到就是说是这个NVIDIA收购的一个一个以色列公司，他们做的一个产品。这个的话也是其实是在这个supercomputer领域非常常见的一种连接大型的大型的note的这样子的一个111个1个相当于是一个一个standard。它特别因为整个这半年出来的是根据microsoft透露的信息，是它是一个big fact tree的这样子的一个topology。这种topology的话，现在在这个supercomputer领域也是非常通用的。那么它非常它作为做这种for real transfer computation是很快的。所以的话跟这个GPU会有很多的共通性，这些是我也是其实也是通过一些个官方的一些报道了解到的说针对这个OpenAI是怎么样去连接着上万科的GPU的。但是通过其他的一些了解，我会发现其实里面有很多的这些个。我为什么当年，阿硕专门成立了一个项目组来解决这个问题？",
      "speaker": "发言人3"
    },
    {
      "time": "00:39:24",
      "text": "其实里面很多的一些障碍障碍，包括就是，提到有好几次的这些个GPU他如果他要去做大型的这种冗长的这些个训练的话，其实GPU的所有的资源是要全部要gain schedule的。但是其他的那些辅助资源，比如说CPU的资源，它又是双就包的。所以在这个问题上有时候解决起来是比较麻烦的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:39:49",
      "text": "然后我还了解到有些公司是肯定也是包括在内，他可能他有好几代的这种GPU，他不见得说这一清一色都是a one country，清一色都是one hundred来做一个这种大的模型的训练。所以的话这些个cluster是它是抑制的那怎么样去能够那意思就证明这就说明了这些个GPU它可能它的performance差异是很大的。那么你要怎么样去就能够调节这种有差异的这些个GPU，然后让到你的这个training job能够很很smooth的就勾出所有的这些GPU，这也是他们的一个难点。",
      "speaker": "发言人3"
    },
    {
      "time": "00:40:25",
      "text": "还有一个难点的话，就是electrical了，这是一定的。因为这他他这一点的话其实跟这个big data的有点区别。作为作为要训练一个大的模型，它的这个elasticity的话microsoft其实有一个新的东西，他们把它叫做singularity，这个名字很酷，Michael oft singularity。他们干的一件事情，他去他不去微调那些个hyper paramo，相反，他其实是他他他就是设很多的这种check pointing strategy。他作为这个republica spicing，然后来把这些个把这就是能够固定整一个training job的statistical efficiency。然后根据这样子的一些个做法，它能够保证或者尽可能的保证一万科的这个PU能够他的他的比如说scale up scare down，这也是比较顺畅的。这些都是我的一些粗浅的了解。很很想听一下在座的一些专家级的这种distributed computing的专家聊这个问题。",
      "speaker": "发言人3"
    },
    {
      "time": "00:41:30",
      "text": "赵涵你们的团队正好在serving这一块，能否就分享一下google的一些经验？",
      "speaker": "发言人1"
    },
    {
      "time": "00:41:36",
      "text": "好的，我觉得刚才佳佳说的很多都很好，也很就是说的也很在点，对GPU的确实有很多的挑战。主要是说GPU相对于CPU来讲，还是个比较heavy wait的这么一个resort，对吧？所以对于GPU来讲，主要是我觉得有几个挑战。像刚才说的一些异构的调度，CPU和GPU之间的通信，我觉得都是一些比较常见的问题。",
      "speaker": "发言人4"
    },
    {
      "time": "00:42:07",
      "text": "还有一些就是从分布式计算的角度考虑，那肯定有很多生产上的问题。比如说reliability的问题，比如说怎么有效的利用GPU。如果GPU那在GPU只是硬件，那么在硬上我们一般都是啊由cluster来控制，在container或者在炮的这个级别，那么有对吧？然后GPU在如果把它，比如说重启，还是相对比较expensive的。因为你重启以后，你可能你要装driver，要重新做很多东西。那么你对auto skating的话提出的很多就提出很多挑战。",
      "speaker": "发言人4"
    },
    {
      "time": "00:42:46",
      "text": "另外一点就是怎么样去有效的利用GPU的资源，往往是确实是GPU的资源，没有在大规模的training，或者就是说training influence，训练和推理当中都没有获得特别有效的资源的利用。可能相对对的利用率比较低。这里面就涉及到不光是硬件层面，在软件层面涉及到。",
      "speaker": "发言人4"
    },
    {
      "time": "00:43:11",
      "text": "具体训练和推理中的一些很具体的一些算法，是层层面上的优化。比如说一些embedding layer的一些optimization，一些GPU上。比如说有的时候有有一些训练的步骤会有hot sporting的问题。那么怎么去做load balancing对吧？这都是一些很具体的问题。再有一个就是说CPU和GPU之间包括往往数据的传输是一个很大的瓶颈。就是说怎么样去去让数据传输跟上计算，这也是一个很大的挑战。",
      "speaker": "发言人4"
    },
    {
      "time": "00:43:52",
      "text": "最后一个限制就是就说大规模的GPU的集群。集群管理的话，在控制层面的话，肯定是你的class达到一定规模了。尤其是比如说nester，在控制层来讲就有很多挑战。比如说怎么分配网络资源，然后怎么保证这个控制层能够从监控这些硬件，包括底层的这些pod、container，他们的healthy，然后做这些控制层面的这些判断。这些都是一些限制这个GPU cluster往更大的无限的去give up。这里面肯定有很多挑战，包括像刚才佳佳提到一很好的一点就是说往往这个GPU不是都是同一种类型，那么怎么样去平衡这个不同种类类型的GPU之间怎么做coordination，都是一些相关的挑战。",
      "speaker": "发言人4"
    },
    {
      "time": "00:44:51",
      "text": "我好奇，比如说就你知道比如goole内部他们能够处理的同时manage的这个GPU集群大概在什么样的规模。",
      "speaker": "发言人1"
    },
    {
      "time": "00:45:01",
      "text": "GPU的话，我觉得我们现在的往往面对的问题最大的问题就是没有那么多的GPU。因为确实GPU是个非常珍贵的资源，大家都想买GPU。所以实际上因为我们是做了一个云平台，我们云平台实际上是to b去，相当于最后我们是卖资源，去给我们的客户卖资源。这里面就涉及到，实际上如果内部去训练一些大的模型的话，其实规模可以做到蛮大。但是往往是受限于比如说我们的一些一些limitation。比如说就是到了上万个GPU以后，然后肯定是在软件的层面有一些limitation。但是现在对我们大多数的consumer来讲的话，可能现在大家还用不到那么多的GPU。",
      "speaker": "发言人4"
    },
    {
      "time": "00:45:54",
      "text": "我们面临的其实是一个问题，对于GPU或者TPU来讲store code的问题。我们没有那么多的资源去既保证google自己的业务的增长，又来保证那个云云客户的业务的增长。这个是个比较大的问题。",
      "speaker": "发言人4"
    },
    {
      "time": "00:46:14",
      "text": "现在如果说比如我买不到那么powerful的这个GPU，用一些没有那么powerful的这个GPU，但是我用更大的集群去堆，用这种方式会可能会遇到什么样的这个问题。",
      "speaker": "发言人1"
    },
    {
      "time": "00:46:27",
      "text": "其实是有可能的，我们内部也有办法，对于一些没有那么大，其实这里面很多问题主要在于模型的规模。如果你模型规模很大的话，一个GPU的memory塞不下，那么是实际上就是会有很多的挑战。当然你可以做distributed的training和distributed service，但是这里面主要是如果说你模型规模没有那么大的话，其实很多不那么powerful的GPU确实可以达到相似的效果。可能需要3分之1到4分之1的，那么是个非常好的一个补充。",
      "speaker": "发言人4"
    },
    {
      "time": "00:47:04",
      "text": "我注意到这个media不仅发布了针对training的这种很powerful的硬件，这个硬件其实也它同时还发布了三个不同的GPU是指针对不同task的influences。为什么我们需要不同的GPU来进行不同任务的推理，还是说以后大家觉得会更多的往one size fits all来去来去发展。",
      "speaker": "发言人1"
    },
    {
      "time": "00:47:25",
      "text": "这次这个GTGTGTC是发布了4款针对不同的task的influence的GPU。第一个是LF，LF是专门做这个AI也不能说专门，这是一个错误的用词，就是它比较的就做AI video方面是比较强。这一块其实也是google class atom，实际上是我们的最早的一个CSP客户，google会向cloud用户会提供L4的这一款GPU。",
      "speaker": "发言人3"
    },
    {
      "time": "00:47:57",
      "text": "那么它的这个为什么他是我们说他做这个AIV这个方面比较强的是啊就是因为GPU上面有有tsl库，然后tsl库的个数会不一样。有些GPU的tsl call上面还有这个arc call。那么arc call是专门做retracing。比如说有一款是l forty，那么L4的RT，这个artic call非常的强大。另一款可以拿来做这个image generation，也是我们的这个universe的主打的GPU。Olivers cloud实际上是基于这个omniverous x就是OVX，上面就是这一个node，就是八个l forking。这个区别跟这个l four的区别就是这个artic core有多强大了，这是一个。",
      "speaker": "发言人3"
    },
    {
      "time": "00:48:45",
      "text": "还有第三款的influence CPU，实际上是h one hundred MVL，这个稍微有点点绕口。这个MVL其实是一个dua GPU的这样子的一个111个选则，也就是刚才提到有有几次提到的这个GPU上的memory。那么这款的memory是double了一个普通的h one hundred，因为它是两个串在一起的，现在它的这个memory是比较大。一般的，因为它还有188个get版式的memory，所以一般的这种模型可能所有的这些个param ic估计是塞得进去你HOMV。",
      "speaker": "发言人3"
    },
    {
      "time": "00:49:25",
      "text": "第四款就是刚才有位嘉宾也提到的这个Grace hopper。它是一个CPU和GPU串在一起的一个可以叫它super chip也可以。在这两个trip之间，有一个NV link trip to trip的这样子的一个连接。那么让这个CPU和GPU之间的通信完完全全没有任何障碍。他们就是没把对方当外人，就是这样子的一种通讯的速度，大概是900个give by per second这样子的速度，他们的主要的区别就是归根结底，事实上如果你扒拉了其中任何一款来做influence，不管是什么样的一个task它都能做。并不是说OK这个l forty只能做image，我就不能够做这个language东西，那肯定不是这个意思。只不过他对于image或者对于video，或者说对于这个LLM，它有更加多的一些个其他的一些个一些个优化。导致他在做另一款东西的这个inference的时候会可能performance更好一些，是这么个意思。",
      "speaker": "发言人3"
    },
    {
      "time": "00:50:29",
      "text": "Grace hopper就是这个CPUGPU的这个super chip最主要的其实是拿来做recommendation。他做recommendation是非常的快。因为这个样子的因为它有它这个它它可以支持这个vector database。And by the way, 我记得这个matching engine其实就是一个vector data base，是这样子。那么含我印象。",
      "speaker": "发言人3"
    },
    {
      "time": "00:50:51",
      "text": "中是这样子的，是部分的vector database。我们接下来产品还会有一些做feature engineering方面的feature，更接近于vector database应该有的形态。但是他他确实解决了这个vector database的很大一部分问题。",
      "speaker": "发言人4"
    },
    {
      "time": "00:51:10",
      "text": "了解就Grace hopper会是一个非常好的一个选择来来来做这种vector base，vector database，为基础的这种recommendation，所以就是是是这样一个区别，就是说他是术业有专攻，但是让他做别的他他也能做，只不过performance不如让他做他专门设计的时候干的那件事情。",
      "speaker": "发言人3"
    },
    {
      "time": "00:51:32",
      "text": "上层的workload已经固定下来的话，那未来大家是更拼一个general user这种GPU，还是更多的往这个专业芯片这样走。",
      "speaker": "发言人1"
    },
    {
      "time": "00:51:41",
      "text": "做one size face for all肯定很难。所以我觉得专用芯片可能会是将来的一个趋势。因为实际上AI的应用的话很有很多领域，然后每个领域实际上最后虽然多多少少他们可能有一些foundation model，但是在上层应用上来讲有很有很多的差异。那么专用的芯片可能对于这些不同的差异有更好的optimization.",
      "speaker": "发言人4"
    },
    {
      "time": "00:52:10",
      "text": "这个硬件似乎就要越来越powerful。代理模型的开源社区都会注意到。尤其其实大家都比方说包括safer的这个appa，然后google的欠缺了，还有最近一些什么问问困扰，就各种陀类动物的命名的项目，其实都在把这个模型的训练和部署成本降低。然后几十万美金我就可以可能就可以做到像这个GPT这个three or three point five这种模型层面上的一些优化，能够带来的成本的下降，到底他这个边界在哪里？因为我们看到说ChatGPT这个API？一拿出来发现比之前GPT的这个API下降了90%。那到底这个边界在哪？未来这个成本是否还会成为一个制度呢？",
      "speaker": "发言人1"
    },
    {
      "time": "00:52:53",
      "text": "了解，感谢。对我觉得其实这问题就是分成几个，我尝试分解一下。第一个这就是说未来这个模型它随着这些开源这些东西去发展，那么我们是不是就可以就不用拆GPT，我们可以有这in house的模型。这里就是我所我大胆抛一个我的看法。因为我们发现其实在GPT出现到现在，其实也就半年多一点的时间。然后整个开源界其实就是这个土味动物，还有包括国内ChatGLM，还有RKV这些项目进展都非常快。所以我就是大胆抛一个我的看法。",
      "speaker": "发言人2"
    },
    {
      "time": "00:53:27",
      "text": "我觉得今年开源节应该能搞出来差不多GPT3.5水平的一个产品。其实对于很多应用场景来讲，产GPT3.5的这个中文水平其实已经很可以了。就是我们不一定说所有的场景都需要特别强的总结能力，非常强的这种涌现能力，然后拿到这个GGPT4甚至未来GPT5这个水平才能做产品。",
      "speaker": "发言人2"
    },
    {
      "time": "00:53:48",
      "text": "GPT3.5其实已经可以做很多产品了。所以我在猜如果是这样的话，那么这个技术很快就会扩散。那每一家可能都会针对自己的应用场景去微调一个开源的模型。其实你看昨天AWS发布的这个产品，它已经把这个东西当成一个service，这已经没有什么技术门槛了。只要你这些厂商能够出数据，他就能给你出一个模型。然后你就可以在自己的私有场景去部署，你想部署多少就部署多少，你想要开多少QPS就开多少QPS，你想要怎么办就怎么办。这实际上是比调查GPT API的这个方式，对于大厂来讲其实是更友好的这这是第一个。",
      "speaker": "发言人2"
    },
    {
      "time": "00:54:25",
      "text": "然后第二个是说这个成本会下降到什么样的程度。就是说现在chat BC最早出来的时候，大家都说这个token怎么卖的这么便宜。就是大家去想说你用这么多卡，然后去跑这个推理，然后这个成本应该是很高的。其实后来我们也发现，这个chat BT我们AI他们做了非常多的优化。然后导致说他的这个成本相对于我们想象的其实还是下降了不少，但是其实还是很高。那未来是不是还有进一步下降的这个潜力？我觉得是有的，就是包括让我们GPU就像给大家介绍的这个GPU上面的一些进化GPU上面一些进化。随着芯片越来越适应这个大模型的这个work，随着芯片能够不断的去增加它的算力，然后降低它的省成本，降低它的能耗，未来这种推理的成本还会再下降。",
      "speaker": "发言人2"
    },
    {
      "time": "00:55:15",
      "text": "我这里还有一个非常大胆的V，就是我认为推理这一边，未来很有可能出现一个板卡及交付的这种推理的东西。就是说它可能不是一块芯片上面去跑大模型，它可能是一个板卡，然后在上面有一些SM还有一些东西，放在一起，然后你只要给他发一个信号，这就能把这个结果给你吐出。然后这种板卡级的产品，它可以未来接到我们智能家居，接到我们什么天猫精灵，直接到我们个人电脑上面。然后它可以在离线做一些相对来讲比较小规模的一种推。比如说现在我们看到check GMGM6B还有V这个14B其实这个效果都非常不错。那这种产品如果能够离线的去跑的话，那么跑在我们的端上，其实这种应用场景想象力是非常大的，而且它的成本也会变得非常可负担。因为你不需要网络，然后你不需要去根据这个token长度去去花这个钱，就是相当于是一次性的付费，然后就拥有永久的拥有了这样的能力。对，这是我我的两方面的看法。",
      "speaker": "发言人2"
    },
    {
      "time": "00:56:18",
      "text": "我们现在看到比如说这个总陀类项目的优化，大家现在有哪几种比较主流的这个路径。",
      "speaker": "发言人1"
    },
    {
      "time": "00:56:26",
      "text": "就刚刚佳佳其实提到的就是最常见的一种方法就qualification。然后H100如果能在硬件层面帮大家去自动去做，这个是其实就是对这个使用门槛降低很多。然后呃还有就是比如说算子的优化，然后尽像行业提到尽量减少CPU和GPU之间的通讯，其实这些都是非常常见的这些优化的方式。",
      "speaker": "发言人2"
    },
    {
      "time": "00:56:47",
      "text": "然后我这里就是提一个就是目前来讲可能大家关注稍微少一点，但是其实业内已经产生很大影响力的一个优化方式，就是说绕开python。潘森实际上是一个非常厉害的语言，然后它效率非常低。但是需要大规模算力的这些research或者是公司，其实都离不开python，因为python这东西实在是太方便了。但是一旦这个python这边不需要这么大的灵活性，那我们用CIDI去重写它所有的这些逻辑其实是能达到一个起效的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:57:20",
      "text": "就是大家传统上来讲说，我我我在这个离线跑一个大模型的，我至少要有一张什么4090或者说同样级别的显卡才能做。但是有一些神人，他把整个模型用C加加去重写，然后就只用CPU还有intel或者是ARM芯片上一些斯比特的优化。然后做一些CND的指令。它可以在不用GPU的情况下达到和GPU差不多的一个效果。",
      "speaker": "发言人2"
    },
    {
      "time": "00:57:48",
      "text": "比如说一个token是50毫秒，那么这种场景是完全可以支持离线场景去去用的，而且这个方向还有更多优化的空间，这只是刚刚开始。未来我们其实会有非常多的东西。一旦说这个模型这边我们认为它已经演进到一个不是那么需要可扩展性的阶段。那么大家就尝试用rest，用C加加，然后用各种魔改的方式去去把它重启，然后提高它的速度。最后我们可能可以看到，就是至少在推了一段，还会有一个数量级别的推理成本的下降。",
      "speaker": "发言人2"
    },
    {
      "time": "00:58:22",
      "text": "Y tex的团队在降低整个链路成本上有哪些尝试？可以给大家介绍。",
      "speaker": "发言人1"
    },
    {
      "time": "00:58:28",
      "text": "一下现在业界的一个比较普遍的做法。往往不是说一个模型主打全部，对吧？这往往是一个hybrid的模型。这个就是说有一个不那么expensive的模型去处理大部分的大部分的请求，然后有那么一个贵的模型去处理一些很难处理的请求。然后实际上最后的形成的这么一个架构是啊可能就是平摊下来，最后的cos会降低。至于说将来这个成本下降的边界在哪里，我觉得现在谁也不知道，这个确实是一个很难的问题。但是短期之内确实在软件层面上可以做很多的优化。",
      "speaker": "发言人4"
    },
    {
      "time": "00:59:08",
      "text": "我想提一下从business model的角度的话，其实很多的企业，我们说你像这些中小企业，其实他们根本就没有必要去自己建个大语言模型，完全都不需要去尝试。他完全可以去就购买这种LLM as a service的这样子的一些服务包。AWS其实昨天也推出出来了，这个NVIDIA在三月份的时候也提出来，就是我们的那个nimo。Nimo的话就像这样子的一种framework。",
      "speaker": "发言人3"
    },
    {
      "time": "00:59:40",
      "text": "假设一个企业客户也不大，可能几百号人，那么他有一些preparation data，他肯定是不放心把这些data交给一个比如像OpenAI这样子的公司去处理。那么他可以把这些个data交给提供。比如说像NVIDIA的公司，你们的这个framework。作为NVIDIA来说，他有自己的已经训练好的一些11M，有好几个级别的，有非生菌A发生军，很dirty BB脸的，这个是我们最大的。然后那个是try on，那个是叫什么？是叫mecon on。还有现在比较现在做的比较多的是forty three billion的。",
      "speaker": "发言人3"
    },
    {
      "time": "01:00:26",
      "text": "也就是说这些客户的话，其实可以把他自己的数据交给一个这个LLM as a service的这样子的供应商提供商。让这个提供商帮他把他的这个数据加到这个供应商原本就有的这个foundation model上面。再可能花一两个月时间再做一些training。接下来时间的话，这个供应商，这个LOMS service的供应商，还会帮他提供这个influence，相当于是一个one页这样子的一个description。",
      "speaker": "发言人3"
    },
    {
      "time": "01:00:59",
      "text": "其实对于一个中小企业来说，这种服务的形式是非常合适的。它也可以这个东西可能就是可能它只是对自己内部使用。有很多的公司现在可能需要在内部建一个像是一个内部的一个codex，或者说内部的一些个，甚至是内部某一个function。比如说是marketing financial r某一个function的一个一个非常简单的一个这种的一个FAQ的这样子的一个工具。因为他一旦他有很多的policy，很多的细节，或者call center也可以用这样子的一个东西，那么他他去这个比比去搜索它相关的信息要来的快的多，要来的准确的多。其实这是这个business model的话，其实对中小企业来说，我觉得是很值得去考虑的。没有必要自己从零开始去建一个model，也根本不需要花花精力去找一些open source的东西，然后自己拼凑出来一个小型驼类动物。",
      "speaker": "发言人3"
    },
    {
      "time": "01:01:53",
      "text": "刚刚佳佳说的其实是代表了很多厂商的这么一个诉求，然后开源这边也有很多诉求。其实现在我们说这个生成是AI是A2.0两个时代。但实际上我们面临的很多问题，在AI1.0时代已经完全就存在了。比如说当年我是要自己训练一个什么rest net，然后去做一些比如说视觉场景的这个任务。我是自己训练用开源去堆，还是我去买人家一service？",
      "speaker": "发言人2"
    },
    {
      "time": "01:02:14",
      "text": "每个厂商根据他自己的这个背景，他都有不同的诉求。比如说一些厂商，他比如说他是什么制药、银行，他对这个privacy什么的非常有非常高的要求。它的数据不仅不能跨境，还不能出机房。这种场景它一定是要有一些自研的能力，或者说一些完全隔离外网的方式，能够在他们机房里完成训练和部署的这种东西的那还有一这些场景，他可能这些公司他没有那么在意，他不介意说他把这个request发到别人的server里面去做一些东西。尤其是现在新的这个stup，他这个叫AI这个领域非常卷，然后流量非常大。但是时间也非常的重要，产品非常的重要，所以他们要赶紧把这个产品做出来。这种他其实是直接就是调用人家现成的API，他在上面加一点点能力，先把这个客流就是这个知名度打起来，提供一些价值。我觉得这个对他们来讲可能是更有吸引力的。",
      "speaker": "发言人2"
    },
    {
      "time": "01:03:08",
      "text": "然后还有一些公司，他可能是说比如说大家去玩一玩，或者说他们公司也要培养一些自己研发的这个能力，不然后面就逐渐掉队了。那他们可能会在开源上去发力，然后去不停的去做一些开源的贡献。我觉得这些就是都是有的，就是完全是根据客户，他们针对自己的情况去判断，去去去选择的那像比如说在刚才提到这种比较具体的商业服务，那我觉得很多公司其实是有这样的需求。他没有这种资源的能力，或者说他不愿意在这个上面去投入。那找一个给他能做非常好服务，提升服务的这种to b的企业，帮他把这些事情都搞定。那他来讲对他来讲实际上是非常简单，然后也非常容易选择的一种路径。",
      "speaker": "发言人2"
    },
    {
      "time": "01:03:57",
      "text": "确实是我们就是做这个，帮助客户在google这个云的这个平台上去host自己的模型。这个host的模型，现在确实也有很多的enterprise customer在我们这上，各行各业都有主要是用vertex I这个平台去，不光是host的模型，往往涉及到整个AI的一整个生态系统。包括像训练和推理都只是一些比较核心的功能。但是往往围绕着这个周边有很多很多从一开始的数据的清洗到feature的准备，然后包括你的数据质量的这种监测，包括模型的质量的解释，种种那都是啊我们是希望提供一整套生态系统，去帮助客户去能够满足各种各样的deploy模型的需求。",
      "speaker": "发言人4"
    },
    {
      "time": "01:05:00",
      "text": "包括企业级用户，他们也有很多自己的需求。包括比如说像这个compliance方面的需求。比如说数据只能存在于某一个region，包括有一些security方面的需求。往往这些都是这个云厂商希望能够帮客户去解决，然后去，这样的话就是相当于一个turn key的solution，一个for fully manage的solution，包括很多reliability方面的，一些来自，客户的一些concern。我们就是说不光是google cloud，其他包括AWS，还是都会帮客户去做这些，Carry on page，去做这些reliability方面的这些management。这一块是主要我觉得是任何提供AI perform这个平台的云厂商都会提供的一个附加价值。",
      "speaker": "发言人4"
    },
    {
      "time": "01:05:59",
      "text": "Vera tex AI整个产品架构里面，针对这个大语言模型这种deployment做了哪一些相应的一些优化，可以再给大家介绍一下。",
      "speaker": "发言人1"
    },
    {
      "time": "01:06:08",
      "text": "OK, 好的，那就是说首先从我的team的scope来讲的话，可能主要还是相当于serving相关的一些一些东西。包括像现在我们有这个包括我们提供这个呃推理方面的这个平台，然后包括我们最近有这个关于LLM大语言模型。我们有一个evidence API的这么一个对于某一些客户来讲是啊算是private preview开放了，就是说这个是in bedding generation，然后可以跟比如说我们的matching engine连在一起去做一些做整个的embedding的life cycle的management。这一块可能就相对比较方便，包括一开始怎么去，我们将来还是有有很多计划。",
      "speaker": "发言人4"
    },
    {
      "time": "01:06:59",
      "text": "在我们这个roadmap上有很多的东西可以做。包括怎么去帮助用户manage这个embedding，怎么去利用这个。其实google在这方面内部的很多团队，往往是在这方面有很多的经验，包括google内部的很多这些B零级用户的这些team，像youtube，search这些大家耳熟能详的这些service。那么内部关于怎么去有效的去去做ebel ding相关的这些machine learning的这些task。在做到很主要是在性能方面的一些优化。我们作为这个cloud，就是想把google级别的这个optimization相当于帮帮助我们的用户能够更好的host他们的workload，在在这个google cloud上能能利用到google在这方面已有的一些先发的一些经验。",
      "speaker": "发言人4"
    },
    {
      "time": "01:07:57",
      "text": "另外关于LM相关的话，包括像一个是提供给用户一个像tax generation，像chat这种multiple image generation，包括将来的这个code tuning，code generation，这个都是在计划之内。然后另外一点，在这个基础上就是说我们可能会提供一个平台去让用户去从相当于一个marked place，或者一个叫model garden去去拿到一些build on existing的一些foundation model。然后可以去customize自己的这个model，包括自己的这个model，deployment training，deployment，包括还提供一些比如说像prompt tuning的平台，这都是一些，相关的一些，计划。",
      "speaker": "发言人4"
    },
    {
      "time": "01:08:50",
      "text": "如果说以后AI应用更多是基于这个大模型来去做，那对于我们传统的这个MO ops工具链有哪一些影响？",
      "speaker": "发言人1"
    },
    {
      "time": "01:08:59",
      "text": "首先从整个LMML ops的上一代产品来看，基本上无论从哪个维度，无论是从这个公司的数量，还是从整个公司的估值来看，其实最最有前景。或者是use case最多的领域主要是两个，一个是workflow，另外一个是money monitoring。Workflow就是比如说在之前在防对称报道之前，大家如果用机器学习会是一个非常复杂的过程。首先你需要准备数据，然后你需要准备原始数据。完事儿你需要你对数据做清洗，再到后面的训练evaluation，再到模型的上线，这整个过程非常复杂。这也是为什么在17年开源领域里面有了q flow ML flow这样的开源项目。这都是想再帮大家去解决这个workflow的复杂性的问题。这个呃就是在上一代或者是在condition model之前的m office一个非常非常有需求的方向。",
      "speaker": "发言人5"
    },
    {
      "time": "01:10:07",
      "text": "还有一个方向就是monitor对吧？就是因为线上web的模型其实都是自己训练出来的那它的指标都是需要自己去做monitoring，然后自己去做监控的那在监控领域也有非常多非常多无论是商业化还是产品都做的非常好的公司。产品这两个方向目前看起来在在之前的MOS里面是非常不错的。",
      "speaker": "发言人5"
    },
    {
      "time": "01:10:35",
      "text": "然后法律是毛囊出现之后，我认为我个人觉得对于workflow的影响其实是非常大的。那那对于monitoring其实影响会有，它会带来一些变数，但是monitoring的需求还是存在的那之所以说对workflows的影响非常大，主要是因为我们为什么会觉得foundation model或者是large language model，它它是一个非常好的东西。从企业的角度来讲，它其实是降你的开发和使用AI的投入产出比。不对，应该是提高了投入产出比就是提高了它的收益，同时降低了它的成本。",
      "speaker": "发言人5"
    },
    {
      "time": "01:11:11",
      "text": "为什么这么说呢？其实就是你以后可能不再需要去做从零开始去做模型的训练，去做准备海量的相对比较多的数据，你可能只需要翻去就可以了，对吧？你只需要去对模型准备一些你在公司里面准备一些相对质量比较高，但是可能不是那么多的，数量上不是那么多的数据集。完事儿你对它去进行翻译。这个过程其实比你之前的过程是要是要省钱的多，不仅省钱而且省人，对吧？",
      "speaker": "发言人5"
    },
    {
      "time": "01:11:40",
      "text": "也是为什么我可能在之前说过觉得自己要失业了的一个原因。就是你如果用了反对是model，你的无论是你的data science st team，还是你的这个infor team，可能你都不再需要那么多人了。你可能就需要一些自动化的info，或者是像刚刚佳佳还有还提到的这个API as service这种服务其实就已经可能可以解决你的问题了。你的IOI是很顺的过来的那在这个前提下，你的workflow变简单了，或者说你的workflow发生了很大的变化，那workflow的之前的workflows的工具可能就会受到一些挑战。这个是我觉得影响相对比较大的一个领域。",
      "speaker": "发言人5"
    },
    {
      "time": "01:12:19",
      "text": "再看其他的，比如说像v BIOS关注的explained taking这样的领域，包括monitoring这样领域，我认为需求还是存在，并且还有一个相当大的增量的需求会出现，因为foundation model让这个AI的受众变得更多了，所以他们的潜在客户可能更多了。同时其实也提出了一些挑战，是说怎么样在condition model这样一个场景下怎么样去做。比如说训练指标，或者是指标的跟踪，或者是怎么样定义指标，怎么样去评估一个比如说翻圈的大模型。这个其实是带来了新的问题，也有一些新的公司，比如说像human loop这样的公司，他们其实在关注这样的领域。所以我觉得机会是在更多的需求和机会会出现在比如说monitoring，serving，functioning，包括experience checking这样的方向。那对传统的workflow这样的方向，我认为影响还是蛮大的那。",
      "speaker": "发言人5"
    },
    {
      "time": "01:13:18",
      "text": "我觉得那个高速刚才说的非常赞，基本上我想说的都有。然后就是稍微补充一点，这块就是我同意work flow这块可能会比较严重的受影响。但是我个人感觉monitor这块也是会受到比较大的影响。为什么什么呢？而是因为这个workflow和monitor其实是一个闭环的结构。你work flow就在训练出一个模型上，monitor发现有问题之后，你肯定是要去改，然后把这个问题去解决。包括我们最近这征求意见稿其实也说，如果你这模型有问题，三个月之内部改正的话是有很大的问题的，所以monitor要尽早发现问题，然后要去改正。",
      "speaker": "发言人2"
    },
    {
      "time": "01:13:51",
      "text": "但是对于大模型来讲，其实一个非常大的点就是说大模型它太大了，它数据量太大了，它造成他是或者说它在说一些奇奇怪怪的东西，它的原因可能太多了，这些东西也不是说完全我们就能控制。比如说百度你是这个文心一言，你去搜丹顶鹤，他给你搜，他给你触发起重机。百度工程师要把这个问题解决了，他要把他的所有数据集全都弄一遍，这实际上是不是特别现实的一个功。",
      "speaker": "发言人2"
    },
    {
      "time": "01:14:18",
      "text": "我就我想说的点就是说，其实对于用language model，在生产环境，尤其是在我们要让他做一些可控的生成的时候，遇到问题其实是比较难处理的，这方面我们就是有一些办法。比如说我们去可以去去看一下我们这个微调的数据，去仔细的读读一下。但因为这个数据量非常大，所以其实是挺难的。或者说我们用这个prom，然后就先告诉大模型一些东西，然后让他把自己的知识库，长期这个知识储备去在线更新一下。我觉得这也可以，但是可能也不是能解决所有问题，总的来说我觉得这方面挑战蛮多的。",
      "speaker": "发言人2"
    },
    {
      "time": "01:14:57",
      "text": "然后我想聊的另外一个问题就是说我们现在已经有了这个大模型。然后其实m ops只是说我们上线生产环境的一部分，我在想说这个大模型尤其是规则的model service出现之后，其实整个我们编程的范式就是超越我们的ops更大的一个编程的范式。其实会有一个很大的改变。如果说大家都是想要快速上线一个产品，然后调一个比较强大的march language model，让自己的上游去解决所有的这些workload和monitor的问题。然后自己比较专注于这个场景上的业务，其实这种开发模式会比较快。然后我们就会发现这个方向上，整个编程范式会非常不一样。",
      "speaker": "发言人2"
    },
    {
      "time": "01:15:43",
      "text": "就之前就比如说国内大家都比较喜欢搞这个闭环。然后一个APP出来之后，说白了就是这卖汉堡就是这APP卖汉堡他要从养牛开始的，他基本上整个一套都是自己搭的，所以这个APP实际上是非常重的一层。那么一旦用户有一个什么需求，他想要去，比如说他用这个office或者用word，他他有一个地方不爽，然后他想去去改一下，就哪怕是一个非常小的功能，但是这基本上是不可能，因为整个这东西全都是作为一个必然的正确的东西去做的。",
      "speaker": "发言人2"
    },
    {
      "time": "01:16:13",
      "text": "但是新的一波AI创业潮，我们看到这有非常多各种各样的AI然后也有很多是私有部署的AI他们这个APP层其实是非常轻的，因为他不需要管所有的这些后台，他他实际上是把这个产品和计算这两个给分离开了。他产品侧只要专注于满足用户的这个需求就可以，做的非常轻量级，很多都是开源的那这样用户去到这个APP里面，然后他有什么不爽，他甚至是可以直接去给这个产品团队TPR，然后就相当于是可以飞快的帮这个APP去做一些进化。我觉得就是未来这个方向，就是会发生非常有意思的影响。影响不仅仅是and of是比如说整个的产品这一边的形态，然后这个公司团队的组织，未来的融资形式什么，我觉得这个影响会非常深远。但是具体怎么影响，其实现在看的不是特别清楚。我只知道说这可能会和我们过去所经历的这个世界就是不是特别一样。",
      "speaker": "发言人2"
    },
    {
      "time": "01:17:17",
      "text": "比如你们有看到什么，他由此诞生出来的新的一些需求。",
      "speaker": "发言人1"
    },
    {
      "time": "01:17:21",
      "text": "我看到那个database的需求市场非常的大，基本上你要做知识库的话，就是最快的一个方式就是去做in adding simple，然后去去跑这个database，拿到一些新的知识，然后去喂给这个模型来来做对这方面我看到或者是增长都还蛮快的。",
      "speaker": "发言人2"
    },
    {
      "time": "01:17:43",
      "text": "前两天twitter上不是有人报说他一直用pinko，然后发现里面数据丢就是有大量数据丢失。下面很多评论的人我发现大家都是各推荐各的。很明显这个领域真的还非常的就还非常的新，就是还没有一个代表统一的觉得用的比较好的一个一一个工具。",
      "speaker": "发言人1"
    },
    {
      "time": "01:17:59",
      "text": "确实是我觉得刚才说的那些点都非常好。然后我可能提一个包括像product engineer这一块，可能也是新新的一些需求。那可能有一个点可能我们还没怎么聊到，但是我觉得也挺重要的，就是在监管方面的需求。因为这一波实在是太新了，而且就是一下爆点。我觉得各国，包括政府层面，包括像做responsible AI可能好多东西都没有跟上。但是确实我们知道就是客户在自己的领域，不管是大的也好，小的也好，将来多多少少的都会受到一些监管方面的影响。所以这一块儿可能是要催生出一些新的。",
      "speaker": "发言人4"
    },
    {
      "time": "01:18:40",
      "text": "需求反映的工具层面，比如说是指一些可解释性。",
      "speaker": "发言人1"
    },
    {
      "time": "01:18:44",
      "text": "可能是需要有自己的customize的一些safety check。然后比如说你你如果是你你你要开发一些自己的APP，然后你要做一些你要做包括刚才刚才说到的轻量级的APP，可能很多东西都交给后端的话，但是或者交给这种manage的service。但是你可能有自己的unique的一些关于像监管方面的需求，你可能需要满足这方面。可能manage的service会帮你提供这样的平台去满足，或者你需要自己找人来做。",
      "speaker": "发言人4"
    },
    {
      "time": "01:19:23",
      "text": "最近正好跟欧洲的一些客户开会，然后听他们我一直都很好奇。比如说欧洲对这整个generative AI还有ChatGPT的热潮的一些个反应。因为国内我们知道大家的这个反应是非常迅速，甚至是有时候过激的。大家都粉墨登场，老师我当时特别的问了一些base在欧洲的一些一些公司，然后我会发现他们确实相当的保守。",
      "speaker": "发言人3"
    },
    {
      "time": "01:19:58",
      "text": "那么很多的尤其政府我们也看到了，意大利的GV伙伴现在又又又又look back，他们对这个可解释性确实是要求很高。他们又又本来他们动作就稍微慢一点，对于这种新的不管是AI也好了，新的这种的technology train，他们的反应是会慢一些。他们也比较严谨，像以德国为代表的这种这种工业的这种精神。所以的话现在就是整个generative AI在欧洲那边推展起来，其实难度是非常大。因为这个可解释性正好是啊这一波现在大家可能还真没有顾得上的。然后欧洲对数据的管理也是非常严，那么这些个generated AI的数据的来路来历不明，有些还有数据，还有产生出来这些东西的版权。所以所有的这些问题，这可能就不是一个MOOS能够单独去解决的问题。所有的这些，我现在观察到的我感觉这一次的这这一波generative AI的热潮，可能又会是一个新的一个一个milestone。",
      "speaker": "发言人3"
    },
    {
      "time": "01:21:15",
      "text": "把一些个把一些个政府，还有一些个企业，甚至一些区域，能够又再进一步的去分离开来。分离开来的意思就是他们的这个发展的速度，相当于有一些个国家，有些政府，有些个公司可能得到了一个更大的一个加速度。有一些可能就滞后了，现在可能感觉不出来。但是五年、八年、十年以后是可以看得出来，到时候我们到时候再画一个这个画一个图的话，你会发现这样一波的公司，这样一波的政府，在这样子的一个热潮当中，是如何的没有反应过来，是如何的失去了这一波的成长的机会。我我我不知道MRX会不是解决所有的这些个可解释性也好了，还有这个数据的安全性也好了，这些问题的一个唯一的一个办法。可能不是很很明显的这种regulation，还有还有很多的这个其实是OpenAI现在其实正在做的一些事情。",
      "speaker": "发言人3"
    },
    {
      "time": "01:22:16",
      "text": "By the way, open a好像最近link出来了一个video，那自己说了应该不会有这个ChatGPT five，他是这么说的，他说接下来可能整个open a公司会还是致力于在这个check，在这个GPT four的这个基础上做一些个优化和一些个清理。其实的GPT four串了之后，6到8个月的时间，他们也一直在干这件事情，但是很明显还可能还有很多的工作要做。其实我也很好奇，从ML ops的角度能够有一些多大的一些个帮助。",
      "speaker": "发言人3"
    },
    {
      "time": "01:22:53",
      "text": "佳佳那个问题就特别好。我觉得OpenAI其实是一个还是非常有初心，然后非常复杂责任的AI他不是说为了去赚更多的利润，然后就把这个AI能力无限的去提高。他希望这个AI能做一个非常responsible去去line人的行为。其实这块就会涉及到很多ethic什么问题，我觉得后面我们就专门做一些的讨论都都值得。我这里就只是想给一个点，因为我觉得这个其实离我们的生活就非常近，因为我宝宝其实就经常会去问我很多问题，我就是有的时候就回答不出来。",
      "speaker": "发言人2"
    },
    {
      "time": "01:23:27",
      "text": "然后我看到有一个朋友，他是直接把他的宝宝接入到这个ChatGPT上，每天让他宝宝问chat PT3个问题，这个实际上是非常有意思的点，因为在这个题非常有耐心，非常的愿意去解答各种各样奇奇怪怪的问题。但是时间长了，其实这是有一个concern，就OpenAI是想让人去alive chat个，让ChatGPT尽量和人去变得一致。但是毕竟机器肯定不会和人完全玩。但是我担心的是随着未来大家大规模的使用产值BC甚至是很多宝宝，甚至很多没有足够判断能力的人去使用ChatGPT，反而会导致人被ChatGPT去去玩。其实就是这方面整对整个社会的影响，对教育的影响，对整个社会治理方面的影响。我觉得现在大家讨论的还不够多。随着科技越来越往后发展，我觉得这方面的讨论其实是非常值得的。而且这些讨论不仅仅是我们技术圈的人讨论，就是我们在自己圈子里的讨论，其实是需要放到社会上让让更多的人去讨论，去探讨说什么是应该让AI做的，什么是不应该让AI做的，这个边界到底在哪里？",
      "speaker": "发言人2"
    },
    {
      "time": "01:24:30",
      "text": "你这个宝宝介入ChatGPT有点未来范儿了。对。",
      "speaker": "发言人4"
    },
    {
      "time": "01:24:33",
      "text": "因为那天我也跟一个有有宝宝的朋友去继续聊，他也说他说的挺有意思。观察点他说他的宝宝可能也是五六岁，大概五六岁这样。然后他就说他我发现他他说这个小孩其实对这个GPT，他自己也很比我们有耐心多了。他可能他对他可能没有什么exciting，可能他只要他有个回答就好，所以他其实你回答的慢一点或者怎么样他也无所谓。他其实他可以跟他真的可以跟这个距离一直交流下去，宝宝真的会把它当一个人去不停的去去交互，这些东西很难说，我们想让他停就可以，这就可以停下来的。",
      "speaker": "发言人1"
    },
    {
      "time": "01:25:09",
      "text": "是的，我家孩子也是很小的时候就会说可以google，就是他们下一代跟这个呃语音助手交流非常的难受。",
      "speaker": "发言人4"
    },
    {
      "time": "01:25:19",
      "text": "个人是觉得这个事情是有一些controversial的。因为这个产品就包括语音助手和chat PPT，这个是为成人设计的，它是一个工具，他要的是解决我们的问题。但是小孩他其实需要更多的是引导，他需要的是学会整个思维。他不仅仅需要一个答案，他需要有家长去教育他。然后就是说他在问问题的过程中，不仅是告诉他答案，像HP直接告诉他答案，并且要引导说你可以想想这个东西，这个样子他跟他有相似的东西有什么，然后让他慢慢去观察世界，感受世界，而不是直接告诉他一个答案。这种就是小孩可能就会对知识没有兴趣，因为他觉得我想要什么我就有什么。但是如果是小孩问问题的时候，我们去对他有更多的这种耐心的去去引导。然后就用人家长教育的这个方式去教育孩子的话，那他那小孩子可能会萌生出更多的好奇心，去做更多的这个事情。",
      "speaker": "发言人2"
    },
    {
      "time": "01:26:15",
      "text": "所以我是我个人其实是不愿意让我家的孩子直接接到ChatGPT，我宁愿让他PT教我，然后我再用教小孩子的方式去教小孩子。未来会不会有一个专门针对小孩子教育场景训练的，这个机器人除外，我觉得是有可能的。但是作为家长其实我是比较担心，如果我孩子是跟机器人一起长大的那他未来能不能跟我说？",
      "speaker": "发言人2"
    },
    {
      "time": "01:26:35",
      "text": "就好像说我的孩子如果是在孩外长大，那他能不能跟我在本土中国人的这个环境下长大的这个思维一致。我的小孩子如果是机器人养大的，我的小孩子如果是完全是一个工具养大，那他能不能跟我是从从人的沟通上长大的这种人的未来的思维保持一致。我觉得这个领域其实是有非常多可以讨论。",
      "speaker": "发言人2"
    },
    {
      "time": "01:26:56",
      "text": "其实我我最近反而关注到一个有点完全不一样的一个看法。对于ChatGPT as a teacher这样子的一个角色，因为ChatGPT它事实上当了可能太小的小孩子可能还不懂得怎么样去去提问。这个prompting其实是一个一个技巧。所以popping我个人甚至幻想他可能不久的将来会成为一个专为中语言，可能很简单像C口那样子的一种语言。但是可能还会是要比自然语言稍微有一点点新tex什么的。",
      "speaker": "发言人3"
    },
    {
      "time": "01:27:27",
      "text": "Anyway所以我回过来说这个ChatGPT as a teacher这样子的一个feature的话，很多的人我自己包括在内会发现事实上HAPP可以很耐心。因为他现在还没有concious，对不对？所以他没还没有脾气，或者说脾气显然不会像人这么这么这么这么容易的就爆发。所以你可以去问ChatGPT，我看到有一个例子，当时给我印象非常深刻，说的是一个人一哥们儿也一直是用excel，从来没有用过python，但是想学。想学pandas，那么他就去问ChatGPT，这个花了大概半个小时一个钟头，他就chat PT就非常有耐心，循循善诱。当然他这哥们儿的话他是用的这种他他首先他给PPT的一个预测，就是你现在是一个像苏格拉底一样的老师，你会不断的给我提问题，而不是直接给我答案。然后你会去诱导我，或者说去去引导我自己把这个答案想出来。",
      "speaker": "发言人3"
    },
    {
      "time": "01:28:27",
      "text": "这是他的最开始的这个相当于是这样子的一个11个1个condition，给到这个tragedy ity。他就开始问非常简单的问题，关于拍卖的。后来他说他他尝试过去看youtube video也好，或者甚至去做这个document都没有搞懂的一些个东西。一直他对于这个pi都有有这种畏惧情绪的这样子的一个excel用户在跟ChatGPT学习了大概半个钟头，一个钟头之后，他可以做一些简单的表格。他可以用这个python去去，比如说去读一个CSV file，然后去可以去做一些非常简单的一些个甚至charge。然后他当时他他就是这么说，他说没有哪一个老师会对我这么愚笨的一个学生有这样子的耐心，循循善诱的去教我一些最基本的东西。通过不同的角度来教我，而且让我来得到这个答案，而不是说给到我这个答案。我当时非常的震惊。",
      "speaker": "发言人3"
    },
    {
      "time": "01:29:22",
      "text": "说实在话，后来我看到这个com academic实际上有这个plugin了，这个我还没有试过。但是我相信其实在这个教育领域的话，可能一些个简单的一些个调整。就可以把这个ChatGPT作为一个非常好的一个private twitter来offer给到可能不同的年龄段，不同的或者甚至不同的专业方向的这样子的作为一个排位出场。我觉得这个应用的话，实际上是是是非常容易实现的。如果哪个初创公司愿意在这里做这样子的一个应用的话，这个市场也算上是也很巨大。",
      "speaker": "发言人3"
    },
    {
      "time": "01:30:02",
      "text": "对，就是刚才说这个我非常有有感触。就是我回想起来，我觉得我的整个上学生涯中，有几个老师对我的影响其实非常的大。而且大家都知道就是一个好的老师其实非常难得的。所以刚才你说苏格拉底是老师这个例子，你想世界上有多少个老师能够做成是成为苏格拉底这样的老师。我觉得未来可能人去辅助那个大语言模型来去完成一些教育的工作，是还挺还挺有可能的一个事情。",
      "speaker": "发言人1"
    },
    {
      "time": "01:30:31",
      "text": "对。",
      "speaker": "发言人2"
    },
    {
      "time": "01:30:32",
      "text": "这个话题聊完，我都不知道后面这些俗气的话题应该怎么接了。好，我们回到一下这个地面，到底大语言模型这个foundation model的能力能够会cover掉多少上层应用的空间，就是我想从这个角度去聊一聊这个上层应用其实既包括了这些，我们看到的不论是建站做一个chatbot那个应用，像jasper这一些也其实也包括了这一些developed tools，比方说这个LangChain这些这这些工具。怎样看待一个AI应用的价值，就是一个它怎么样能够more than just a GPT rapper。好，现在做一个工具比以前简单太多了。这些工具其实我们现在看到其实也挺也非常的同质化。",
      "speaker": "发言人1"
    },
    {
      "time": "01:31:17",
      "text": "AI应用其实未来会会变得比较轻量级，然后大家去贡献，然后去去不停的去用开源的这种方式去迭代，然后让这个AI应用更适用于某一个场景。其实我觉得这种场景会就是非常多。但是确实遇到你刚才说的问题，就是说它的这个价值壁垒在哪儿？其实这块我我因为我自己就是不是看这个视频也非常多，主要是看这个纯技术项做开源。所以我这块其实思考的不是特别的清晰。但是就是我感觉至少在目前的这个角度，在GPT其实它并没有相当于是吃掉人类整个社会的所有的知识。还是有很多领域，它的有自己领域细分的一些知识，TTPT是不知道的。那么对于这种场景，其实做一些这种AIF，其实我觉得就是非常有价值。",
      "speaker": "发言人2"
    },
    {
      "time": "01:32:08",
      "text": "然后还有就是说tech BT目前它只是一个大脑，它是一个刚中之道，它的生命值就只有他的这context这么长，32K这么长。那么他想要让人类去他想要更多的帮助人类去，当然这是一个非常controversial的话题。就是说他如果想要更多的去帮助人类的话，那我们需要给他配手配脚，让他能够跟外界进行交互。我们看到这个auto GPT其实在这个领域已经做了一些前沿性的探索。那么未来这个东西的边界在哪？我们有什么东西我们是可以放心让跟踪指导帮我们去去完成的。有什么东西是啊一定是要经过人类去做review，然后去去完成的。",
      "speaker": "发言人2"
    },
    {
      "time": "01:32:49",
      "text": "然后这种接了手接了脚的这个APP，它未来发展会是什么样子？我觉得这个领域我觉得会是有非常大的变化。但是我现在看的不是特别信息，我只是觉得这个领域非常值得保持关注。",
      "speaker": "发言人2"
    },
    {
      "time": "01:33:04",
      "text": "Monitor你刚才其实提的很对，就是这些个that tool都有。我的感觉实际上他们其实差异化是越来越小的。而且的话death two的这个确实很不赚钱，为什么呢？因为我我我观察到的就是这个platform providers，他都会有自己的一些个开发工具，现在专门做开发工具的公司其实并不多见，都可能数都数得出来的，就是算得上这个名号的数都数的出来。也就是说这个市场的话基本上很小就是想做开发工具来来盈利，这个市场非常的小。这些开放provider都已经把这一部分就免费的给供了。而且这些发放额外的，哪怕这个发放本身的同质化程度现在也越来越高。所以我觉得做这种工具类的，我觉得可能里面能够去这个可能有空间会比较小一点。这是我的一个看法，可能会有些偏见。我的感觉就是这个市场不是并不是一个能够这并不是一个欣欣向荣的，可以有很多的和很多的道德可以玩的这样的一个市场。",
      "speaker": "发言人3"
    },
    {
      "time": "01:34:30",
      "text": "的确现在我们看到的GPT native的一些APP，都可以很大程度上的去简化你的很多工作。使得原来的很多工具在现在看来可能就是一方面当你的这个interface都变成了以这个语言交互为主的话，那的确就变得趋同更多。我觉得上次好像也是跟高策一起的一个饭局里，我们聊到就是现在很多开发者工具，过去几年这个竞争的点都是说我提高在开发者的experience，这个developer experience is king，right? 你看现在可能当你的统一的UI都越来越趋同成这样了以后，其实大家的其实能够在什么developing experience上的差距，其实也能你自己能够做的空间其实是比较小的。甚至我们也会看到，就以后这种么前端、后端、dev ops这些你们可能本身的定义或者说边界也会慢慢的融合。",
      "speaker": "发言人1"
    },
    {
      "time": "01:35:23",
      "text": "铁证还有佳佳的观点都很有，那我都某种程度是赞同的，不过我有一点不同的意见是在于，比如说在在开发者工具这个方向六是其实还是有最近尤其是最近两个月有非常多新的开发者工具在出现。不管是我不管是为了大语言模型而设计的，还是说用大语言模型在其他的领域做了新的开发者工具。尤其我们看比如说YC的的W23最新的一期batch，然后里面40%的公司是AI公司，而且40%里面有非常多公司其实是在做开发者工具的。",
      "speaker": "发言人5"
    },
    {
      "time": "01:36:09",
      "text": "那我是怎么样看待这个问题呢？我认为是说他人工具确实在在当下这个时间点看起来我我听我很赞同佳佳就是觉得能做的事情其实很少。但是我们从另一个角度想，就是现在有点像这个互联网或者是移动互联网刚出来的时候，其实很多事情都都充满了不确定性。包括比如说GPT3.5、GPT4，我觉得都只是刚刚都只是一个开始了。未来的GPT5，还有大家都在说OpenAI正在做的GPT6，这都有非常多的可能性存在。那么在未来里面，这个developer two一定还会肯定还会有更多的新的新的需求涌现。就包括刚刚我们提到的，像莫妮卡提到的，比如说前后端，那可能都会被自然语言的这种方式取代。那怎么样更好的用自然语言来描述需求，然后来生成。",
      "speaker": "发言人5"
    },
    {
      "time": "01:37:03",
      "text": "比如说可能就不是生成代码了，可能就直接生成前后端的这个application或者之类的那这个也是有非常多的developer tour的需求存在的，它也也有也有非常多的细节。我自己是觉得developed two还是挺有潜力的，尽管我现在看不清楚应该做什么，如果看清楚可能就去做了，对吧？但是我觉得developer tour是一个值得期待的市场，哪怕就是为蓝圈，不是不好意思，哪怕就是为large language model本身去设计的这个developer two，我认为也是蛮有市场的。尽管它可能大家都觉得它是一层rapper对吧？但比如说我们看，我非常喜欢having faith这家公司，我们看hugin face其实最早也只是做了一个transformer的library。但是也不妨碍他现在比如说社区各方面都做得特别棒，然后商业化也有了一些收获。",
      "speaker": "发言人5"
    },
    {
      "time": "01:37:54",
      "text": "我们其实看ML ops或者是large language model Operation这个方向，它其实就很像当初的develops，就像hash cop那个领域一样。Hash code其实在这个公务云刚开始，公有云其实就很像现在的这个large language model的provider。他们是提供商。Hash cop那个那个时候就他他在很早的时候就看到了，AWS不可能一家独大，对吧？我们现在有了GCP，有了ager，他在很早的时候就看到了这个趋势，然后就做了terraform。",
      "speaker": "发言人5"
    },
    {
      "time": "01:38:30",
      "text": "然后在那个时候没有我相信没有多少人认为terra m这个东西他真的能够有很大的价值，并且能够帮助hash corp走向上市。虽然他现在营收不主要靠tea form，但是为他带来了非常多的帮助。比如说为他带来了更多的feed back等等，才让他后来有了vote，有了packer这样的新的development的产品。所以我相信lantry其实包括蓝券类似的工具，当然就是large language model的provider，他们肯定会提供自己的工具，但是他们的工具肯定是winter lock。",
      "speaker": "发言人5"
    },
    {
      "time": "01:39:06",
      "text": "但在这个场景里面，大家也提到一个趋势，就是vendor与vendor之间的差异其实越来越小。既然在这样的趋势下，我们为什么要vender lock呢？为什么我一定要用OPI OpenAI，我也可以用别的。那如果我要用别的，我肯定要选择一个多window而无window无关的一个development。错吧？那我觉得完全就是一个很好的这样的工具。所以我自己是觉得哪怕large language model的developer tour也是有非常强烈的需求的。",
      "speaker": "发言人5"
    },
    {
      "time": "01:39:35",
      "text": "因为一方面的这个open source的社区做的非常好，另外一方面这个CEP就是large language model vender自己本身差异化也在慢慢的越来越小。它有点像公有云，公有云慢慢都在更多的是卖自己的爱。Magic VM就是large language model，也更多是在做卖自己的large language model本身。所以它的developers tour必定不会像没有文必定不会像那个文档无关的这个开源的工具的社区那么活跃，这是我的想法。",
      "speaker": "发言人5"
    },
    {
      "time": "01:40:08",
      "text": "然后再说到application，我觉得application其实壁垒非常小的。一个很重要的原因是大家现在还没有自己的数据。然后如果在TPT这个时代或者在foundation model这个时代，那你其实你唯一的壁垒就是你的数据。你没有数据的话，你如果是做一个rapper你其实很容易就被人取代。因为对，所以从这个角度出发，我觉得application以后的壁垒一定是自己的数据。",
      "speaker": "发言人5"
    },
    {
      "time": "01:40:33",
      "text": "刚才聊了很多很具体落实的东西。在未来你这个AI的发展让你最期待和最兴奋的是什么？",
      "speaker": "发言人1"
    },
    {
      "time": "01:40:41",
      "text": "我期待觉得AI可以多解决一些实际的家务方面的问题。我有两个小孩子，然后这个没有这个support system，就我跟我队友两人。所以的话很多的林林总总的小事儿在家里，现在这些个那些小萝卜、扫地机器人、煮饭机器人都差强人意。所以我真希望我不知道LLM会不会是一个solution之一，我希望在不久的将来，希望五年之内有一些能够基本上能够对付的过去的一些个煮饭机器人、扫地机器人、切菜机器人跟衣服机器人这样子的AI应用。",
      "speaker": "发言人3"
    },
    {
      "time": "01:41:22",
      "text": "好，这个伊拉must人形机器人正在向你走来，不知道这个还要多长时间。对，我们可以搞一个机器人这个机器人专题。",
      "speaker": "发言人1"
    },
    {
      "time": "01:41:30",
      "text": "对我我也有类似的痛点。刚才佳佳说的，所以这个是个很好的方向。另外我觉得这个machine learning就将来就是AI总体将来给我们一直都是这些年有很多惊喜，往往很多是这种爆点式的这种发展，不是这种渐进式的发展。就比如说最早我们看到一些像阿尔法狗这些，然后包括最近的这个LM就是突然之间好像一一下子就有了这个非常革命式的能。所以这一块还是将来蛮多期待的。",
      "speaker": "发言人4"
    },
    {
      "time": "01:42:03",
      "text": "我个人其实非常期待，就是刚才我说的这个端上的art language work，就是有一个足够大的芯片，能够在端上以我可以接受的电力消耗来跑一个还可以的模型。那么它可以部署在我家，放在我电脑，然后随时跟我交流，我也不需要担心这个隐私的问题。然后它可以作为我非常好的一个处理。就比如说他可以通过这个auto GPT或者是RPA的这个方式帮我去驱动现在已经有的这个电脑。",
      "speaker": "发言人2"
    },
    {
      "time": "01:42:33",
      "text": "我不知道大家有没有看过那个流浪地球2，其实里面那个550C就是所谓的貌似的原型，其实他就在做类似的事情。他给人提供的是一个自然语言接口，然后他会把人的这个指令通过不知道什么方式，COT也好或者怎么样，然后一步一步展开。他说我们要第一步做什么，第二步做什么，第三步做什么。然后他自己会去反思说他为了做第一步我们要怎么做。实际上你可以把它看成一个说计算机的编译的过程。",
      "speaker": "发言人2"
    },
    {
      "time": "01:43:02",
      "text": "其实compaction也是一样，就先拿到这个code，然后再做AST，然后再转成AR最后再转成机器码。其实它也是把这个自然语言的方式做一个转换。先是自然语言，然后再拆分布出，然后再转换成我对应需要调用的APP，它的这个接口的一些指令。这个APP如果是图形界面，它可以控制鼠标点击。如果这个APP是一个编程界面，它可以生成代码。如果这个APP是一个，比如说工控的叫什么IOC的接口去去跟它连接，那可以生成的对应的这个通讯的方案。那么我只要用我的自然语言跟这个机器连接，这个机器它可以自编译、自组织、自创造这么一套控制的流程出来。那相当于是我跟机器聊，机器可以帮我把所有的时间，所有的事情都都做。",
      "speaker": "发言人2"
    },
    {
      "time": "01:43:46",
      "text": "这个发展到后面的极致，就是说整个月球基地他们在建建三个发动，就是月逐月推动发动机的时候完全是没有一个人在月球上，完全是机器控制机器机器控这个车来进行交互的。而且最最精彩的地方是说，它的控制的这些东西是生成的，所以它是兼容现在已经有的所有的东西的，就是你不需要专门为AI的纯自动化去生成一些东西。AI可以降维去理解人的这些不好用的是UI界面，然后去去去做这些东西。所以我个人就是对这种AI跟实体的连接，然后AI从这个地方出来，然后变到我们周，变到我们身边，变成一些我们日常能见到的设备，让AI有手有脚去去帮我们一起去去改造世界。",
      "speaker": "发言人2"
    },
    {
      "time": "01:44:37",
      "text": "我觉得这个领域其实是非常exciting的。当然也会有很多监管，立法，然后人类社会的接受程度，然后对未来教育的影响等等这方面的影响。这个我觉得需要社会更大的一个audience去去一起去探讨。",
      "speaker": "发言人2"
    },
    {
      "time": "01:44:52",
      "text": "我觉得郭导他把这个东西放到了一个科幻的电影里面，放出来。然后其实也是去引发整个社会的一个思考。然后看一下最后我们应该有什么让AI做什么，不能让AI做，然后一起去推动这个行业的前进，我觉得这些其实都是非常好的点。",
      "speaker": "发言人2"
    },
    {
      "time": "01:45:10",
      "text": "我觉得Monica你其实有非常好的平台，我们可以去找更多跨行业的这个人去做一些brocas，然后激发一下跨行业的去思考，甚至说说我们去一起去畅想一下说我们不考虑完全不考虑技术，那么未来应该是一个什么样子。然后我们就是定一个我们有一个30年的愿景，然后我们去一步步往那个方向去努力，而不是说我们AI圈的人自嗨，让大家觉得说我这个可以做，那个也可以做。但是对社会是不是能接受我们做这些事情？社会是不是说会因为我们做的这些事情，未来会去恐惧害怕。也觉得说我们比如说降本增效搞得太厉害，导致社会失业，或者是等等这方面。就是我觉得我们千万千万不能圈子里自嗨，我们一定要跟社会上有有更多的沟通交流，大家有一个共识，然后去往这个方向去去推动。",
      "speaker": "发言人2"
    },
    {
      "time": "01:46:04",
      "text": "对我觉得铁战队精分享非常非非常精彩。我也跟一个朋友聊，也是开发者，然后他就说他说他说发现这个越用越发现这个ChatGPT真是什么都能做，经常能够超越他的expected。他发现一旦他什么如果他什么时候发现这个，他就给他这个回答没有那么好的话，他觉得大概率都是因为他自己问的不太好。所以他说他给他们的员工都是说，你如果发现HPT给你的回答不够好的话，你先反思一下是不是你自己的这个prom没有写好。什么时候我们用一个软件，用一个工具，当他perform没有我们我我们期待那么好的时候，我们居然就是blame我们自己，而不是觉得这工具不好。我觉得我印象中应该从来没有发生过。所以我觉得真的我我发现当一个工具它强大到一定程度的时候，我觉得他已经在反向去改变。真的是反向的改变我们自己跟他交互的方式，是我们工作和生活很多方式。",
      "speaker": "发言人1"
    },
    {
      "time": "01:46:56",
      "text": "好，今天非常感谢大家，后续又聊了这么长的时间。进行了很多这些天马行空的一些畅想，我觉得聊得非常的非常的开心。然后也希望以后能够有更多这样的一些讨论，我们一起来关注业界的一些进展。好，今天就到这里了，非常感谢几位时间，感谢大家的收听。如果你喜欢我们pocus内容，欢迎你点赞并分享给可能感兴趣的朋友。有任何建议反馈都可以在评论区留言，我们都会很认真的看的。如果你在用apple podcast收听，也希望你花几秒钟给我们打个五星好评，让更多人了解到我们我们下次再见。",
      "speaker": "发言人1"
    }
  ],
  "lab_info": {
    "summary": "讨论者们集中讨论了大语言模型（LLM）在云服务领域的应用及其对提升用户工作负载管理能力的潜在影响。特别提到，随着Google等领先企业的积累，未来计划将包括自动生成文本、图像多样性和代码优化等功能，以及建立平台供开发者定制和部署模型。此外，讨论扩展至AI在教育领域的应用，如个性化教学，以及如何简化软件开发流程。技术进步的同时，也引发了对数据隐私、工具同质化和教育影响的深切关注。强调了探讨这些技术伦理问题的重要性，以确保AI的发展能惠及社会，避免负面影响。讨论还涵盖了AI生态系统的发展，包括硬件和软件工具链的迭代，以及AI在软件领域的革命性变化，如代码生成工具和AI辅助编程。特别提到了NVIDIA、Google Cloud Hacking等从业者的见解，以及AI模型部署成本降低的路径，包括使用专门芯片和为中小企业提供AI作为服务的商业模式。整体而言，对话展示了对未来AI发展方向的深度讨论和探索。",
    "qa_pairs": [
      {
        "question": "这一期讨论的内容重点是什么？为什么选择讨论AI的生态系统？",
        "answer": "这期讨论主要关注大语言模型周边的生态系统，包括硬件、软件工具链的发展和迭代，以及这些生态系统对大语言模型发展所起的关键作用。因为在深入探讨大语言模型的过程中发现，其生态系统对整个大语言模型的发展具有至关重要的影响，因此值得深入讨论。",
        "time": "00:00:18"
      },
      {
        "question": "这一期我们非常荣幸地请到了哪些在AI核心生态位的嘉宾？",
        "answer": "我们邀请到了来自NVIDIA、Google Cloud和face info等公司的从业者，包括负责产品管理和技术领导的佳佳，以及在Google从事在线服务和产品开发的赵涵。",
        "time": "00:01:04"
      },
      {
        "question": "我们这一期的第一个嘉宾是谁？她的背景是什么？佳佳分享了哪个与大语言模型相关的有趣应用？",
        "answer": "第一个嘉宾是佳佳，她在NVIDIA负责on mivers产品的对外客户产品管理，之前主要负责AI基础设施搭建，并在加入NVIDIA前有在Cisco的工作经历。佳佳提到一个GPT四的插件应用，该插件可以提供对话式的搜索体验，让用户通过自然语言与AI交互完成信息搜索和筛选，大大提高了便捷性。",
        "time": "00:02:35"
      },
      {
        "question": "赵涵在Google的主要工作是什么？",
        "answer": "赵涵在Google主要负责云AI部门的在线服务数据计划部分，以及一个名为ANN search的产品，该产品利用匹配引擎做高效的产品推荐和文本搜索。",
        "time": "00:04:27"
      },
      {
        "question": "对于未来编程模式的变化，几位嘉宾有什么看法？",
        "answer": "嘉宾们认为未来编程模式可能会转变为面向需求生成代码，而非编写通用性代码。随着技术发展，像Copilot这样的工具将能更好地辅助开发者编写代码，提高编程效率和质量，同时也讨论了代码合规性和模型适用性等问题。",
        "time": "00:08:37"
      },
      {
        "question": "随着AI技术的发展，代码生成速度将加快，这是否会对code review提出更高的要求？",
        "answer": "是的，随着代码生成速度的加快，对code review的要求确实会更高。我们需要考虑如何确保生成的大量代码能够应用于production environment，并且能够满足质量要求。",
        "time": "00:14:26"
      },
      {
        "question": "未来在软件开发中，是否会出现更多基于大语言模型的代码生成工具，以及针对特定领域进行微调和部署代码的情况？",
        "answer": "绝对会。大语言模型的发展会让未来越来越多的应用性代码生成变得可能，例如通过微调和部署特定领域的代码，这将会是一个快速发展的方向。",
        "time": "00:16:30"
      },
      {
        "question": "对于未来一次性生成满足需求的代码，或者函数级别、单元测试级别的代码，你认为这个发展趋势如何？",
        "answer": "我认为一两年内，一次性生成满足特定需求的代码将会发展到比较成熟的阶段，开发者可能只需花费当前10%的精力就能完成高质量的代码编写。",
        "time": "00:17:12"
      },
      {
        "question": "是否担心模型在没有足够数据支持的情况下无法生成高质量代码的问题，尤其是对于拥有独特商业逻辑的小公司？",
        "answer": "这是一个值得关注的问题。确实，对于拥有大量独特业务逻辑的小公司，由于模型缺乏足够数据训练，可能无法生成足够高质量的代码推荐或生成。对于这种情况，目前还没有明确的解决方案。",
        "time": "00:19:56"
      },
      {
        "question": "对于完全的代码生成或者非硬核编程领域的未来形态，有何看法？",
        "answer": "我赞同铁砧的观点，即在未来很多非硬核编程领域，代码生成可能不再是以传统编程方式呈现，而是通过自然语言描述任务并利用大语言模型逐步分解问题，实现任务解决。尽管这可能需要较长时间的发展才能真正实现，但我对此持期待态度。",
        "time": "00:21:35"
      },
      {
        "question": "H100和H800的主要区别是什么？",
        "answer": "H100和H800的区别主要在于芯片之间的传输带宽，这个带宽对大语言模型的训练非常重要。当未来能够实现芯片全光口直连时，带宽将会大幅提升，进而影响整个网络设计、模型大小以及计算效率等方面。",
        "time": "00:28:03"
      },
      {
        "question": "PPUV four采用了什么样的技术以提高数据传输速度并节能？",
        "answer": "PPUV four使用了optical circuit switches技术，能够显著加快数据传输速度并具有节能效果，相比传统的电传输方式更为高效。",
        "time": "00:29:04"
      },
      {
        "question": "从芯片角度出发，如何对生成式AI进行优化？",
        "answer": "最近的H100芯片通过加入名为transfer from transformers engine的软件层，在不影响精度的前提下，通过自动管理并调整数据格式（如FP8、FP16等）来优化模型性能，从而实现约2到3倍的速度提升。",
        "time": "00:31:09"
      },
      {
        "question": "H100在硬件层面有哪些优化改进？",
        "answer": "H100在硬件上进行了多项优化，例如SM（streaming multiprocessor）数量从108个增加到135个，处理数据的速度提高了约两倍；同时，它的时钟频率也提升了3分之1，这些都极大地有助于大语言模型的计算和inference。",
        "time": "00:31:53"
      },
      {
        "question": "H100在保密计算方面有何新特性？",
        "answer": "H100是首款集成dental computing能力的GPU，支持confidential computing，这对于保护大语言模型训练过程中数据的安全性至关重要，防止数据被云端或其他无关方查看。",
        "time": "00:32:54"
      },
      {
        "question": "NVIDIA在优化路径上与其他厂商有何不同？",
        "answer": "相较于其他AI加速器厂商，NVIDIA的优势在于其生态系统更为全面，不仅有硬件优化，还包括软件和networking communication等辅助设备的优化，以及搭建大规模GPU集群所需的基础设施如NV link switch和infused bandwidth等。",
        "time": "00:34:23"
      },
      {
        "question": "GTC大会上有哪些令人印象深刻的产品发布？",
        "answer": "在GTC大会上，特别关注到了Grace CPU的发布，其拥有强大的功能，如能与GPU良好互连并支持NV link特性，有望在服务器领域构建完整的生态，并展现出巨大的潜力。",
        "time": "00:35:17"
      },
      {
        "question": "对于大规模GPU集群管理的挑战有哪些？",
        "answer": "管理大规模GPU集群面临诸多挑战，包括异构调度、GPU与CPU之间的通信、分布式计算中的可靠性问题、有效利用GPU资源以及GPU重启后的恢复机制等。此外，针对不同代际和型号的GPU，如何实现资源调度的精细化和高效性也是一个难点。",
        "time": "00:41:36"
      },
      {
        "question": "在GPU训练和推理中，存在哪些具体的优化挑战？",
        "answer": "具体的优化挑战包括embedding layer的优化、GPU上的hot sporting问题以及load balancing。此外，CPU和GPU之间的数据传输是一个瓶颈，如何确保数据传输与计算需求相匹配是一个重大挑战。在大规模GPU集群的管理层面，如何分配网络资源、监控硬件状态（如pod、container的健康状况）并进行有效控制也是一大难题。另外，不同种类GPU之间的协调与平衡也是一个需要关注的问题。",
        "time": "00:43:11"
      },
      {
        "question": "Google内部能够处理并管理的GPU集群大概在什么样的规模？",
        "answer": "Google虽然面临GPU资源有限的问题，但如果作为云平台提供者，面对的主要是资源限制。实际上，对于内部训练大型模型，规模可以做到很大，但受限于软件层面的限制。目前大多数消费者使用的GPU数量还未达到上万个级别，所以GPU资源并不是最紧迫的问题。",
        "time": "00:45:01"
      },
      {
        "question": "如果使用较不强大的GPU并通过增大集群规模来解决问题，可能会遇到什么样的问题？",
        "answer": "若使用较小规模的GPU并加大集群规模，确实可能遇到模型规模过大导致单个GPU内存不足的问题。这时，可以采用分布式训练和分布式服务的方式。但如果模型规模不大，较小的GPU也可以达到相似效果，可能只需要1/3或1/4的资源即可满足需求。",
        "time": "00:46:27"
      },
      {
        "question": "针对不同任务，为什么需要不同类型的GPU，而不是追求one size fits all？",
        "answer": "Google最近发布的四款GPU针对不同的任务优化，例如L4和L8在AI视频处理方面表现出色，L4还拥有强大的artic core用于retracing；Oliver Cloud基于OMNIVRUS X，具有8个L4，其artic core强大；H100 MVL内存较大，适合需要大内存的模型；Grace hopper则是一个CPU和GPU集成的超级芯片，提供极高通信速度，尤其适用于推荐系统和vector database相关的任务。每款GPU虽通用，但在特定领域有更多优化，使其在相应任务上的性能表现更好。",
        "time": "00:47:57"
      },
      {
        "question": "随着开源社区的发展，未来大家是否会更倾向于通用GPU还是专业芯片？",
        "answer": "未来很难再有一个通用的GPU满足所有需求，专业芯片将会成为趋势。因为AI应用涵盖众多领域，每个领域虽有共性，但在上层应用层面存在显著差异，专用芯片能更好地针对这些差异进行优化。",
        "time": "00:51:41"
      },
      {
        "question": "模型训练和部署成本是否会继续下降，以及开源模型是否会带来技术门槛降低？",
        "answer": "随着开源项目的快速进展，预计今年开源模型能达到接近GPT3.5的水平，且随着数据积累和技术进步，每家公司都能针对自身应用场景微调模型并部署服务。推理成本方面，通过芯片技术的发展，如增加算力、降低能耗等，将进一步降低推理成本，并可能出现板卡级的推理产品，支持离线、低成本的小规模推断，应用场景更加广阔。",
        "time": "00:53:48"
      },
      {
        "question": "在当前业界实践中，模型部署是否趋向采用hybrid模型架构？",
        "answer": "是的，现在业界普遍采用的是hybrid模型架构，即使用一个较为便宜的模型处理大部分请求，而用一个更昂贵但更精准的模型处理复杂或难以处理的请求，这样可以降低整体成本。至于未来成本下降的边界在哪里，目前无人能知，但短期内在软件层面上可以进行很多优化。",
        "time": "00:58:28"
      },
      {
        "question": "对于中小企业来说，它们是否有必要自行建设大型语言模型？",
        "answer": "对于中小企业而言，他们通常没有自建大型语言模型的需求，完全可以购买LLM as a service的服务包，例如AWS和NVIDIA提供的服务。这样，他们可以将自有数据交给这些提供商，与他们已有的基础模型结合，并进行一两个月的训练，从而获得定制化的服务。",
        "time": "00:59:08"
      },
      {
        "question": "这种服务模式对于中小企业有哪些实际应用场景？",
        "answer": "中小企业可以通过这种LLM as a service的形式构建内部工具，如内部的codex或特定部门（如营销、财务等）的简单FAQ工具，以提高工作效率和准确性。他们无需从零开始研发模型，也无需投入大量精力寻找并拼凑开源资源。",
        "time": "01:00:59"
      },
      {
        "question": "厂商在面对AI服务选择时有哪些不同的诉求？",
        "answer": "不同厂商根据自身背景有不同的诉求。部分厂商对数据隐私要求极高，需要在内部完成训练和部署；另一些厂商则倾向于快速调用现成API，以节省时间和提升产品知名度。还有一些公司希望通过开源贡献来培养研发能力，或者选择与能够提供优质服务的to b企业合作，以简化和加快项目进程。",
        "time": "01:02:14"
      },
      {
        "question": "Google Cloud如何帮助客户管理和优化模型？",
        "answer": "Google Cloud提供vertex AI平台，不仅用于托管模型，还涵盖了整个AI生态系统，包括数据清洗、特征准备、数据质量监测、模型质量解释等核心功能。对于企业级用户，Google Cloud会提供一套完整的解决方案，满足合规性、安全性、可靠性和易管理性等方面的需求。",
        "time": "01:03:57"
      },
      {
        "question": "VeraTex AI在大语言模型部署方面做了哪些优化？",
        "answer": "VeraTex AI在其产品架构中针对大语言模型部署进行了优化，比如提供推理平台和服务，如证据API、LLM模型的embedding生命周期管理等，并计划在 roadmap 上推出更多功能，如模型管理和定制、prompt tuning平台，以及提供一个供用户构建在预训练模型上的自定义模型的平台。",
        "time": "01:07:57"
      },
      {
        "question": "大模型的应用会对传统MOops工具链产生何种影响？",
        "answer": "对于传统MOps工具链，大模型的应用将对workflow产生重大影响，显著降低开发和使用AI的成本和复杂度，因为企业不再需要从零开始训练模型或准备大量数据，而是通过基础模型进行微调。这将改变原有的工作流程，对原有的一些工具和流程带来挑战。而在monitoring领域，虽然需求仍然存在，但也会面临新的问题和挑战，比如如何在大模型环境下定义和跟踪指标，以及评估模型的表现。",
        "time": "01:12:19"
      },
      {
        "question": "对于大模型在生产环境中的问题，比如其生成内容不受控，百度工程师是否能通过处理全部数据集来解决这个问题？",
        "answer": "百度工程师要解决这个问题确实很困难，因为需要查看整个数据集，但这在现实中并不现实。",
        "time": "01:13:51"
      },
      {
        "question": "针对可控生成的问题，目前有哪些应对办法？",
        "answer": "我们可以通过查看微调的数据或者使用prompt引导大模型更新知识库，但这些方法可能无法解决所有问题，总体上挑战较多。",
        "time": "01:14:18"
      },
      {
        "question": "大模型出现后，编程范式有何变化？",
        "answer": "大模型尤其是规则的model service出现后，编程范式将超越传统的ops层面，带来一个更大的编程范式变化。快速上线产品时，可以调用强大的march language model处理工作负载和监控问题，开发模式会更高效，但具体编程范式如何改变还需进一步观察。",
        "time": "01:14:57"
      },
      {
        "question": "新一波AI创业潮中，APP形态有何变化？",
        "answer": "新的AI创业潮中，许多APP变得更加轻量级，与计算分离，用户需求得以快速反馈并得到改进。这种模式下，产品层形态、公司团队组织和融资形式等都将发生深远影响，但具体形态尚不清晰。",
        "time": "01:16:13"
      },
      {
        "question": "在监管方面，大模型带来了哪些新需求？",
        "answer": "在监管方面，各国政府对大模型的反应较为保守，尤其在可解释性和数据管理上存在滞后。因此，可能会催生出新的需求，例如定制的安全检查工具，以满足不同领域对于监管的要求。",
        "time": "01:17:59"
      },
      {
        "question": "OpenAI对于ChatGPT未来的发展方向是什么？",
        "answer": "OpenAI计划在GPT四代的基础上进行优化和清理，致力于使其更符合人的行为和道德规范，同时也关注AI的可解释性和数据安全等问题。",
        "time": "01:19:58"
      },
      {
        "question": "是否担忧儿童直接与ChatGPT等AI交互的影响？",
        "answer": "有人担忧儿童过度依赖ChatGPT等AI工具可能会影响他们的思维发展和问题解决能力。尽管AI可以提供答案，但家长应更多地引导孩子学会独立思考和探索世界，而非直接给出答案。未来可能会出现专门针对儿童教育场景训练的机器人，但家长对儿童接触AI持谨慎态度。",
        "time": "01:25:19"
      },
      {
        "question": "ChatGPT作为教师的耐心表现是怎样的？",
        "answer": "ChatGPT在教学过程中表现出极高的耐心，它不会像人类那样轻易爆发脾气。例如，一位对Python和pandas一无所知的用户向ChatGPT学习，ChatGPT循循善诱地引导该用户通过不断提问，从理解拍卖这种基础概念开始，逐步学会使用Python制作表格。即使面对畏难情绪强烈的Excel用户，ChatGPT也能耐心地教授他使用Python读取CSV文件并进行简单操作。",
        "time": "01:27:27"
      },
      {
        "question": "未来是否有可能将ChatGPT应用于教育领域，作为智能教育工具？",
        "answer": "是的，未来完全有可能利用大语言模型如ChatGPT来辅助教育工作。通过调整和优化，它可以作为一个优秀的私人教师，针对不同年龄段和专业方向的学生提供个性化的教育服务，这将是一个巨大的市场空间。",
        "time": "01:29:22"
      },
      {
        "question": "对于AI工具的价值，以及它们能否超越基础功能如GPT rapper？",
        "answer": "AI工具未来会变得更轻量级且易于迭代改进，以适应不同场景需求。尽管目前存在许多同质化工具，但每个细分领域仍有机会做细分和创新，创造出具有更高价值的AI工具。此外，AI工具与开发者工具的边界在未来可能会融合，尤其是在自然语言交互方面，可能会出现新的开发者工具需求，例如用自然语言描述需求并生成前后端应用等。",
        "time": "01:35:23"
      },
      {
        "question": "开发者工具市场的发展趋势如何？",
        "answer": "开发者工具市场看起来竞争激烈且差异化变小，许多平台提供商已将相关工具免费提供给开发者，导致市场盈利空间变小。然而，随着大语言模型的发展和应用场景的拓展，未来开发者工具仍有很大发展空间，特别是在利用自然语言处理生成代码和其他应用场景的工具上，尽管目前尚未明确具体的切入点和实现方式。",
        "time": "01:35:23"
      },
      {
        "question": "AI在解决实际生活中的问题上有哪些前景？",
        "answer": "期望AI能在接下来的几年内，特别是在五年内，能够研发出能够有效解决家务问题的AI应用，比如智能扫地机器人、烹饪机器人、切菜机器人和洗衣机器人等，以帮助忙碌的家庭更好地管理日常生活。",
        "time": "01:40:41"
      },
      {
        "question": "AI如何与实体设备连接并改变人们的生活方式？",
        "answer": "AI有望通过端上的自然语言接口与各类设备无缝对接，执行用户的指令，实现智能化交互。比如，用户可以通过自然语言与AI沟通，让其自动编译、组织和执行一系列任务，从而控制各种应用程序和实体设备，实现智能化的生活环境改造。同时，这一领域也引发了对未来教育、社会接受程度以及伦理法律等多方面的思考和讨论。",
        "time": "01:43:46"
      }
    ],
    "chapters": [
      {
        "time": "00:00:00",
        "title": "探讨AI进展及生态系统的迭代",
        "summary": "AI技术的快速发展引发了广泛讨论，特别是大语言模型及其周边生态系统的迅速迭代，包括硬件和软件工具链的相辅相成。本期节目聚集了来自NVIDIA、Google Cloud、Face Info等领域的专家，深入讨论了AI的发展机遇、挑战及未来趋势。话题涵盖了芯片架构、GPU集群管理、开发工具到AI的社会影响等多个方面，呈现了对AI未来发展的全面思考。"
      },
      {
        "time": "00:01:41",
        "title": "大语言模型生态系统及其应用前景探讨",
        "summary": "在本次讨论中，来自NVIDIA、Google和社区开源领域的专家分享了他们对大语言模型及其生态系统发展的见解。他们探讨了大语言模型在改变传统用户交互方式、提升搜索效率、优化产品推荐系统等方面的潜力。特别是，讨论重点放在了如何通过大语言模型的创新应用，比如通过对话式界面快速获取信息，来彻底改变to C的使用工具，以及如何在开源社区中推动AI技术的民主化进程。"
      },
      {
        "time": "00:06:47",
        "title": "代码生成APP改变软件开发模式",
        "summary": "讨论了一个名为Curse的APP，该APP基于聊天机器人帮助用户编写代码，体现了未来软件供给模式的转变。不同于传统的通用性中间件开发，Curse提倡根据具体需求生成一次性代码，简化了代码编写过程，提高了代码生产效率。尽管目前该APP生成的代码尚不完美，但随着技术的发展，预期能够更有效地对接API并确保代码正确性，进而推动软件开发向更具体、需求驱动的方向发展。"
      },
      {
        "time": "00:09:15",
        "title": "高策分享对copilot for code领域看法",
        "summary": "高策在交大读书时专注于分布式计算，包括集群调度和资源调度等领域，是全球排名前三的keep flow开源项目贡献者。他认为基础模型将是AI的新形态，扩大AI用户基础。目前他所在的团队正在开发一个服务列表推理平台，以方便用户尝试最新的开源模型。高策看好copilot for code领域的发展，尤其是GitHub的Copilot，尽管其补全效果目前仅能达至10%到20%的代码，但他预计未来随着技术进步，如GPT-5或6的出现，Copilot的补全能力将大幅提高。他认为Copilot在代码质量和架构设计方面将长期是一个优秀选择，对这一方向的发展持乐观态度。他还提到，对于需要满足数据合规性要求的公司来说，自托管Copilot或更小的模型是一个很好的创业方向。"
      },
      {
        "time": "00:13:21",
        "title": "代码生成与审查的未来趋势",
        "summary": "讨论集中在代码生成工具的发展及其对代码审查提出的新要求上。一方面，随着代码生成工具的不断进步，代码的生产速度将显著加快，尤其对于不需要放入生产环境的项目或原型，使用这些工具可以极大地提高效率。另一方面，对于需要高度严谨性和质量保证的代码，如航天航空领域，仍需保持传统严谨的编码方式。同时，随着大语言模型的发展，未来可能需要编写非常底层严谨代码的人员将逐渐减少。此外，对于AI辅助的代码审查成为了一个有趣的探索方向，它可以帮助确保生成代码的质量和适用性。讨论还提到了代码生成模型能够根据公司的代码库和新API学习新能力，预示着未来一次性代码生成和应用层面代码的快速开发将成为可能。"
      },
      {
        "time": "00:17:54",
        "title": "代码自动生成与编程的未来趋势",
        "summary": "讨论集中在代码自动生成的技术上，探讨了直接让机器生成代码以快速满足需求的可能性与挑战。一方面，有人认为通过花费少量时间学习API，可以更快速地调用和解决问题；另一方面，提出了利用自然语言处理技术，让机器直接根据需求生成代码或直接完成任务的想法，认为这种方式可能更高效、直接。此外，也提到了对于拥有独特商业逻辑的小公司和大型科技公司而言，代码自动生成面临的挑战不同，以及对法律、知识产权问题的考量。最后，讨论了未来编程可能不再以传统代码形态存在的观点，提出了通过自然语言描述任务，机器即可完成复杂任务的愿景。"
      },
      {
        "time": "00:22:37",
        "title": "大语言模型对未来技术的展望与NVIDIA的新产品讨论",
        "summary": "讨论集中在大语言模型如何帮助解决复杂问题，包括但不限于代码生成、利用不同开源插件或通过长链推理。尽管这类技术前景被看好，但其成熟应用可能需要时间。讨论还提到了NVIDIA GTC 2023上发布的令人兴奋的新产品，特别是库里总库，该库通过优化芯片生产过程中的计算密集型任务，大幅减少所需时间和资源，体现了GPU在提升制造过程效率方面的巨大潜力。"
      },
      {
        "time": "00:26:11",
        "title": "H100 GPU与光口技术对未来超级计算机的影响",
        "summary": "对话中讨论了H100 GPU引入光口技术的重要意义，认为这可能是未来超级计算机发展的一个重要方向。光口技术可以极大地提升芯片间的传输速度和效率，降低延迟，对于处理大语言模型等需要极高算力的任务尤其重要。通过光纤连接，未来可能会实现成千上万颗芯片的互联，大幅提高计算能力和效率。此外，还提到了光模块和光交换机在数据中心能效提升方面的应用，以及对未来网络设计和模型计算效率可能带来的革命性变化。"
      },
      {
        "time": "00:29:37",
        "title": "NVIDIA H100芯片与生成式AI优化",
        "summary": "讨论集中在NVIDIA的H100芯片以及其对生成式AI的优化上。H100芯片通过提升数据传输速度、增加streaming multiprocessor的数量、提高处理数据的速度以及支持更低精度的计算，从而实现对大语言模型处理速度的显著提升。此外，H100还引入了confidential computing能力，保障了模型训练和推理的安全性。比较了H100与前代产品A100在硬件和软件层面的优化，突出了NVIDIA在构建全面生态系统方面的优势。同时，也提到了NVIDIA在GTC上发布的Grace CPU，强调了其在服务器领域未来的潜力。"
      },
      {
        "time": "00:36:35",
        "title": "管理大规模GPU集群的挑战与解决方案",
        "summary": "在训练大语言模型等超大参数数的环境下，管理大规模GPU集群面临多项挑战。首先，GPU技术的迅速迭代导致硬件更新换代快，如从v100到A100的过渡。其次，GPU间的通信和数据传输成为瓶颈，初期依赖Message Passing Interface (MPI)，但未利用当时尚在研发中的MV Link技术。当前，采用如NVIDIA收购的以色列公司产品和big fat tree拓扑结构来提高数据传输效率成为通用解决方案。此外，GPU资源的调度、不同代GPU间的性能差异管理、以及弹性扩展能力（通过像Microsoft的Singularity这样的策略实现）也是关键挑战。解决这些问题需要深入的分布式计算知识和专门的项目组来攻关。"
      },
      {
        "time": "00:41:29",
        "title": "GPU集群管理及资源利用挑战",
        "summary": "讨论集中在GPU相对于CPU在资源管理上的挑战，包括异构调度、CPU与GPU之间的通信、分布式计算中的生产问题、可靠性、GPU资源的有效利用以及重启的代价。进一步探讨了在大规模训练和推理中GPU资源利用率低的问题，包括硬件与软件层面的优化、数据传输瓶颈、GPU集群的控制层挑战，以及不同种类GPU之间的协调。特别提到了谷歌在管理GPU集群方面的经验，强调了GPU资源的珍贵性和分配给内部与云客户资源的挑战。"
      },
      {
        "time": "00:46:14",
        "title": "GPU选择与AI任务性能优化探讨",
        "summary": "在面临无法获取高性能GPU的情况下，可以通过构建更大规模的GPU集群来应对，但会遇到模型规模过大导致单个GPU内存不足的挑战。分布式训练和服务成为可能的解决方案。对于不同AI任务，存在针对特定任务优化的GPU，如针对AI视频处理、图像生成、推荐系统等的专门GPU，这些GPU通过不同的设计优化，在特定任务上表现更佳，但同样能够执行其他类型的任务，只是性能有所不同。未来趋势可能更倾向于专用芯片的发展，以满足AI应用在各个领域的特定需求和优化。"
      },
      {
        "time": "00:52:09",
        "title": "开源模型和硬件进步对AI成本影响及未来展望",
        "summary": "随着硬件能力的增强和开源社区对代理模型的关注，AI模型的训练和部署成本正在降低。特别地，一些开源项目如土味动物、国内的ChatGLM和RKV等迅速发展，预示着未来可能实现与GPT-3.5相当水平的开源产品，满足大多数应用场景需求，而无需依赖昂贵的专有模型。此外，随着GPU等硬件的进化和优化，推理成本有望进一步下降。展望未来，可能会出现板卡级的推理产品，能够大幅降低运行成本，为智能家居、个人电脑等设备提供离线推理能力，拓展AI技术的应用范围和降低使用门槛。"
      },
      {
        "time": "00:56:17",
        "title": "探讨项目优化的主流路径与绕过Python的策略",
        "summary": "在当前的项目优化领域，几种主流的优化路径包括资格认定（qualification）、硬件层面的自动化优化、算子优化和减少CPU与GPU之间的通信。特别地，绕过Python的优化方式因其在业内产生的巨大影响而受到较少的关注。Python虽然因其便利性被广泛使用，但在不需要极大灵活性的场合，通过使用C++重写逻辑可以显著提高效率，甚至在没有GPU的情况下达到与GPU相近的性能。此外，通过采用各种优化技巧和编程语言，有望在未来实现推理成本的大幅度降低。"
      },
      {
        "time": "00:58:22",
        "title": "探讨降低链路成本的策略与LLM as a service的商业模式",
        "summary": "在减少整个链路成本方面，业界通常采用一种混合模型，即使用一个成本较低的模型处理大多数请求，而使用成本较高的模型处理复杂请求，从而在总体上降低成本。短期内，通过软件层面的优化可实现成本降低。从商业模式角度看，中小企业不必自建大型语言模型，而是可以选择购买LLM as a service服务包，如AWS和NVIDIA提供的服务。这些服务允许企业将自己的数据加入到服务商已有的基础模型上，并进行一些训练，之后服务商将提供模型推理服务。这种模式尤其适合中小企业，它们可以借此创建内部知识库或针对特定功能的FAQ工具，提高信息检索的效率和准确性。"
      },
      {
        "time": "01:01:52",
        "title": "AI技术应用及企业需求多样性探讨",
        "summary": "在AI技术发展的背景下，企业对于AI解决方案的需求呈现出多样化的特点。一方面，一些特定行业的企业如制药、银行等，由于对数据隐私和安全性的极高要求，倾向于采用自研的方式，确保数据不跨境或出机房，强调了在本地完成训练和部署的必要性。另一方面，面对AI技术高速迭代和市场需求快速变化的压力，一些企业选择调用现有的API服务，以便快速推出产品，抓住市场先机。此外，还有企业通过在开源项目上投入精力，以保持研发能力，不落于技术发展的后尘。为满足这些多样化的商业需求，云服务提供商如Google Cloud和AWS等，致力于提供一站式的AI生态系统服务，包括模型托管、数据处理、质量监测等，同时确保满足企业的合规性和安全性要求，提供可靠性和全方位管理的解决方案。"
      },
      {
        "time": "01:05:58",
        "title": "Vera tex AI大语言模型部署优化及未来计划",
        "summary": "Vera tex AI针对大语言模型的部署进行了多项优化，主要集中在推理平台和Embedding生命周期管理方面。他们提供了私预览版的evidence API，用于生成Embedding并与匹配引擎结合，以优化Embedding的整个生命周期。此外，团队计划帮助用户更好地管理Embedding，利用Google在该领域积累的经验优化性能。未来，Vera tex AI计划提供文本生成、多模态图像生成、代码调优和生成等功能，并建立一个平台，让用户可以基于现有的基础模型进行定制，包括模型的训练和部署，以及提供Prompt调优平台。"
      },
      {
        "time": "01:08:49",
        "title": "AI大模型对MO ops工具链的影响",
        "summary": "随着AI应用越来越依赖于大模型，传统MO ops工具链面临重大变革。大模型的引入显著减少了AI开发的成本和复杂性，特别是在workflow和model monitoring两个关键领域。对于workflow，大模型降低了从零开始训练模型的需求，简化了数据准备到模型上线的整个过程，从而可能使得相关的MO ops工具受到挑战。在model monitoring方面，尽管大模型的引入带来了变化，但监控需求依旧存在，只是方式和侧重点可能会有所不同。大模型的普及提高了AI的投入产出比，减少了对数据科学家和信息化团队的需求，提出了对自动化和API服务的更大依赖，预示着MO ops工具链未来的方向和发展需求将围绕这些变化进行调整和优化。"
      },
      {
        "time": "01:12:19",
        "title": "大模型对AI开发和应用的深远影响",
        "summary": "随着基础模型的发展，AI领域对模型监控、服务和功能等方面的需求呈现增长趋势，同时也带来了新的挑战，例如如何在条件模型场景下进行有效训练和评估。大模型的普及促使AI应用开发模式发生变化，强调了轻量级APP和私有部署AI的重要性，以及将产品与计算分离的趋势。此外，还探讨了大模型如何影响传统的工作流和监控，以及对未来编程范式、产品形态、公司团队组织和融资形式的潜在影响。"
      },
      {
        "time": "01:17:17",
        "title": "生成式AI技术的发展及其面临的挑战",
        "summary": "生成式AI，特别是知识库和数据库技术，市场需求巨大，但同时面临数据丢失、监管要求高等挑战。讨论指出，虽然技术发展迅速，但各国政府和企业在适应这一新技术时表现出不同程度的保守性和严谨性，尤其是在可解释性、数据安全性和版权问题上。此外，技术进步可能加剧不同国家、政府和公司在AI发展上的差距。"
      },
      {
        "time": "01:22:16",
        "title": "探讨ChatGPT对儿童教育的影响及未来应用",
        "summary": "讨论重点在于ChatGPT技术的当前发展状态、对儿童教育的潜在影响，以及未来可能的应用方向。首先，提到了OpenAI公司对于GPT-4的持续优化工作，强调了公司在AI开发上的责任感，不仅仅是追求利润最大化。随后，通过家长让儿童与ChatGPT互动的例子，引发了对AI在教育领域应用的深思，担忧儿童过度依赖AI可能影响其正常的社会交往和思维发展。最后，提出尽管ChatGPT展现出了作为教育辅助工具的巨大潜力，比如能够耐心解答问题和引导学习，但同时也强调了需要对AI在教育中的应用进行更多讨论和探索，特别是针对儿童的特定需求和成长过程。"
      },
      {
        "time": "01:30:00",
        "title": "大语言模型对未来教育和AI应用的影响",
        "summary": "讨论重点在于大语言模型，如GPT，在教育领域的潜在应用及其对AI应用开发的影响。一方面，大模型可能辅助教育工作，成为像苏格拉底那样的导师；另一方面，它们可能改变AI应用的开发方式，使得应用变得更加轻量化和同质化。特别地，虽然GPT等模型能大幅度简化工作，但它们对于特定领域的深入知识仍有所欠缺，需要人类的补充。此外，讨论还触及了AI应用的未来趋势，包括自动GPT的探索，以及如何让这些模型更有效地辅助人类。最后，提出了对于开发者工具市场现状的观察，认为由于平台提供商的工具日益普及，专门开发此类工具的公司的生存空间可能变得有限。"
      },
      {
        "time": "01:35:22",
        "title": "开发者工具的未来与大语言模型的应用潜力",
        "summary": "近期出现了许多新的开发者工具，特别是在大语言模型领域，这反映了开发者工具市场的活跃和潜力。随着技术如GPT-3.5、GPT-4的发展，未来大语言模型如GPT-5和GPT-6将带来更多可能性，可能彻底改变开发方式，例如通过自然语言直接生成应用程序。尽管目前对于具体应做什么还不明确，但开发者工具领域，尤其是为大语言模型设计的工具，被视为一个值得期待的市场。历史经验表明，像Hugging Face这样的公司通过专注于特定技术领域的工具和社区建设，成功实现了商业化。未来，随着公有云和大语言模型提供商之间的差异逐渐缩小，无供应商锁定的、开源的开发者工具将更加受到青睐。此外，应用程序的壁垒在于数据，没有数据支持的应用很容易被替代，强调了数据在大语言模型时代的重要性。"
      },
      {
        "time": "01:40:33",
        "title": "期待AI在家庭日常中的更广泛应用",
        "summary": "讨论集中在对未来AI技术发展的期待上，特别是希望AI能够在家庭日常任务中扮演更重要的角色，如家务支持系统。期待AI技术能在五年内实现更实际的应用，比如家务机器人，解决煮饭、扫地、切菜等日常琐事。同时，对未来AI与实体世界的连接，AI在处理实际问题时的自我编译、自组织能力表示出极大兴趣。讨论也触及了AI发展的社会影响、人机交互方式的变革以及对AI技术社会接受度和监管的思考。"
      }
    ],
    "mindmap": {
      "children": [
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "GPT-3.5, GPT-4"
                },
                {
                  "children": [],
                  "content": "面向未来的期待: 更强大模型（如GPT-5）的潜力"
                }
              ],
              "content": "大语言模型"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "教育领域: 个性化教学助手"
                },
                {
                  "children": [],
                  "content": "家务自动化: 扫地机器人、煮饭机器人等"
                },
                {
                  "children": [],
                  "content": "开发者工具: 代码生成、API交互优化"
                },
                {
                  "children": [],
                  "content": "通用AI助手: 自然语言交互，处理日常任务"
                }
              ],
              "content": "应用场景"
            }
          ],
          "content": "AI技术与应用"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "AI的自主学习和创造能力增强"
                },
                {
                  "children": [],
                  "content": "更多的物理世界与数字世界交互"
                }
              ],
              "content": "生成式AI的未来"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "加速AI技术的普及和应用"
                },
                {
                  "children": [],
                  "content": "促进跨领域合作和创新"
                }
              ],
              "content": "开源社区的影响"
            }
          ],
          "content": "技术发展与展望"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "个性化学习的潜力与挑战"
                },
                {
                  "children": [],
                  "content": "人类教师的角色转变"
                }
              ],
              "content": "AI对教育的影响"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "用户数据的保护"
                },
                {
                  "children": [],
                  "content": "AI系统的透明度和可解释性"
                }
              ],
              "content": "数据隐私与安全"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "人类与AI的共存"
                },
                {
                  "children": [],
                  "content": "避免AI技术的滥用"
                }
              ],
              "content": "AI伦理"
            }
          ],
          "content": "社会与伦理问题"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "在家庭和工作中的应用"
                },
                {
                  "children": [],
                  "content": "自主学习和任务执行能力"
                }
              ],
              "content": "人形机器人与AI助手"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "AI技术的社会影响评估"
                },
                {
                  "children": [],
                  "content": "监管与伦理准则的建立"
                }
              ],
              "content": "社会接受度与法律框架"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "促进AI技术的负责任发展"
                },
                {
                  "children": [],
                  "content": "解决全球性挑战（如气候变化、医疗健康等）"
                }
              ],
              "content": "全球合作与跨学科研究"
            }
          ],
          "content": "未来技术与社会形态"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "家务处理、个人助理"
                },
                {
                  "children": [],
                  "content": "生活品质的提升"
                }
              ],
              "content": "日常生活的自动化"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "自动化工具和助手"
                },
                {
                  "children": [],
                  "content": "职业转型和新技能需求"
                }
              ],
              "content": "工作与职业的影响"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "个性化学习资源"
                },
                {
                  "children": [],
                  "content": "终身学习与技能提升的便利"
                }
              ],
              "content": "个人发展与学习"
            }
          ],
          "content": "AI技术对个人生活的影响"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "算力需求与能源消耗"
                },
                {
                  "children": [],
                  "content": "数据隐私保护"
                },
                {
                  "children": [],
                  "content": "AI模型的偏见与错误"
                }
              ],
              "content": "技术挑战"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "新的商业模式"
                },
                {
                  "children": [],
                  "content": "开源与社区驱动的创新"
                },
                {
                  "children": [],
                  "content": "跨界合作与应用拓展"
                }
              ],
              "content": "商业与创新机遇"
            }
          ],
          "content": "未来技术挑战与机遇"
        }
      ],
      "content": "对话脑图摘要"
    }
  }
}