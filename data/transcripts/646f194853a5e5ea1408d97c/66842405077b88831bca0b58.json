{
  "pid": "646f194853a5e5ea1408d97c",
  "eid": "66842405077b88831bca0b58",
  "title": "深度对谈 vLLM 团队：如何从零搭建一个成功的开源生态",
  "task_id": "g34dn8ewplognwjz",
  "transcription": [
    {
      "time": "00:00:02",
      "text": "在过去这一年，感觉VLM变化的特别多。它真的从一个小小的技术点变成了一个非常完善的一个framework。从一个科研角度来说就是一个百宝箱。你想实现各种的想法feature，都可以在VLM找到一个入口。",
      "speaker": "发言人1"
    },
    {
      "time": "00:00:21",
      "text": "作为有能力做出一个很有意思开源项目的人，你怎么去抑制一种我行我上我去把它改的更好。而是去让社区慢慢的成长起来，去给予他耐心，然后让大家的贡献一个一个搭起来。",
      "speaker": "发言人2"
    },
    {
      "time": "00:00:43",
      "text": "如果你是做一个纯粹的research的话，比如说你要发一篇论文，你可以有很多的assumption。但是如果要做一个系统，大家都能用，你不能说我只能在这些环境下用。所以我觉得这是最困难的一点，就是你需要有全面的支持。",
      "speaker": "发言人3"
    },
    {
      "time": "00:01:04",
      "text": "我们项目的目标从最一开始就是成为一个最快和最好用的开源引擎，并且能够成为这个大语言模型推理的一个标准。",
      "speaker": "发言人4"
    },
    {
      "time": "00:01:19",
      "text": "欢迎大家收听此话当真，这个基金团队将在此和各领域的领军人物一起分享最新热点和行业洞察。真格你的创业第一站，我是真格基金的温迪。本期节目我们邀请到了VRM主导团队的sim莫、李卓翰、刘凯超和刘晓轩、lily和我们一起来聊一聊VRM创立的故事，以及作为一个具备国际影响力的开源项目，VRM做对了什么。",
      "speaker": "发言人5"
    },
    {
      "time": "00:01:46",
      "text": "真格基金正式宣布向VLM提供捐赠，我们今天同时也邀请到了真格基金的管理合伙人戴雨森。他在过去的一年半时间里，密切地关注着AI方向的创新机会，也主导了这次向VLM项目的捐赠。首先请宇森和大家打个招呼。",
      "speaker": "发言人5"
    },
    {
      "time": "00:02:06",
      "text": "嘿大家好。",
      "speaker": "发言人6"
    },
    {
      "time": "00:02:08",
      "text": "今天是首次邀请到VRM团队全员做客计划当中，相信每一位成员都有自己的视角和故事。想先请大家介绍一下自己在VRM团队中负责的工作，以及比如大家最开始都是怎么知道，然后怎么如何决定加入VRM的。那要不先按加入的时间顺序来，先有请卓翰。",
      "speaker": "发言人5"
    },
    {
      "time": "00:02:31",
      "text": "大家好，我是李卓翰，目前是加州大学伯克利分校的博士生。然后我的研究领域是机器学习系统。而在这个领域我们会通过系统优化提高机学习算法和模型的一些执行速度，以及研究如何做更大规模，更加高效的并行执行等等一系列的问题。",
      "speaker": "发言人4"
    },
    {
      "time": "00:02:49",
      "text": "我其实从第一天就是在这个VM团队里边，我从这个大概是VM还是一个idea，不只是一个从零的时候一直在开始。然后我们首先是小伙伴为你做了这个page attention的这一个算法。然后我们先投了这个research paper，然后再下一步的这个开源项目整理，然后再从开源项目发布，一直到今天。目前的话我还是主要是负责一些从开源社区上来说的，就是比方说来录一下这些博客，这种开源的一些宣传以及开源社区的管理。然后在技术上的话，我可能会看一些比方说high level的整个框架的design，或者是更高层的一些设计的一些事情。",
      "speaker": "发言人4"
    },
    {
      "time": "00:03:28",
      "text": "谢谢卓翰哲瀚是创始成员。下一个邀请丽丽大家好。",
      "speaker": "发言人5"
    },
    {
      "time": "00:03:34",
      "text": "我是丽丽刘晓萱。我现在也是伯克利的一个PHD，我是在BLM paper release的时候知道了这个项目，觉得非常有意思。然后和导师也聊了一下，正好当时也做了一些关于推理优化的科研工作。我们就觉得可能可以在VLM里试一下自己的科研方面的想法，所以当时就决定加入vm，我在VIM你做的工作其实主要就是做一些跟科研结合的，就是看一看在VM这个框架内部有没有可以继续探索继续挖掘的科研方向。做一些比如说推理加速，或者比如说scheduling的优化等等等等。所以是更加科研导向。但我们做科研的目的，最后还是希望有更多的人可以看到，更多人可以用。",
      "speaker": "发言人1"
    },
    {
      "time": "00:04:25",
      "text": "谢谢lily。然后是Simon。",
      "speaker": "发言人5"
    },
    {
      "time": "00:04:28",
      "text": "好的，大家好，我叫Simon，我是伯克利大学第二年VHD学生。我知道vn和page tension是在写论文的时候我就知道了。其实我可能有一个小机会加入这个论文的团队，但当时我还在我看其他的相关的项目。我大概是在暑假结束的时候加入了v one团队，帮忙然后搭建整个开源的生态。这也是我大概现在的一个工作。因为我从七年前在伯克利读本科生的时候，就在做相关关于开源项目，特别是在机器学习在生产环境中的这种serving和instance项目。做过技术上的和product上面的一些工作。所以说现在我大概算是BMM的PM或者是community manager，或者是架构相关的一些工作。",
      "speaker": "发言人2"
    },
    {
      "time": "00:05:22",
      "text": "谢谢Simon，最后想有请凯超，因为凯超是最新加入团队，也可以分享一下最近几个月的体验。",
      "speaker": "发言人5"
    },
    {
      "time": "00:05:29",
      "text": "好的，我是游凯超。我和他们不一样，就是我是清华的博士生，然后今年是在伯克利这边访问。我之前做的有一些机器学习的算法理论，包括一些机器学习编译器相关的内容，我知道VOM是因为我要来这边访问，我的访问的那个host professor是VM的，这个项目的professor就是Young story a因为我之前做了一些关于机器学习编译器相关的内容。本来来这边是想说VOM里面有大模型的推理，然后推理优化里面有一部分就是用机器学习编译器，所以我比较自然的就是加入到VOM这边来，作为我访问的一个主要的重点项目。但是加入之后，就是在开始那个机器学习编译器的相关的进展之前，我发现VOM作为一个open source project，还有很多其他的需要优化的部分。如果我直接撇开那一部分不管，我就强行做一点点那个机器学习编译器的部分，就觉得意义不是那么大。因为得先把VOM做好，后面里面的每一部分才会有它的价值。所以我现在就是先在做WM的open source的maintenance这一部分。",
      "speaker": "发言人3"
    },
    {
      "time": "00:06:54",
      "text": "首先想还是请大家和听众朋友们介绍一下VRM项目。要不我们先做一个简单的，没有技术名词的介绍，方便还不了解VRM的听众们熟悉我们的项目。有请三位来跟我们解释一下。",
      "speaker": "发言人5"
    },
    {
      "time": "00:07:11",
      "text": "好的，没有问题。VN是你在在AI产品整个站里面，其中关于大模型大语言模型的推理与服务引擎。简单的说VOM的功能是将已经训练好的模型部署到生产环境里面。它这个项目着重于效率与成本。比如说你跟这个模型进行沟通或问问题的时候，他能回复的更快，也能让GPU物尽其用。也就是在产品方使用这个服务的时候，能让一个他付了一个GPU的钱，能让他能服务更多的用户。然后最终可以让AI产品落地更快这么一个软件的产品。",
      "speaker": "发言人2"
    },
    {
      "time": "00:07:58",
      "text": "我其实也想补充一点，这些技术里面首先是我们提出的这个page attention的算法。Page attention是一种新的attention的计算的方法。我们利用这个操作系统里面的配件和virtual memory的技术，来管理transformer里面的attention操作里面用到的KV cash memory。然后我发现就是通过这个技术，能够比之前的最好的系统能够有大概提高4到5倍的内存利用率，导致大概四倍的吞吐量提升。或者说就是用同样的一块GPU，你可以比之前更多四倍的一个请求。",
      "speaker": "发言人4"
    },
    {
      "time": "00:08:30",
      "text": "然后就除了这个配角三项之外，VM也是一个比较完整的一个大元模型的推理和部署的引擎。所以像是其他的，比方说像continue batching，像a graph，然后模型量化模型并行previous catching by decoding等等一系列的技术都会包括在我们这个项目里面。并且我们也有很多一系列为推理定制的一些扩大kronos。然后这些技术一起提高了这个大元模型的推理和速度和吞吐量。然后使得最后我们会直接使用比方说的小模型，用transformer library做推理相比，最终能够有大概1到2个数量级的吞吐量提升。",
      "speaker": "发言人4"
    },
    {
      "time": "00:09:12",
      "text": "除此之外我们VIM也是一个作为一个开源项目，它也和比方说哈根face社区紧密集成。这也使得我们能够支持一键部署high face上的大部分的主流的模型。我们现在应该支持大概30种不同的大语言模型。比方说像是拉马misty，也包括国内的像是千问名义万物等等的模型。行，我们都有做相应对应的适配。",
      "speaker": "发言人4"
    },
    {
      "time": "00:09:35",
      "text": "同时我们也不在不停的做一些前沿的探索。比方说除了英伟达GPU之外，我们怎么样能够把VM在其他的各种各样的硬件上面跑起来，这也是我们一个优化的目标。并且我们也在做一些前沿的research，能够让进一步的提高这个系统的效率，能够让我们推进的速度变得更快。这个大概就是一个从技术上来看VM的一个介绍。",
      "speaker": "发言人4"
    },
    {
      "time": "00:09:59",
      "text": "好的，那现在看来VRM就是具体的用户都包括哪些呢？",
      "speaker": "发言人5"
    },
    {
      "time": "00:10:06",
      "text": "其实这么说，作为一个开源项目，很难知道你具体的用户是谁。因为用户他可以直接就不用告诉你，不用sign up，也不用问任何人就能使用你的软件。但从我们所知道的来讲，用户包括几乎所有的云服务平台。当他们在提供代源模型服务的时候，基座都是用的是VOI。同时也有很多开源大模型的厂商直接就把VR拿来用了，作为他们服务的后端。与此同时也会有很多用AI辅助的产品，他们会有能力去微调一些模型，去用一些开源模型，然后再用VOM再作为他们的使用引擎。在后面。",
      "speaker": "发言人2"
    },
    {
      "time": "00:10:58",
      "text": "其实这些用户在分类上还是挺广泛的。然后我很好奇的一点就是像这种大模型在VRM上跑，有没有数字可以显示，出现在成本可以节省多少，然后大概对他们来说有什么样的优势？",
      "speaker": "发言人5"
    },
    {
      "time": "00:11:16",
      "text": "这个问题怎么说，就很难找到一个具体通用的数字。当然因为这个是这样子的，它是对各种硬件模型和场景都极其依赖的一个数字。然后同时也问题在于就是你跟什么去对比，比如说你看我们的论文本，里面写的是可以对标hugin face，在OPT模型可以快个24倍，当然那是一个非常具体的一个我们叫做benchmark set up，也就是一个使用场景。所以说我们其实是推荐大家都在自己的使用场景上尽量去试，然后尽量去比。然后也能看一下VOY在那个上面有没有帮助。当然我们也能看到很多厂商大家最后选用VOM是因为了它的能节省成本并提高速度。",
      "speaker": "发言人2"
    },
    {
      "time": "00:12:04",
      "text": "在这个地方我想问一下一个follow up的问题就是。关于模型推理的加速，其实有不少的技术方案思路，对吧？能不能跟大家再进一步的介绍一下，咱们VM是在什么技术方向上去提供这样的性能的。",
      "speaker": "发言人6"
    },
    {
      "time": "00:12:20",
      "text": "这个的话可以分很多个方向来讲。首先能不能去忍受一些在正确性across上面的一些损失。可以的话你就会走模型量化的一条路子。如果不可以的话，就会走其他比如说speculating一条路子。或者你想要更大的吞吐量，或者是调整不同的performance street off的时候，就是你要去知道它有多快响应。",
      "speaker": "发言人2"
    },
    {
      "time": "00:12:49",
      "text": "去调的时候，你可以用其他的算法，包括chunk preview，或者是用一些更多的memory去做preface这些都是通过trade不同的一些系统的资源去提高整个模型推理能力。比如说最开始说的quantization，就是你会失去一些正确性accuracy来提高速度。Spect decoding是用更多的compute去提高速度。Chm prefer是失去了一些time to first token，也就是产生第一个token的时间，来提高overall大家in lens y的一个稳定性。A previous cussion是用了多余的memory GPU RAM去提高你time to first token的一个速度。所以说其实到最后就会变成有不同的优化。然后大家都在其中在里面选择对使用场景最合适的一个方式。要不卓翰也跟大家继续介绍一下。",
      "speaker": "发言人2"
    },
    {
      "time": "00:13:50",
      "text": "对我其实也想补充一点，其实就是刚刚三个列举了非常多的各种推理的优化的方法。其实我觉得还有一种把这些方法分类的方式，就是我们可以把这些优化方法分成大概两类。第一类是在单个模型和单个请求上面做了一些话。就比方说像是Simon刚刚提到的quantization，就是说做模型量化，我用一个更低的精度去跑这个模型，或者是一个after GPU coronal，就是说我把这个GPU的这个代码写的更好一点。比方说我就在推理的时候，我把这个推理的这些矩阵乘法这些操作，用一个更好的针对于推理的方式来实现，让它变得更快。这一类的优化其实是在VIM之前，有很多的推理都已经在做了一些优化。就不像包括英伟达的tensor RT或者像是英伟达的发射transformer这个library，他们也都是做这样的一些优化。",
      "speaker": "发言人4"
    },
    {
      "time": "00:14:38",
      "text": "但是对于大模型推理来说，还有一类其实更加重要的优化是，就比方说在scheduling或者是在你有好多个不同的请求同时来到你的这个系统的时候，你需要做的优化。这个优化就包括像是continue fashion，就是在一个请求执行到一半的时候，突然来了一个新的请求。你要想要让这个新的请求和执行到一半，从就业一起执行。大概这么一个技术。以及像我们提出的配置attention是为了管理在做continue action的时候碰到了内存的问题，需要做的一个特定的优化。然后sam刚刚提到的像是trunk preview和preview的这其实都是为了在你跨好几个不同的请求的时候需要做的一些优化。然后这一类优化其实在VIM之前基本上没有什么大模型推理的框架，会想到要去做在这个角度的优化。所以对我觉得这个可能是VM的一个一开始比较特殊的一点，这也是他为什么能够比之前的框架快那么多的一个原因。",
      "speaker": "发言人4"
    },
    {
      "time": "00:15:38",
      "text": "所以卓翰你们是怎么在2022年，我记得当时还是在ChatGPT出来之前，就想到这样一个想法，推出VLM的，应该是第一个版本的。这是一个非常新颖的一个思路对吧？",
      "speaker": "发言人6"
    },
    {
      "time": "00:15:53",
      "text": "对，其实这里也有一个小故事，大概是在2022年年底的时候，这确实像刚刚雨森说的是在GPT lunch之前。然后那个时候我们在学校里面set up了一个大语言模型的demo。然后当时service的模型还是一个facebook的OPT175B模型。当时facebook应该也还叫facebook对还没叫meta。然后那个时候是一个非常古老的一个模型。然后我们当时一开始set up这个demo的目的其实是为了宣传我们之前的另外一个开发项目，叫做alpha ALPA。然后这是一个自动做模型并行和推理的一个研究的项目。",
      "speaker": "发言人4"
    },
    {
      "time": "00:16:30",
      "text": "但是我们当时在部署这个demo的时候，我们发现我们的这个demo特别慢。并且这个GPU的利用率特别低，这个就是让我们意识到大语言模型推理本身是一个值得关注的问题。特别是就是你超越这个模型本身的一些优化。就比方说就alpa本身其实在做一个模型，我这一步推理的时候，我怎么样做这个并行执行能够做得更快这一件事情。但是只做这一点优化是不够的，你需要做更多的这种跨请求的这些优化。我们发当时在市面上应该完全没有任何做大语言模型优化的一个开源系统。所以我们也就打算自己动手做一个，就是从零开始做。",
      "speaker": "发言人4"
    },
    {
      "time": "00:17:09",
      "text": "写到一半的时候就会发现，在这个大模型做推理的时候，其实GPU的memory本身是一个非常大的瓶颈。并且之前管理这个memory的方式都有很大的一个浪费。因此我们也在多次迭代之后，就提出了一个新的attention计算的方法。Attention是一个transformer，或者是大约模型里面非常重要的一个操作。我们就是为了改善这个操作的一个内存使用的一个瓶颈，我们提出了一个新的算法叫做page attention。在这里我们也像刚刚提到的，我们其实利用了操作系统里面的这个配件和retro memory的技术来管理这个transformer里面用到了这个KV case的memory。我们会发现这个技术能够比之前节省很多内存，并且也能够让这个最后的存储量提升了很多。",
      "speaker": "发言人4"
    },
    {
      "time": "00:17:53",
      "text": "然后我们大概是在二三年的二月份左右，开始把配置attention作为一个research idea开始做对。然后我们开始写paper，并且我们也开始build一些research prototype，跑一些实验来证明我们这个idea是value。然后大概到4月底的时候，我们把论文投出来了。在二三年6月底的时候，我们release我们的open source拓展。我们也非常幸运有很多人了解和使用了我们拓展对吧？我们一直维护到了今天。",
      "speaker": "发言人4"
    },
    {
      "time": "00:18:22",
      "text": "对，因为VLM的论文作者和现在的维护者，现在项目组成员，其实还是经历了一个演变的过程，对吧？然后咱们也是一个开源项目，跟我们经常聊的很多商业创业公司也不太一样。能不能介绍一下咱们现在团队成员的组成，然后大家是以怎样的组织形式一起来进行合作的。要不现在的这个项目维护者Simon，还有佐汉作为项目的发起人都可以介绍一下。",
      "speaker": "发言人6"
    },
    {
      "time": "00:18:56",
      "text": "其实有个很简单的答案，就是大家作为主导团，对，都是伯克利的博士生。我们都在同一个lab，都有同样的一组的。The riser其实也没有非常严谨或者严密的组织形式。大家都是因为兴趣，然后对这个项目的喜爱，同时是愿意去在这做一些开源相关的工作才聚在一起的。当然这也代表着我们并没有任何业绩考核，也没有OKR，然后adviser也对我们也没有什么硬性的要求，全靠一个内在驱动的一个合作项目，就是为爱发电的意思。差不多。",
      "speaker": "发言人2"
    },
    {
      "time": "00:19:37",
      "text": "我想补充一点，就是说其实作为伯克利博士生，我觉得我们实验室有一个比较好的传统，是说我们喜欢做这种非常practical的问题。就是说非常在工业界大家现在碰到的一个最痛点的问题。我们也希望我们做出来的idea能够在工业界能够被部署，能够被直接使用。",
      "speaker": "发言人4"
    },
    {
      "time": "00:19:56",
      "text": "在工业界有直接的impact。做开源项目本身是一个非常好的直接实现这种impact的一个方式。对你相当于如果你有一个自己的research idea，然后你把它做成一个比较好的开源project。然后大家就会能够很容易的使用上你的这个开源project。因此也能够用到你的这个research idea。这一点本身其实也能够增加你的这个research idea的impact和影响力。所以我觉得这个对于特别是系统方向的博士生来说，也是一个很有吸引力的点。",
      "speaker": "发言人4"
    },
    {
      "time": "00:20:25",
      "text": "所以在一开始咱们这个项目的目标是什么？然后你们现在怎么看自己离这个目标的距离实现程度呢？",
      "speaker": "发言人6"
    },
    {
      "time": "00:20:33",
      "text": "我觉得项目的目标从最一开始就是成为一个最快和最好用的开源引擎，并且能够成为这个大语言模型推理的一个标准。我觉得两方面最快，我们一直在努力。对，因为有大家一直在不停的追赶，然后我们也有不停的新的技术出来，我们也不停的在优化我们的框架，我觉得我们还是非常好用的。我们把这个好用这一点，其实从第一天开始就把它的这个priority优先级提到非常高。所以我们觉得这个好用是一个开源项目，必须要有的点，这样子大家才会开始用，然后才会给你提问题，或者给你做进一步的contribution。所以这个是第一道门槛，我们也需要把第一道门槛放的非常的低，成为一个标准。这件事情我觉得我们也还是在努力，我们希望能够做到这一点，也还是一个逐渐的一个过程。",
      "speaker": "发言人4"
    },
    {
      "time": "00:21:19",
      "text": "我想补充一下因为在我刚加入BLM的时候，当时是读他们的论文。他们当时是只有配置tension，主要是围绕这个technique，然后它是一个是框架。但是在过去这一年，感觉VRM真的变化的特别多。他真的从一个小小的技术点，或者说是一个优化的方向，变成了一个非常完善的一个framework。真正的framework就像刚才Simon和卓翰他说了各种各样的优化。所以我觉得BLM现在作为一个推理框架，就是目标不知道，但是从一个科研角度来说就是一个百宝箱。就是说你想实现各种的想法，或者想尝试各种的功能feature，你都可以在VLM找到一个入口，然后你可以很快的实现它。",
      "speaker": "发言人1"
    },
    {
      "time": "00:22:03",
      "text": "丽丽要不正好讲一讲，比如说这几年。两年，你觉得刚才从page tension对吧？然后现在加上去的最大的比1到2个这个更新升级是什么？",
      "speaker": "发言人6"
    },
    {
      "time": "00:22:15",
      "text": "对我来说可能最开始VLM是continuous fashion和page attention。对我来说因为我肯定有buy，我做科研做的是speculative decoding，就是说减少这个推理的latency，让它变得更快。所以对我来说这是一个非常重要的feature。因为为什么重要？因为其实我们以前做facility科研，都不是非常的真不是production level的story coding。他就是说这个算法很厉害，大家稍微验证了一下。但是没有人在一个production的场景下真的去证明这个技术是可以work的。在VLM我们实现了这样的一个framework，我们也可以验证它确实可以提速，而它可以对这个lencs有比较大的帮助，在有些场景下可以2倍3倍减少，所以我觉得这对我来说是一个很大feature。然后还有一些比如说像卓翰提到的prefix cash或者quantization，特别是quantization就是低精度的这种推理对latency，然后对硬件都有比较大影响。",
      "speaker": "发言人1"
    },
    {
      "time": "00:23:12",
      "text": "是因为其实在如何提高推理效率这件事情上，其实是一个兵家必争之地。然后我们也看到在AI兴起的时候，很多人都觉得大厂因为有更多的compute，因为有更多的资金，更多的人。大家会觉得大厂面对学界会有很多的优势。但咱们作为一个叫做为爱发电这样一个团队，在这样一个重要的领域获得了世界级的performance，以及带来了这么多好用的框架。这里面有没有一些特别的原因，这样是有点像大卫战胜歌利亚，或者至少战士跑赢歌利亚的故事。",
      "speaker": "发言人6"
    },
    {
      "time": "00:23:56",
      "text": "可能可以从战略上来讲一讲。其实之前讲到了关于我们实验室开源的一个传统，这个其实之所以开源也有点路径。另外如果看我们实验室，就是伯克利系统方面之前做的一些项目，从最古早的时候有一个操作系统叫BSD，然后之后有个数据库叫post gress。然后在那之后在硬件方面有risk five，然后在数据处理方面有Spark，在单位方面有ray。这些开源的项目都是一脉相承的，是去做一个大家从学界做出来来的实用的一个项目，让业界大家一起去参与。同时学界也可以去知道新的一些idea，然后去新的框架。",
      "speaker": "发言人2"
    },
    {
      "time": "00:24:42",
      "text": "我们当把VON做大的时候，我们想要做的是不能只让伯克利的博士生用爱发电的去携带。因为我们知道几个点，一个是博士生不可能一天到晚的全职写代码。毕竟大家还是要写论文，喜欢去写论文，去写做做研究的。这就代表着我们要让这个项目有足够多的业界的人参与进来。但他参与进来的原因是什么呢？他能参与进来的原因是他可以通过这项目，我公司A来贡献一个他自己觉得很重要的，对他业务有影响的feature公司B又去做他对他有影响的feature c要去做D又去做，等大家公司都做了一些不同的feature，或者是在这个上面。他们是这些公司毕竟是有全职的那甚至是资深的工程师的时候，他们把所有的feature都加起来的时候，他们就会一下子三个月然后就收益到了。如果他自己只是闭门造车的话，收益到了可能三年的一些成就。",
      "speaker": "发言人2"
    },
    {
      "time": "00:25:50",
      "text": "刚刚三位主要是提到一些在这个开源社区建设上面，为什么我们能开源的这个模式，能够比方说打赢之前的一些其他的这种大厂里面的这种开发模式。然后我想讲一讲这个技术上，大概我也想提两点。第一点就是刚刚宇森提到的，就是说搞大模型需要很多很多的GPU。其实这一点在做大语言模型推理的时候，其实这一点不是特别成立。比方说你想要做一个lama 7B或者lama 13B模型的一个推理，其实只需要一块大概A100GPU你就能够做这一件事情。然后你即使是一个比较大的模型，像18 70B你可能有大概4到8块GPU你就可以做这件事情。所以这个大于模型推理系统这个事情是一个学术界也能够做的比较真的一个领域。",
      "speaker": "发言人4"
    },
    {
      "time": "00:26:35",
      "text": "然后第二点我觉得一个技术上，我这两天的一个感悟是说，就是VM我们做对的一件事情是因为我们是一个开源框架。我们有一个要求就是说我们会把我们的代码质量要求的非常高。我们希望我们的代码就是非常简洁易读，并且模块化非常好。我们做这一点的原因是因为我们是一个开源项目，我们需要吸引更多的contributors，更多的贡献者。如果你的代码是一个一团乱麻，那大家也不知道从哪儿下手，但也就没有人给你贡献了。所以我们必须要把这个代码质量控制的非常高。",
      "speaker": "发言人4"
    },
    {
      "time": "00:27:06",
      "text": "但其实这一点带来的一个好处就是说，我和Simon提到了一大堆大语言模型推理的技术。大家可以感受到就是做一个大模型推理的系统本身是一个比较复杂的一个工程。如果你没有一个好的抽象，好的这种代码的组织的结构的话，你做一个复杂的工程，你做着做着会做崩溃掉，就是没有办法加任何的feature，然后整个系统又非常慢这样一个模式。对我们因为一开始一直有对这个代码质量的坚持，所以使得我们一直能够比较好的不停的integrate这些比较新的一些优化的方法，比较新的一些这种feature。这使得我们也能够一直跑在相对比较前面的一个位置。",
      "speaker": "发言人4"
    },
    {
      "time": "00:27:43",
      "text": "这里我想追问一下，就是因为咱们既然是一个相对来说比较松散的组织，然后咱们很多成员也都是学术界的同学，咱们这个代码质量是怎么样做到这么好的？",
      "speaker": "发言人6"
    },
    {
      "time": "00:27:55",
      "text": "首先代码质量这个事情是一个非常主观的事儿，它是一个没有一个量化标准，就不像是我的速度有多快，就是一个很直接的一个数字。但这个事情我觉得我们一开始可能就会坚持。就这件事情比方说一个随机的一个contributors来了一个pull request，我们会非常认真的做code review，并且甚至很多时候我们会take over这个PR，我们会自己给他contribute在这个PR上面接的是哪些code。然后是在写完code之后，再来去保证这个代码的质量比较高。并且我们会经常做各种各样的refections。",
      "speaker": "发言人4"
    },
    {
      "time": "00:28:31",
      "text": "Ory我觉得一个比较重要的事情是，比方说for quantization，对，就是量模型量化这件事情。其实我们如果你是在一个公司里面，如果你是一个service ener，然后你的领导给了你一个任务，你要做框架，你的想法应该就是OK，我要尽快的不管用各种方式，我先把这个feature给实现出来。我不管我的代码写成什么样，我都要先把这个feature实现出来，然后赶紧上线。对，这样能够满足我的KPI。",
      "speaker": "发言人4"
    },
    {
      "time": "00:28:56",
      "text": "但是在我们这边，我们其实当时伯克利团队想的问题就是就我们要能够要搭一个好的框架。Zon的框架使得让其他比方说发明了一个新的框架zon算法的人，可以比较简单的在VRM里面实现它的这个算法。并且这个算法能够在其他的所有的模型上都可以被很简单的使用起来。然后我们当时做了一系列的reject me来改变我们模型这个实现的方式。使得把这个框nation这个量化算法实现的方法和这个模型的本身的这个definition完全的分离。这样子你只要加一个新的quantitation的方法，就会变得比较的简单，比较clean。同时加模型的人也不需要考虑我的这个模型到底是能要这么框架还是那样子框架。",
      "speaker": "发言人4"
    },
    {
      "time": "00:29:39",
      "text": "对，这个只是一个例子。我们也在其他方面我们也做了很多这样子的这种abstraction。比方说对于不同硬件，我们也做了硬件的object不同的甚至分布式的orchestration的方式。这个最近凯超也在做，这在这个方面很多的一个refectory。所以我觉得这个是我们保证这个代码质量的一个比较重要的原因。",
      "speaker": "发言人4"
    },
    {
      "time": "00:30:00",
      "text": "再加一点就是很多在伯克利读系统的博士生，大家其实都在业界工作过。然后同时在业界的好的系统，大家都去读了一些比较高质量的代码，也知道好高质量代码是长什么样的。所以说到最后其实更多偏向于变成个人和团队的选择，而不是一个能力的问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:30:23",
      "text": "是的，我觉得刚才其实你们讲到的都还是我们其实作为一个开源组织，作为一个VI发电的组织，因为没有明确的KPI或者release的压力，反而能做到一些在大厂里面不太容易实现的目标。或者说大厂里面可能因为他比较考验一个KPI，所以可能反而他做不了像咱们这么好，对吧？然后我正好也想追问Simon，因为Simon是any scale的finding engineer。在也是any scale，其实也是从伯克利实验室以开源项目成长起来的一个独角兽公司，对吧？然后你在这个过程中对于开源和公司其实是有很多的了解和理解的。当然VRM刚才也提到，其实一开始就是从一个开源项目学术研究开源项目的方向去的。能不能再展开讲讲伯克利开源的这样一个传统，以及开源怎么样能够带动业界和学界一起探索。",
      "speaker": "发言人6"
    },
    {
      "time": "00:31:17",
      "text": "伯克利的开源传统很多，简单的来讲就是让学界最新的一些idea去跟业界的生产环境和具体的USK去碰撞。同时也让学界的一些特别是博士生的各种探索性的性格，去与业界比较谨慎的工程师的一些人去碰撞，然后去做出一些新东西来。比如说我们之前做瑞的时候，如果作为一个博士生去写锐，很多东西会选择给他做的更快、更有意思、更好的抽象，或者总的来说比较酷、比较厉害的东西，而业界就会偏好更实用的东西。在这个上面大家其实会双方都在互相的学习，然后就会学会让。最后做出来的开源项目到了一个非常好的一个点。这样的话又实用，但是又带领了其他的竞品在接着提升，然后又会有新的idea。比如说当时做瑞的时候，我们说了一个叫action的一个obstruction。然后在这个上面之后，几乎所有的框架都加入了相关的一个abstraction。",
      "speaker": "发言人2"
    },
    {
      "time": "00:32:35",
      "text": "接下来其实还想聊一聊，就是在这两年的时间里面，VM发展有哪些关键的节点，咱们在这个团队的思路上有些什么样的转变，比如说我们最开始的时候，其实从当时SOPT175B对吧？然后早期用户跟我们合作有哪些的这样一个需求和反馈。然后后面我们在整个发展过程中有没有遇到什么样的困难，这个要佐汉给我们分享一下。",
      "speaker": "发言人6"
    },
    {
      "time": "00:33:01",
      "text": "最一开始的时候，我觉得我们的唯一追求就是快，一个字就是快。然后比方特别是facebook OPT那个demo的时候，这个实在是太慢了。就是大概有三个人一起用的时候，他就已经卡的不行，然后就排队十分钟，大概是这么一个状态。所以这个就是不可接受的，所以我们对一开始的需求就是快。其实刚刚有一点没有提到，就是大概从有page attention paper paper我们投出来在之后，然后在我们开园之前不是大概有两个月的时间，其实这两个月我们也不光是在整理代码，我们其实也在我们实验室的另外一个项目，就是这个chat for arena这个项目以及项目上面落地。",
      "speaker": "发言人4"
    },
    {
      "time": "00:33:40",
      "text": "就是因为当时他们要about需要评价很多不同的LM，那他们就需要自己设很多的LM他们一开始是用一个比较简单的hugg face的这个framework，也就是我们的base要来来serve，随着他们的使用量不断上升，他们发现他们的就使用hunting face是完全不能够support他们的流量。所以因此他们也就是在很早就到和这个VRM这个框架，在我们还没有开源之前做到。在和他们合作的过程中，我们也修了很多的这个bug。然后也使得我们在一开始release的时候，就相当于其实已经release之前就已经有一个比较好的user，比较有硬盘的一个user在的。我觉得即使是从最一开始的OPD demo到后来，就比如南山柏瑞娜，我们一开始我们的目标可能就还是只有一个字快。但是在开源release之后，我们有很多的不同的go。比方说第一个是我们要支持很多的模型，就因为当时是各种各样模型每天都在出来。然后首先我们要改造我们系统，加一个比较新的模型，变得这件事情非常容易。",
      "speaker": "发言人4"
    },
    {
      "time": "00:34:47",
      "text": "其次就是对于一些比较重要的模型，我们可能也会第一天或者我们自己扑上去把这个模型的support加入进来。这个是当时第一个比较重要的事情。第二个是一个更general的，各种这样的优化方法。我们要怎么样integrate这些优化方法，并且我们怎么样能够有一个Better system architecture，使得其他的contributors能够比较容易的加入一些这种新的各种各样的这些优化。然后我们可能在开源之后，我觉得我们一个比较重要的一个变化就是从这个纯performance focus到更多的像和开源社区，以及和这种开源贡献者为他们服务的这么一个过程。",
      "speaker": "发言人4"
    },
    {
      "time": "00:35:27",
      "text": "正好我想问一下，因为有其他的推理加速引擎其实也有不少，对吧？比如tensor RT，像这个哈根face自己也有对吧？那么在这个过程中我们支持的模型非常的多，并且之前我们也聊到有很多主流的开源模型。比如说像千问，或者像妈妈mr这些。在他们发布模型之前，其实都会来主动找我们来进行这样一个适配。然后这种紧密的合作关系是咱们是怎么建立起来的？刚才你正好也提到了，我们的这个知识是非常广泛的，能不能在这个再展开讲一讲？",
      "speaker": "发言人6"
    },
    {
      "time": "00:36:04",
      "text": "我觉得是一个比较有机比较自然的一个过程。最大的原因还是因为我们的框架比较好用，然后用户用了之后确实能够感觉到速度的提升。所以我们的这个开源用户的使用量上来了。在这之后就是当一个开源模型发布的时候，他们也希望自己的这个模型能够有最大的impact，能够让更多的人用上。如果大家都在使用VAM来serve模型的话，那大家会想接着用VAM去serve他们新发布的模型。因此把他们新发布的模型这个应急无人机VM也成了一个他们比较高优先级的一个事项。所以我觉得这个也是一个比较有机的过程。也我们也因此也和很多开源模型开发的团队建立了比较多的联系。",
      "speaker": "发言人4"
    },
    {
      "time": "00:36:47",
      "text": "这边有什么比如说你比较有意思的故事可以分享的吗？就比如说跟这些模型厂商一起来进行微调，或者说上线前的准备，然后做了比较特别的事情。",
      "speaker": "发言人6"
    },
    {
      "time": "00:36:58",
      "text": "可以讲一下master。对对对，我觉得mr o是一个比较神奇的事情。Mr o其实他们第一个模型发布的时候就找到我们了。当时他们还是一个名不见经传的欧洲公司。我一看到他的一个点就是他们融了很多钱，然后就verge，然后merge之后我们就看着他们一路起飞，对，然后一直到了今天。下面你要不。",
      "speaker": "发言人4"
    },
    {
      "time": "00:37:18",
      "text": "讲讲今天他们会有一些比较有意思的神奇的操作。比如说在一个周五的时候，大家都比较放松了，准备过周末的时候，突然在他们的twitter x上面发布一个PTP总结。对，然后那个里面就只有模型的weight和一些最简单的一些conflict private就是你不能直接跑起来的一些东西，然后他就让大家去猜，然后大家去逆向，然后大家去玩儿。但在这个时候，其实maso团队已经给我们发了一个P12。但是他当然是邮件里面发的，完全没有任何公开的信息。特别也像就像是给一个媒体团队说有一个emerg一样说，这个东西你现在不能告诉大家，等我们announce了，你才能正式的加到VR里面去。但是我们想先给你看一下，然后这样子的话，等我们正式向世界的发布之后，你就可以直接拥有第一时间的支持了。所以说还挺有意思的，比如说我们周末的话就会说看着大家都在猜这个模型，然后自己我们已经知道这个模型代码也知道是什么了，但是哪都不能说。这个在第一次的时候还挺好玩的，可以看一下，那就是大模型厂商做的这些有意思的公关上的一些宣传。",
      "speaker": "发言人2"
    },
    {
      "time": "00:38:40",
      "text": "对我感觉现在大模型发布前的这种悬念其实是越来越拉满，对吧？大家都还很期待。不管是像OpenAI这样的闭源模型，还是像拉玛或者像前文deep sik这样的开源模型，他们接下来要做什么，以及他们会有什么样新的特点，就好像我们会拿到新的玩具一样。说到这个其实不是千问，最近都发了VR，其实在开源界反响都很好。我不知道咱们在比如说跟他们合作的时候，有没有什么有意思的故事，或者说观察到他们为什么能够成长的这么快的一些。",
      "speaker": "发言人6"
    },
    {
      "time": "00:39:14",
      "text": "来观察这个说实话好像我们没有特别多的信息，反而确实是有很正面的一些合作的一些体验。",
      "speaker": "发言人2"
    },
    {
      "time": "00:39:24",
      "text": "我觉得有一点就是一个模型比较火了之后，就是关于这个模型的医术就会疯狂的增加。很多人问这个一表述VM跑不了千问了，那就说明这段时间这个千问比较火。如果这一段时间都是VM跑不了deep sick，对，那就说明这一段时间deep sik比较火。对，而且他们跑不起来原因可能就是因为他们自己的环境有些问题。",
      "speaker": "发言人4"
    },
    {
      "time": "00:39:43",
      "text": "不一定是VM对对对，或者说这些模型挺慢的这也是我们另外知道模型popularity的一个方式。比如说之前可能去年的一段时间是charge ON那个那个模型，当时在VIN的支持不是很好，但是我们一天到晚都能得到用户在问，然后一直在那些医学上面的评论说我想来跑这个模型，我想跑这个模型，然后之后我们会拿到这个信号，然后尽快的去做适配。",
      "speaker": "发言人2"
    },
    {
      "time": "00:40:13",
      "text": "如果我没记错的话，当时可能有大概五个不同的PR来support这个ChatGLM。",
      "speaker": "发言人4"
    },
    {
      "time": "00:40:20",
      "text": "是我觉得这也是开源的好处。开源是一个海纳百川的过程，大家都可以在这里面互相的进步了，并且也能够有更多的信息的分享。因为咱们今天也请到了VR比较新的几位成员，李利和凯超。要不我想因为刚才凯超还没有聊，就是想请凯超分享一下在加入LMVLM以来遇到过哪些印象深刻的事情，尤其是说在技术上有遇到什么样的难点吗？",
      "speaker": "发言人6"
    },
    {
      "time": "00:40:49",
      "text": "我觉得做VOM最不同的一点，就是最特殊的一点在于我们在做的是一个真正很多人在用的系统。作为一个系统，如果有很多用户，那么他最大的困难就在于能出错的地方都可以出错。比如说硬件可以有问题，这个驱动可以有问题，然后软件环境可以有问题，这些你都需要照顾到。但是如果你是做一个纯粹的research的话，比如说你要发一篇论文，你可以有很多的assumption。比如说你可以说我只在H100上面测试，然后我只在这几个条件下测试。这几个条件下我有效，那么我就可以写论文说我在这我可以说我我说我在这些条件下测出来有效，而且审核人员会说，你说明了你的条件就可以了。那我们如果要做一个系统大家都能用，你不能说我只能在这些环境下用，其他环境下我就不支持了。所以我觉得这是最困难的一点，就是你需要有全面的支持。",
      "speaker": "发言人3"
    },
    {
      "time": "00:41:49",
      "text": "然后就像我们之前。这个调试了很久的一个问题，最后发现是英伟达的驱动有问题，然后英伟达的软件有问题。这些东西你如果是在做research的时候，你说好他有问题，我们就跳过它就行了，我们换一个版本就行了。但是如果我们要做一个系统，我们需要考虑说用户装的就是这个版本，那我们没有办法改变它，我们要怎么办？我觉得这些是我们遇到的最困难的问题。然后我觉得在克服这些困难的过程中，个人也学习到很多，这是real world的problem。",
      "speaker": "发言人3"
    },
    {
      "time": "00:42:29",
      "text": "我觉得你说的这一点很重要，因为很多时候研究只是针对某一种情况。但是咱们作为开源，又要在现在support这么多的模型，就得处理很多的corner case，对吧？咱们现在其实是支持，我看有三十多个各种各样的LM觉得未来会怎么样发展？因为要把这么多模型都支持好，我也非常的不容易对吧？有各种各样的corner case，觉得未来我们会重点支持几个模型，还是说还会保持这样一个比较大的开放的方式呢？",
      "speaker": "发言人6"
    },
    {
      "time": "00:42:58",
      "text": "目前其实他们的架构还没有那么的diverse。当然我们现在也有一些还没有支持的比较好的。比如说那个embedding的model，还有encode decode的model。再往后可能如果大家的那个模型再多样一点，我就比如说我们最近正在讨论说有一些state face的model，它就没有那么复杂的显存管理了。然后还有一些混合形式的，又有attention的，又有局部的这个locality的，这个时候可能会更困难一点。就是我们看这个模型支持的复杂程度的时候，我们不能只看它的有多少种，有多少个名字。而更多要看这个名字背后的模型，它有哪些创新的结构，是这个结构是最难支持的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:43:51",
      "text": "你们自己有没有对于接下来什么样的结构会变成主流，对未来的发展有没有一些你们的看法？因为现在其实大家也都在寻找在模型结构上的创新。",
      "speaker": "发言人6"
    },
    {
      "time": "00:44:05",
      "text": "我可以简单说一两句，后面那个3M和卓翰肯定有更多。我觉得一点就是大家正在追求efficiency，就是说我的感觉是模型的能力的天花板，感觉大家都差不多快到那儿了。你看top tier的那些model，可能觉得基本上也都够大家用了。这个时候大家就开始降低cost。所以就是说你在保持大概同样的model里的时候，怎么让它推理的更快。比如说之前deep sick的那个later的attention。类似这样的，我觉得可能接下来这样的工作会更多，就是怎么去探索它的efficiency的极限。",
      "speaker": "发言人3"
    },
    {
      "time": "00:44:47",
      "text": "从我的角度来看，很多是场景驱动的大模型在语言生成或者作为一个聊天机器人上面可能架构没有太大的变化。但是大家逐渐的把它拿去用长文本、长文本，它就会需要一个比较不同的、比较稀疏的，或者比较做一些其他本地attention机制的这些东西，或者是多模态。当你加入了图片生成和图片理解，加入了视频相关的，加入了语音相关的，那么你的模型的架构更好的去适配他们的话，可能还是有一些变化。当然我个人觉得的话，从中间基础的attention机制，可能还是不会有太大的变化。而大家会因为不同的使用场景，不同的模型的variant去改它。",
      "speaker": "发言人2"
    },
    {
      "time": "00:45:41",
      "text": "之前我们看到的一个比较大的位置是MOE mixture of experts。这个是一个凯超哥三个没有提到，但是我们过去经历过的一个比较重要的一个变化。",
      "speaker": "发言人4"
    },
    {
      "time": "00:45:52",
      "text": "是的，MOE其实现在大行逐渐成为开源模型都在做的一个技术方向。我们在MMVIM的发展过程中，其实资源因为有限，所以要把它做得这么好，肯定是有很多取舍的。那么有什么是我们选择不去做的事情，想请Simon聊一聊。",
      "speaker": "发言人6"
    },
    {
      "time": "00:46:14",
      "text": "好的，其实有还有不少取舍，我想到了可能有三个不同的点。一个比如说我们选择了不去做公司，大家说你做了一个成熟的开源项目，你第一步就是去做公司，然后去让这个项目更好的成长起来。我们选择了不去做商业化项目，而希望大家都能在这上面进行商业化，去做一个可以加速所有公司的一个这么一个开源项目，我们觉得这个其实可能损失了一些盈利的空间，但是加速了整个AI或者广东来说AGI的一个历史的进程，我们觉得还是很有意义的。第二个是技术上的，我们选择了不去追求极致的性能，有很多竞品的项目，比如说tensor TLM或者是TGI，他们会选择用C加加或者rust去写重写很大一部分的系统。而我们发现用这些系统去写，同时也代表着会让这开源项目更难的被人参与。尽管用C加加或者rush去重写一些部分会让你的性能提升到极致。但是我们会发现就用拍照拍torch或者和一些非常简单的一些C加加的代码，可以让大家更好去使用它，去改变它，或者去提交PR去contribute去贡献它。这样子的话，就算没有最好极致的性能，我们也足够好，也到那儿了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:47:48",
      "text": "然后当然还有一个很重要的选择是我们不执着于NVIDIA英伟达的显卡。因为尽管说大家现在都在用，但是我们会愿意去把我们很多关注的时间，甚至团队中很多不少小伙伴的时间，都会放在去了解其他的硬件。我们会觉得这是未来必有的趋势和方向，然后会去用这些本来可以在英伟达GPU上面优化的一些时间，去试着去看别的硬件，去发展AI在那些其他硬件上面的一些功能和性能。",
      "speaker": "发言人2"
    },
    {
      "time": "00:48:31",
      "text": "对我其实想补充一点，就是我觉得我们还是追求极致的性能。对，只是还是有的是还是有的。对对对，只是我们可能追求方式不会像是这种，就不会把这个放在绝对的这个top priority。我们会考虑一些比较好的这种能够让代码比较简洁，代码比较clean，然后让大家更容易参与的方式。然后一步一步的慢慢的走，最终走到这么一个极高性能的一个目标。但是我不会想的就是这周有个人比我们快了，那我们下一周就要花这个整周的时间一阵乱嗨，把这个performance给提上去。所以我觉得这个应该是我们不太会选择的一条路。",
      "speaker": "发言人4"
    },
    {
      "time": "00:49:09",
      "text": "对我我可以补充一下，就是我觉得易用性是我们非常重视的一点，就是我觉得tape performance更重要。如果你有一种方式可以提速10%，但是你的代码谁都看不懂，为什么这样能起诉我们不会接受他。就我们经常隔段时间举办一些meeting up，和一些开发者和用户交流，然后经常就会有很多NLP的researcher，他们就是只做ML algorithm的人。他们就会来跟我们说，我们之前只会用hugin face的transformer。然后后来有个同事告诉我们可以用VLM，我们只要换几行代码。这个推理的时间从几个小时变成了一分钟。然后他们就会说你们拯救了我们这个发paper的速度。对于这些人来说，易用性是第一位的。VOM的话易用性这方面就只要会拍摄的人应该都会用。",
      "speaker": "发言人3"
    },
    {
      "time": "00:50:03",
      "text": "所以听上去的话，在开源领域把这个盘子做大是一个非常核心的事情，在这上面我们要做很多取舍。丽丽你说我只想。",
      "speaker": "发言人6"
    },
    {
      "time": "00:50:12",
      "text": "接着凯超的补充一下，除了易用性，还有就是易开发性。包括之前BLM里面也会有一些讨论，比如说要不要加一些功能，就像三木说的，就是可能可以让普通摸的好一点，或者有一些新功能，但是它会让这个代码很难开发。然后我们希望这个community还是有更多的人来contribute到这个code里面，就模块化做得更好一点，代码更干净一点。所以有一些比较复杂的feature，我们最后还是没有把它加进去。这就是易开发型。不仅仅是易用，也有也吸引更多的contributor。",
      "speaker": "发言人1"
    },
    {
      "time": "00:50:51",
      "text": "其实说到contribute，我们发现就是很多VM的用户都成为了contribute。能不能帮我们梳理一下，就是现在VM主要的country都有哪些？然后他们也一起为爱发电，动力来自于什么地方？然后比如说刚才说到咱们的密探这些活动中日常的合作中有哪些有趣的故事呢？",
      "speaker": "发言人6"
    },
    {
      "time": "00:51:11",
      "text": "好的，那我大概起个头，就是VN的贡献在v one生态系统里面，用户肯定远远大过贡献的人。而贡献者也分只是一次性的，大家只是偶尔加一点东西，或者是一些长期性的和他会有一个说我公司愿意用这个团队去做这的项目，其中这些包括比如说有真正在用VN在生产环境里面的使用者，特别是云厂商。比如说AWS oracle IBM，他们就会有固定的团队在VOM上面写代码，也有模型厂商，比如说mr o和千问，他们都会有团队在比如说在模型要发布的时候，很多时间去在VON上面简单的改一改，然后支持适配他们的新模型，或者在内部使用的时候就在使用VOM，然后去进行一些修改和开发。然后他们也会把这些contribute到VII里面去，让更多人能使用他们的模型。",
      "speaker": "发言人2"
    },
    {
      "time": "00:52:18",
      "text": "当然也有硬件厂商，这特别比如说AND intel，google和adb他们各自有各自的比肩英伟达的一些硬件。然后他们都会有独立的团队去做这样的VM的适配和调优的工作。然后也有在产品上面把AI放在第一位的一些公司，比如说linkin或者robots。",
      "speaker": "发言人2"
    },
    {
      "time": "00:52:43",
      "text": "我说了一个社交网络，说了一个游戏公司，他们都会长期的去使用VON，并在这上面有一两个工程师在上面进行开源的开发。这样能让他们一个是对项目未来发展有更好好的话语权。同时也让他们真正的知道有这个专家的in house expert，也就是专家知道VON的人，来帮助他们在生产环境里面，产品里面能好好的用上。",
      "speaker": "发言人2"
    },
    {
      "time": "00:53:14",
      "text": "当然也包括新一代的AI info公司，他们有自己的AI相关的info的产品，但是各自有各自的focus。然后他们都会用VOM作为底层项目。这就包括了公司any skill或者是neutral magic，他们都会基于VOM进行一个二次开发和包装，然后进行一个整套的产品的售卖和支持的一个服务。当然还有很多很多只知道给hub用户名的贡献者，其中不少是学生，也可能只是对LM感兴趣的一些工程师。然后这些我们都非常感谢他们的参与。",
      "speaker": "发言人2"
    },
    {
      "time": "00:53:53",
      "text": "然后我这边有一个问题是因为之前有一个inside，是现在的VRM的contributor，其实已经达到了一个非常大的比例。然后VOM从去年开始到现在时间是非常短的。相比起其他的历史上比较成功的开源项目来说，其实在这么短的时间内，咱们项目是怎么样加速去吸引这么多外部贡献者的，这个场面上有没有什么心得可以分享？",
      "speaker": "发言人5"
    },
    {
      "time": "00:54:21",
      "text": "可能其实很大一部分是运气，但是同样一部分也是去努力的维持一个开源的社区和生态系统。运气方面主要是一个是因为准备的足够好啊，项目本身的质量也好，大家都喜欢用。然后用能直接就可能几分钟就能用上，也没有什么很大的问题的时候，他们就会接着用下去，会告诉别人去用它。另外一个运气就在于刚好那段时间BON在市场方面会显得是一个非常新，但同时质量很好，然后很快就成为了一个大家的第一选择的一个项目。",
      "speaker": "发言人2"
    },
    {
      "time": "00:55:06",
      "text": "当然有了这么一个在市场上的一个我们所谓的leading position的时候，就更需要去关注是谁在使用这个东西，然后怎么去做大你的contribution的一些circle。然后做大contribution的人这个其实没有什么诀窍，就是一个一个的来，就先确保一个公司能稳定的在VN上面进行输出。然后在寻找第二个公司、第三个公司、第四个公司。这中间其实有很多。就比如说在你开源的生态里面，给你所有每个题一是和P2或者是跟你聊天的人，都是非常的有意思。然后你都要去问，都可以值得去问他们，去了解他们，跟他建立connection，去知道他们的背景，他们对为什么会对这个项目感兴趣，他们的工作是什么？他们工作上具体为什么会用到VOM或者这样的东西。去知道了他们是谁之后，就会有很好的建立这样子的stable contribution connection的一个方式。",
      "speaker": "发言人2"
    },
    {
      "time": "00:56:17",
      "text": "之前是不是VOM举办过很多meet up。然后线下就对于一个这样有活力的社区来说，有没有什么有趣的用户或者有意思的人有这样的故事可以分享吗？",
      "speaker": "发言人5"
    },
    {
      "time": "00:56:29",
      "text": "线下有很多，线下的面更像是一个刚好大家可以线下一起答疑，然后去聊天，然后面对面见面的这么一个过程。其实我们目前为止线下的活动一共只办了四次，每次只有一到2个小时这么一个过程。有意思的几个点，比如说我们会见到一些人只是通过网名去称呼彼此，有点像网友见面的感觉。",
      "speaker": "发言人2"
    },
    {
      "time": "00:56:56",
      "text": "社区里面也会有很神奇的，就是有点像那种无名英雄一样。比如说有一位在英国的一个工程师，他会在他白天没有什么事儿干的时候，就帮我们分析很旧的一些issue。然后去看这些report，还是不是Violet看这些用户的feature request是不是已经implement，或者是已经不需要了。然后他就会把这些意识进行triage，把它close掉，或者是去问我们这还要不要做。这很有意思，因为他在英国，然后每次我们早上起来就会看到，比如说他今天又close了二三十个issue，然后第二天又close二三十个issue。一个月之后他可能close了接近4分之1的医学，这些都不再重要了。然后这对我们开源社区其实有非常大的帮助。",
      "speaker": "发言人2"
    },
    {
      "time": "00:57:48",
      "text": "而且最关键是这些东西需要一个人来做。你目前来说可能很难选一个AI去进行一个复现。然后再加上一个去对项目的了解，或者去读代码，去看这些判断这些需不需要close or not。所以说是这种社区里面有很多这样子的，就是一直在默默贡献的一些人。",
      "speaker": "发言人2"
    },
    {
      "time": "00:58:09",
      "text": "很多网络姻缘。",
      "speaker": "发言人5"
    },
    {
      "time": "00:58:12",
      "text": "关于meet up的有一个趣事可以分享。就是我们每次去meet up，每次举办make up都能听到一些新的公司用了VOM。你比如说上次我们就听到那个adobe的人就跟我们说，他们adobe的产品后面用的是VOM在支持。",
      "speaker": "发言人3"
    },
    {
      "time": "00:58:32",
      "text": "其实我们因为我们也proser了另外一个开源项目叫CTRL net，其实也是在湾区对吧，也是华人团队做的。其实挺有意思的就是control net它比较像是吕敏他自己的一个天才的疯狂输出，对吧？就是基本上都是等着这个赛博菩萨发布新的的功能和版本，然后VRM是一个非常diverse，contribute非常多的一个开源项目，我觉得这个对比还挺有意思的。因为Simon也参与过很多的这个开源项目，如果现在因为也有挺多AI领域的团队，我发现他们其实是想从开源项目的角度去切入做一些事情。现在假设有一个全新的开源项目团队，咱们能够给他们什么样的经验进行分享吧。",
      "speaker": "发言人6"
    },
    {
      "time": "00:59:24",
      "text": "我觉得这上面有非常重要的一点，就是你做开源是为了什么，特别是如果商业团队做开源，更要想清楚开源的目的。比如说是为了marketing sales还是recruiting，还是为了产品去寻找PMF。如果只是学生做开源的话，就要想清楚你有多少时间花在这上面，然后你愿意接受多少的一些contribution。",
      "speaker": "发言人2"
    },
    {
      "time": "00:59:49",
      "text": "其中很重要的一个节点，对于所有开源项目来说，就是去接受用户，就是去接受给你提需求或者给你提bug report的人。去了解他们。然后去问他们你为什么需要这些东西，我能不能帮助你来做什么样的东西。或者是给你提P2的人，可能他们一开始给你提代码的贡献不是很好，质量也不是很高。好。但是特别是作为有能力做出一个很有意思开源项目的人，你怎么去抑制一种自己，就是说我行我上，我去把它改的更好，我能做。而是去让社区慢慢的成长起来，去给予他耐心，然后让大家的贡献一个一个搭起来。",
      "speaker": "发言人2"
    },
    {
      "time": "01:00:35",
      "text": "比如说在v one里面，我们提到的很多需求，其实都有一种就是我们其实花点时间就能做。为什么这些人要用一周、两周、三周的时间慢慢做？你给我一个周末我也能做完的感觉，但就是要去抑制这样的一个想去做这样的事情。而是让社区真正的成长起来，让了解你的产品，了解你的开源项目的人越来越多。然后才能啊真正的把所谓的把盘子做大，然后让大家用这个的人做的更多。",
      "speaker": "发言人2"
    },
    {
      "time": "01:01:06",
      "text": "然后包括一开始的团队的搭建组成，这个有没有什么经验可以分享的？因为咱们一开始也是一个非常精干，但是又非常互补的团队。",
      "speaker": "发言人6"
    },
    {
      "time": "01:01:17",
      "text": "最一开始可能是就做研究的时候，主要写代码是我和乌苏。乌苏是我们另外一个VM另外一个co creator，他是一个韩国小哥，我觉得他是一个比较好的这种kernel writer，他们会写比较好的这种叫low level的。然后我之前上一个项目是l pm我刚提到他是做这个dirigo ted model parao training和influence的一个framework for work。做更多的这种像是分布式软件，各种这种通信的这一方面的技术。所以我们两个人还是在技术上角度来说还是比较互补的。就是我可能做一些更加高层的一些这种scheduling和这种managment的事情，然后无所不能，会写写科技之类的事情。但是我们俩看着就都比较tag，就非常的是比较lady的感觉。",
      "speaker": "发言人4"
    },
    {
      "time": "01:02:01",
      "text": "但是Simon因为之前在这个any scale做过PM对，这是有个非常好的产品的经验。所以Simon后来来到我们的项目，也是帮助我们项目让VM成长的好了非常多的，让我们这个项目管理井井有条，然后让能够让更多contributors进来。也包括后来更多的像是比方说lily在这个项目当中贡献了这个作为一个researcher的角度，能够给你提出很多意见，并且也加入一个新的feature。凯涛也是一个最新加入的一个contributors，然后也是一个给我们带来很多新的vision，并且也希望很多很多的log也帮助我们，他们非常多。",
      "speaker": "发言人4"
    },
    {
      "time": "01:02:39",
      "text": "其实我想问一下，在刚开始就时间线往回拉一点，就是有没有一个瞬间让你们感觉到VOM，他找到我自己作为一个产品的PMF，然后这个PMF是。",
      "speaker": "发言人5"
    },
    {
      "time": "01:02:51",
      "text": "怎么样找到的？我用一句话来回答，就是当issue和P2都开始爆炸性的指数级增长的时候，然而我们已经没有精力去加更多的feature，然后来去去适应大家的需求的时候，你就知道大概有个PMF在哪了。当然最好不要是bug report，而是大家真的想要这个产品用更好的那些feature request。这也是非常经典的一个PMF的定义，就是客户和用户想要的东西一直想要更多。而你作为产品的提供方，很难keep up，这说不定就是个PMF。",
      "speaker": "发言人2"
    },
    {
      "time": "01:03:33",
      "text": "下一个问题就是想请教一下丽丽，因为刚刚丽丽提到其实在VRM团队会负责一些比较前沿的学术研究这一块儿。就是想请问一下丽丽，像在比如说推理加速引擎这个领域，现在有哪些前沿的学术研究？开源VRM的项目和这些学术研究怎么样形成一个比较互补的状态？",
      "speaker": "发言人5"
    },
    {
      "time": "01:03:55",
      "text": "我们现在做的比较多的关于推理的加速，主要是还是针对让他更快降latency。然后如果是降latency的话，就有好几个比较大的topic。第一个topic就是speculative decoding，就是说我们通过用一个小模型来近似生成一些结果，然后让大模型验证。这样的话我们可以降低一些跑这个大模型的开销，这个是一个大方向。然后还有第二个大方向，也是可以降latency，并且它可能也可以省一些计算资源。第二个就是做那种低精度的推理，比如说我们最近也一直在推那个FP8的。然后第三个可能也有在做的，就是关于这个KV cash的一些优化。比如说如果是多节点KV开始，我们这个KP开始应该怎么存？",
      "speaker": "发言人1"
    },
    {
      "time": "01:04:45",
      "text": "然后我们应该是放在一个机器上，两个机器上也有一些这方面的研究，这个是目前在进行的，我们是怎么让它和这个VRM，就怎么把这个科研和实际这个VRM结合呢？首先我们会把最基础的这个feature先做到这个BLM里面。比如说我们会先支持一个比较general speculative decoding the framework。我们做这个framework，我们的目标就是它非常的general，非常好用。这样的话，不同的researcher他们可以在里面实现自己的这个algorithm。因为就算是speciality code也有很多不同的类型。对所以这个framework很general，那大家都可以自己在上面build自己想要的这个算法。所以现在其实关于party构建，其实community也有非常多的这个contributor，有有做research的，也有这个公司，他们就想试一试这个algorithm都有。然后我们这边做的可能就是我们会比较active的去review那些PR，保证这些PR和我们这个framework是compatible的。",
      "speaker": "发言人1"
    },
    {
      "time": "01:05:43",
      "text": "我觉得在BLM做人设时间它有很多好处。第一个好处，它可能是说你的科研是真的可以落地的。比如说之前我们做了一个facility coding，就是IBM他们有一个coding的这个算法，他们想做进来，然后他们坐下来那个PR也没了，但是我们还没有好好的测，我们就稍微测了一下，我们我们准备再多测一测，然后再announce。在我们还没有准备好announce的时候，就已经有人给我们报bug。就说好像你们这个在有些情况下不太work。所以我发现作为一个researcher，你自己的work能被别人看到，或者说有人非常active的follow你这个work，我觉得对我们是一个很强的正反馈。所以我觉得这个会给research很多的激励。",
      "speaker": "发言人1"
    },
    {
      "time": "01:06:26",
      "text": "然后还有一个好处，我觉得是说你会可以看到很多新的research问题。就包括我们其实specially coding，或者说KP cat transfer这种，我们有很多follow up的科研。这些科研其实都是我们把VLM实现了，然后我们真的把这个feature也实现了。跑起来我们发现好像这个feature不是很完美，这个feature没有他claim那么好，他有很多的limitation。然后在这种情况下，我们在做一些follow up的工作，可以把这个feature真的可以做好，然后做到它真的可以用。",
      "speaker": "发言人1"
    },
    {
      "time": "01:06:56",
      "text": "所以说我觉得这就是另一个好处。就是说当你真的把这个东西实现了，变成一个production ready的一个东西的时候，你会发现现很多新的问题。为了解决这些新的问题，你可以做更多的科研。所以这是一个正向的迭代的过程。",
      "speaker": "发言人1"
    },
    {
      "time": "01:07:10",
      "text": "真的感觉在VM做科研也是一个很很有趣的事情。还想请问一下凯超和lily，就是像现在VRM下一步的工作重点会放在哪个部分呢？",
      "speaker": "发言人5"
    },
    {
      "time": "01:07:22",
      "text": "我觉得第一个是也是VRM的一个重点，就是说VRM它会有更多的硬件的支持，我觉得这个也是VRM和其他framework最不一样的。比如说像TCRT，他们可能只支持英伟达的卡。像VLM他从一开始创立的初衷就是说他要有啊不同的卡都有比较好的支持。就不仅是支持，而且我们也希望那些硬件厂商，包括我们可能自己也会花一点点时间去优化它的这个性能。",
      "speaker": "发言人1"
    },
    {
      "time": "01:07:52",
      "text": "因为我们相信不同的卡，他们这个成本差异还是比较大的。所以我们也希望这个开源界大家来用我们的framework，可以挑一挑，就是选一个对他们来说，他们对经济适用？性能又能接受，然后又最便宜的这个卡。为了实现这个功能，所以VRM会继续支持不同的卡，不同的哈维。然后我们也会和这个硬件厂商非常紧密的合作，这是第一个比较大的主题。",
      "speaker": "发言人1"
    },
    {
      "time": "01:08:19",
      "text": "然后第二个是也就是VRM它一直有的优势就是性能比较好，performance比较好。我们会继续做我们的performance optimization，无论是在科研方面还是在engineering方面，就是说科研方面我们就有很多research开发新的算法。Engineering方面我因为我们有比较好的community，所以有很多这种非常资深的，非常专业的engineer来帮我们继续做一些performance上的优化。所以说我们希望这个用户可以相信VM的performance是competitive，并且是strong的。是跟其他的framework是不相上下的至少。",
      "speaker": "发言人1"
    },
    {
      "time": "01:08:55",
      "text": "然后第三个也是刚才一直聊的，就是说我们希望VLM它的这个架构是非常的extensible的。就是说用户是很好改的，contributor是很容易在里面加一些新的东西的，将来发展的特别快。你如果看他每天录制的PR数，提交的PR数其实非常多。所以说我们一直致力于有一个非常好的abstraction。这样的话我们可以支持比如说更多的这个scheduling policy，不同的backhand或者不同的optimization。所以说我们会make sure这个code base是干净的，并且是不是一条通用的比较易读的。海超，你有什么补充的吗？",
      "speaker": "发言人1"
    },
    {
      "time": "01:09:32",
      "text": "我觉得我们之后要做的还包括一些更广泛的model的support。因为这波AI的浪潮一个标志是ChatGPT，当然它是纯文本的。到后面就像现在GPT FO它有image的input，还有audio的input，他甚至还有audio的output，这些可能都是未来大模型发展的一个方向。我们除了简单的文字输入输出之外，我们还要处理更多的其他的复杂的模态。",
      "speaker": "发言人3"
    },
    {
      "time": "01:10:07",
      "text": "我们要变得更production level。就是因为我们之前作为research的一个paper，它可能是一个算法的idea。那么到后面要做production level的时候，要能够skill，就是把它的量提上去。比如说一个公司用我们的代码服务成千上万的用户，他要一天24小时连续不断的跑，而且不能出问题。这个时候你需要对他做一些production level的一些处理。比如说日志容错，什么auto scale，这些都很重要。",
      "speaker": "发言人3"
    },
    {
      "time": "01:10:43",
      "text": "我觉得最后还有一点就是说我们是一个open source project。它不只是代码公开，就是open force。我觉得我们有很多open source的best practice，包括我之前也参加过一些像拍拖去之类的，我们觉得比较好的open source的project。他们的一些open source的maintenance的一些一些scheme，我们都是在follow。比如说我们有重大的architecture change的时候，我们都会有一个rfc，那么就是快for comment。",
      "speaker": "发言人3"
    },
    {
      "time": "01:11:14",
      "text": "大家经过了充分的讨论之后，才会提交代码。在讨论之前，我们需要把需要这个修改的地方要反复斟酌，对吧？而不是说一上来有一个人就交了几千行改动的代码，那其实是没有人愿意看的那我们都是有这样一步一步的deadline，说就是怎么样为这个community进行更好的贡献。我觉得这些是啊我们未来发展的一些方向。",
      "speaker": "发言人3"
    },
    {
      "time": "01:11:43",
      "text": "确实感觉VOM1大特点就是非常的user friendly，然后大家也都可以参与到贡献，包括使用上也非常的很好用。其实刚刚还有提到一个商业化的问题了。就是对于过往的一些开源项目来说，有很多的比如说商业画上比较成功的案例。我不知道我们团队对商业化这块是有什么样自己的考虑。然后像VOM有没有看过之前一些项目比较成功的商业化路径？在商业化这块下我的计划是什么样的？",
      "speaker": "发言人5"
    },
    {
      "time": "01:12:15",
      "text": "我们肯定就是说实话，我们肯定想过这个事儿。对。但是我觉得我们碰到的一个问题是，就是只做这种大语言模型的推理本身的话，它的商业模式对于我们来说不是特别的清晰。因为如果你只提供这样子的一个service，那你的一端是开源的模型，大家都那个access。另一端就是可能NV加GPU，主流态都是VDIGPU。那你这两个之间唯一的一个你的护城河就是一个你的技术。但技术这个事情你也很难保持一直的领先。",
      "speaker": "发言人4"
    },
    {
      "time": "01:12:47",
      "text": "我觉得eventually就是大家都会要把推理做到极致。需要的一些技术是什么？那你在这个时候，比方说你比另外一个人的优势，你可能是你的价格更便宜。但是价格更便宜意味着你的的这个利润就越低，那那你的这个business就不能一直的维持下去。所以我们觉得如果你只是做开源大语言模型，在NVDAGPU上的推理本身这件事情我们觉得不是一个特别的一个事情。",
      "speaker": "发言人4"
    },
    {
      "time": "01:13:15",
      "text": "我们现在目前的focus还是说，我们希望把我们的这个开源社区做好。我们能够把这个做成一个成功的open source的project。另一方面就是有很多使用VM的一个公司，他们的这个商业模式也不光是做大语言模型推理。我刚刚提到了两点，第一点是基于开源模型。第二点是在NVDIGPU上做inference。其实这两点你知道有变化，你都可能是一个比较可行的商业模式。",
      "speaker": "发言人4"
    },
    {
      "time": "01:13:41",
      "text": "第一点就是比方说是开源的模型，你可以做一些比方说开源模型的vtune，或者你训练和serve自己的模型之类的。或者像是帮大家做file的这种一个AI service的一个business。另一方面就是硬件，你不一定是这个理解GPT，你可能比方说最极端，你可以自己做一个硬件，像是brock之类的公司，或者38 nova之类的公司。对，他们可能在营业点就在于如何我有个自己的硬件来硬件作为自己的护城河。我觉得这些都是一些比较可行的，在这个LM series info这个领域的一些商业化的模式。",
      "speaker": "发言人4"
    },
    {
      "time": "01:14:15",
      "text": "最后去加了一点，就是说在VOM项目里面我们会发现VOM整个项目所创造的价值远远超过了我们伯克利团队所能捕获到的价值。但是这也代表着在Young的生态圈里面，有很多很多公司会发现他们在VR上的投入是远远小于他们能收到的回报。也就是他们的ROY是正的，然后他们的能收到的回报是足够来justify他们所投入的资源。这么说的话，我们其实是很开心的，很能做到一个非商业化的项目，而让大家都能更好的去商业化，特别对于博士生来说，这是一个已经可以想要的一个很好的一个结果了。",
      "speaker": "发言人2"
    },
    {
      "time": "01:15:14",
      "text": "可以说商业化对VIM来说并不是下一步的重点。right?",
      "speaker": "发言人5"
    },
    {
      "time": "01:15:19",
      "text": "对我觉得我们的focus还是在把开源项目做好，做成功。这就是我们一开始的go，就是成为一个industry and standard.",
      "speaker": "发言人4"
    },
    {
      "time": "01:15:28",
      "text": "成为一个标准，成为一个大家都能使用的，都能去更好的加速他们所想做的东西，能加速整个AI的进程的一个东西，就是我们想做的。",
      "speaker": "发言人2"
    },
    {
      "time": "01:15:42",
      "text": "明白。其实我好奇一个点，就是刚刚提到在现在的这个竞争环境上，如果大家都可以做到一个比较极致的效果的话，我不知道你们是如何看待未来的一些同质化项目的竞争，以及VOM在中间会有什么样的想法和选择。",
      "speaker": "发言人5"
    },
    {
      "time": "01:16:03",
      "text": "我觉得就是你看起来就是做大语言模型，就LM info的公司这么多，其实大家还是做不同的事情的。就是刚刚提到自己能做硬件，到其实上面还有一层是做云的。比方说大家觉得在这个AI为主，GPU为主的时代，我可能传统的这种云厂商的这个模式，像AWS这个公司的模式可能在现在不太适用。你可能需要一些更GPU native，AI native的一些管理云的模式。我觉得有很一系列公司都在做这个云这个角度的事情。再往上就是可能一些更加客制化的LM的service，比方说各不同的应用之类，他们那他们肯定都是不同的赛道，所以我觉得也没有那么挤挤的感觉。对，就是还是一个比较丰富的一个市场，大家都能找到自己的一口饭吃。",
      "speaker": "发言人4"
    },
    {
      "time": "01:16:53",
      "text": "好的，明白。最后一个问题，想请问大家最近有没有关注一些比较有趣，然后比较新鲜的开源项目，以及很想知道大家最近都在看什么书，然后会不会给我们的听众朋友做一个推荐。",
      "speaker": "发言人5"
    },
    {
      "time": "01:17:07",
      "text": "那那我先开始最近看了看项目，我觉得有两个比较有意思的。一个就是纯粹看见的一个叫on slots的一个项目。他们比较类似于VOM，是在做一个比较极致的一个优化，但是他们专注于是模型的微调fine tuning，然后他们是全部用try chino去重写的这么一个项目。我自己看这个从技术上来说是很有意思的一个项目。毕竟他可以推到一个拍拖，是目前来说可能无法达到一个极致。当然我们也很期待看，比如说拍拖到年底或者到明年这个时候说不定就追上了。这也是一个很有意思的一个社会实验。我觉得看什么书，最近我重读了一下，ben horrible的创业维艰，the hard thing about hard things.",
      "speaker": "发言人2"
    },
    {
      "time": "01:17:58",
      "text": "我觉得其实在这个时候读特别有意思。因为一个是在思考VIN作为一个类似于创业项目，但又不完全是商业化的一个项目，跟创业有什么共同点或者不同点。另外一个是去看一下他们经历了2000年的dot com bubble和crash的时候，并且想到我们现在可能在一个泡沫里面会是去怎么应对，我觉得都是很有意思的一些事儿。",
      "speaker": "发言人2"
    },
    {
      "time": "01:18:31",
      "text": "所以你觉得VRM和创业有什么异同？",
      "speaker": "发言人5"
    },
    {
      "time": "01:18:34",
      "text": "就在做产品上面很相似，是在一个寻找PMF并且去维持PMF的这么一个过程。然后在这种指数型的使用增长上是非常相似的。但是不同的点就在于我们的精力不会用在比如说去招人，然后不会用你去做一些sales或者是一些为了商业化而必须去维持的一些活动。这样的目的就会变得更纯粹，然后我们去更享受这样的事儿。",
      "speaker": "发言人2"
    },
    {
      "time": "01:19:09",
      "text": "好的，谢谢Simon。",
      "speaker": "发言人5"
    },
    {
      "time": "01:19:11",
      "text": "对我觉得我最近在感觉比较有意思的一些项目是就是Andrew capacity最近新open source一系列项目。对，因为我觉得最近不知道为什么突然有很多人问我，我想要入门LM应该怎么办？怎么做？从哪开始？我想了半天，最后的这个是就是你可以看看andy的这些项目，我觉得它是一个非常浓缩，非常好的一个关于LM的系统介绍，甚至是就是从零基础开始，一直到一些很多VOM本身需要的一些核心的技术，都可以在一个比较简单的report里面看到。我觉得这是一个对整个community和整个这个世界都很有意义的一个事情。看的书，我最近在读elon mask没有什么特殊的理由，但是我觉得这个就是我在这个飞机上读了一个传奇的人，传奇的经历。",
      "speaker": "发言人4"
    },
    {
      "time": "01:20:02",
      "text": "我说一下，我觉得我最近没有关注什么新鲜的开源项目，但我觉得有意思的开源项目就值得大家看一看的。比如如果大家对于一些我们的system感兴趣的，尤其是用过拍特曲，然后想知道他后面为大家做了什么事情的，我觉得可以推荐大家去了解一下拍touch。我觉得pyto ch是一个非常好的open source的样例。他一开始和tensor FLOW竞争的时候，其实和VOM是有点相似的。就是tense flow的用户可能会看不上拍拖去太慢了。你为了用户的易用性做了这么多的牺牲，这么多的优化都没有办法做。但是后来我们会发现TE sor flow因为太追求性能，而且基本上没有外部的contribute，最后他就逐渐的封闭。以至于现在很少有新的用户在加入了。所以我觉得学习开拓者和他的design可以给我们一些启发。",
      "speaker": "发言人3"
    },
    {
      "time": "01:21:09",
      "text": "说到这一点。",
      "speaker": "发言人5"
    },
    {
      "time": "01:21:10",
      "text": "我觉得其实大家也可以看jack，我可能看了那么多project jx唯一一个觉得我看了之后觉得这个software还可以这么写，这个一切都很有道理的感觉。对这样的一个软件，我觉得这个goole还是吸取了很多text flow的经验，把他们都扔到了jack里面。",
      "speaker": "发言人4"
    },
    {
      "time": "01:21:31",
      "text": "凯超有推荐的书吗？",
      "speaker": "发言人5"
    },
    {
      "time": "01:21:33",
      "text": "最近没有看书，都在扣定。",
      "speaker": "发言人3"
    },
    {
      "time": "01:21:37",
      "text": "谢谢。然后最后请你。",
      "speaker": "发言人5"
    },
    {
      "time": "01:21:40",
      "text": "我最近有看你稍微关于agent的rap，比如说QN agent，allow agent可能这些都比较旧。但是因为VLM它没有到科诺那个level，对吧？但它是一个infant engine level。然后我比较好奇，比如说on top of VLM大家是怎么用的，大家在玩哪些东西。所以我觉得如果大家感兴趣的话，也可以看一看那些。",
      "speaker": "发言人1"
    },
    {
      "time": "01:22:03",
      "text": "好的，问题就是这些，也很感谢今天大家的时间，然后和VRM团队的大家也聊得非常开心，希望听到这期播客的朋友们可以多多的使用v rm如果有需求反馈的话，也欢迎联系我们帮忙对接。这个基金也一直在关注前沿AI领域的发展。如果大家有好的创业想法，也欢迎和我们来聊一聊。同时也欢迎听众朋友们在各大音频平台订阅此话当真播客，我们下期再见。",
      "speaker": "发言人5"
    }
  ],
  "lab_info": {
    "summary": "开源项目持续改进旨在支持更广泛模态和调度策略，确保代码质量和易读性，适应AI领域快速发展趋势，提升至生产级标准，包括日志容错和自动缩放等技术，满足大规模用户需求。项目目标不仅是成为研究工具，也旨在成为工业标准，促进AI进程。开源项目面临的商业化挑战包括寻找商业模式、避免同质化竞争，提出多种商业路径，如模型优化、硬件开发等。VLM项目通过技术创新和社区合作，成长为全面框架，为科研人员提供丰富功能，促进社区成长。团队来自不同背景，解决大模型推理中的性能问题，如GPU内存管理、并行执行优化。项目致力于成为最快的、最好用的开源引擎，目标是大语言模型推理的标准。真格基金的捐赠显示了对项目潜力的认可。未来计划扩大硬件支持、持续性能优化，保持架构灵活性，适应AI领域需求。VLM故事展示了开源项目如何通过社区贡献、持续优化和适应市场需求成长。",
    "qa_pairs": [
      {
        "question": "在过去一年中，VLM有哪些显著的变化和发展？",
        "answer": "在过去这一年，VLM从一个初步的技术点成长为一个非常完善的framework，成为了科研领域的百宝箱，提供了各种实现想法feature的入口。",
        "time": "00:00:02"
      },
      {
        "question": "如何处理在开源项目中抑制个人修改冲动，促进社区成长的问题？",
        "answer": "作为开源项目的一员，应学会抑制自己直接修改项目的冲动，转而给予社区耐心和支持，鼓励大家逐步贡献自己的力量，共同搭建项目的发展。",
        "time": "00:00:21"
      },
      {
        "question": "在开发实用型系统时相比纯科研研究面临的主要挑战是什么？",
        "answer": "开发实用型系统时，最大的困难在于需要全面的支持，不能仅限于特定环境使用，而是要确保系统能在多种环境下稳定运行。",
        "time": "00:00:43"
      },
      {
        "question": "VRM项目的初衷和发展目标是什么？",
        "answer": "VRM项目的目标是从一开始就成为最快且最好用的开源引擎，并致力于成为大语言模型推理的标准。",
        "time": "00:01:04"
      },
      {
        "question": "VRM团队成员最初是如何了解到并决定加入VRM项目的？",
        "answer": "不同成员有不同的加入途径，如李卓翰从项目早期就开始参与，负责开源社区管理和技术框架设计；刘晓萱在了解项目后，因其科研方向与项目结合，决定加入并探索更多科研方向。",
        "time": "00:03:34"
      },
      {
        "question": "VRM项目的核心功能和技术亮点是什么？",
        "answer": "VRM是大模型推理与服务引擎，其核心功能是部署已训练好的模型到生产环境以提高效率和降低成本。其中，Page Attention算法显著提升了内存利用率和吞吐量，同时包含一系列前沿技术如连续 batching、模型量化、模型并行等，支持多种主流模型，并致力于在不同硬件上优化运行效率。",
        "time": "00:07:11"
      },
      {
        "question": "VRM的用户群体有哪些？",
        "answer": "作为开源项目，VRM的用户广泛，包括几乎所有的云服务平台作为模型服务的基础，以及开源大模型厂商和使用AI辅助产品的团队，他们通过VRM实现模型微调和高效利用。",
        "time": "00:10:06"
      },
      {
        "question": "使用VRM能在成本节省和性能提升上带来多少优势？",
        "answer": "节省的具体成本取决于用户实际应用场景，无法给出通用数字。但论文中提到在特定场景下可达到显著加速效果，用户可通过实际测试比较VOM与其在自己场景上的表现。",
        "time": "00:11:16"
      },
      {
        "question": "VRM在模型推理加速上采用何种技术方向？",
        "answer": "VRM在模型推理加速上采取多种技术手段，包括但不限于模型量化、speculating、chunk preview、after GPU coronal等，通过权衡不同资源利用，以实现最佳的推理性能。此外，针对大模型推理，VRM还特别优化了调度策略和内存管理，如continue batching和配置attention机制，这些优化方法目前在大模型推理领域较为前沿且独特。",
        "time": "00:12:49"
      },
      {
        "question": "在2022年，你们是怎么想到并推出了VLM的第一个版本，这是一个非常新颖的思路吗？",
        "answer": "是的，这个想法确实很新颖。大约在2022年年底，我们在学校搭建了一个使用facebookOPT175B模型的大语言模型demo。在部署过程中，我们遇到了GPU利用率低和推理慢的问题，这让我们意识到大语言模型推理本身是一个值得关注和优化的方向。当时市面上没有针对大语言模型优化的开源系统，于是我们决定从零开始做一个。",
        "time": "00:16:30"
      },
      {
        "question": "VLM项目中有哪些重要的技术创新点？",
        "answer": "我们提出的新技术主要包括一个新的attention计算方法——page attention，用于改善transformer模型中内存使用瓶颈的问题，并通过操作系统底层的技术管理内存，显著节省了内存并提高了存储量。此外，我们还优化了大模型推理时GPU memory的管理。",
        "time": "00:17:09"
      },
      {
        "question": "VLM项目的团队成员组成和组织形式是怎样的？",
        "answer": "团队成员都是伯克利大学的博士生，大家在同一实验室工作，基于兴趣和对开源项目的热爱自愿聚在一起合作。我们没有严格的形式化组织或业绩考核，全靠内在驱动来进行这个合作项目。",
        "time": "00:18:56"
      },
      {
        "question": "VLM项目的目标是什么？目前实现程度如何？",
        "answer": "项目的目标是成为最快且最好用的开源引擎，并成为大语言模型推理的标准。我们在追求速度上一直在努力，也致力于提高易用性，让项目成为一个标准的过程正在进行中。",
        "time": "00:20:33"
      },
      {
        "question": "从page attention到现在的更新，VLM有哪些重大升级？",
        "answer": "VLM在过去的两年中变化很大，除了最初的page attention技术，还加入了像 prefix cash、quantization（低精度推理）等众多优化方法，成为一个完善的推理框架，能够帮助科研人员快速实现各种想法和功能。",
        "time": "00:22:15"
      },
      {
        "question": "作为开源项目，VLM在重要领域取得世界级性能和影响的原因是什么？",
        "answer": "开源模式让我们能够吸引业界人士参与，共同贡献feature，形成共赢局面。同时，VLM在技术上注重代码质量，坚持简洁易读和模块化的编程风格，这使得我们能够灵活集成新优化方法和feature，保持技术领先位置。",
        "time": "00:26:35"
      },
      {
        "question": "在VLM这个松散的组织下，如何做到代码质量高的要求？",
        "answer": "代码质量的高低是主观的，但我们始终坚持高质量标准，对每一份Pull Request都进行严格的code review，甚至接手并改进。我们会不断反思和优化代码结构，确保代码的简洁性和良好的模块化特性。",
        "time": "00:27:55"
      },
      {
        "question": "在伯克利团队的工作中，你们如何设计框架以方便新算法的实现和应用？",
        "answer": "我们在Zon框架上进行了一系列的抽象改进，使得新的量化算法（如zon算法）能够容易地在VRM中实现，并且可以在多种模型上通用。通过将量化算法实现方法与模型定义完全分离，添加新的量化方法变得更为简单和干净，这样模型开发者就不需要考虑框架的具体细节。",
        "time": "00:28:56"
      },
      {
        "question": "伯克利实验室开源项目有什么传统特色以及开源如何带动业界和学界共同发展？",
        "answer": "伯克利的开源传统是将学界最新的研究思想与业界的实际生产环境相结合，让博士生的探索性工作与工程师的实践经验相互碰撞，共同创新。例如在做瑞的时候，学术界和业界的合作使得开源项目兼具实用性和前瞻性，推动整个行业进步。",
        "time": "00:31:17"
      },
      {
        "question": "VM在发展的过程中有哪些关键节点和团队思路的转变？",
        "answer": "最初，我们的主要目标是追求速度，尤其是在Facebook OPT demo上展示出的性能表现不佳后，我们意识到必须解决系统响应速度慢的问题。开源发布后，我们的关注点扩展到支持多种模型、集成优化方法和改进系统架构，以更好地服务于开源社区和贡献者。",
        "time": "00:34:47"
      },
      {
        "question": "VM与模型厂商建立紧密合作关系的过程是怎样的？",
        "answer": "这个过程是有机且自然发生的，主要原因在于我们的框架易于使用且能显著提升性能，吸引了大量用户。当开源模型发布时，它们也希望自己的模型能在VM上获得最大影响力，从而形成了双方共赢的合作关系。",
        "time": "00:36:04"
      },
      {
        "question": "在与模型厂商合作的过程中，有没有特别有意思的故事可以分享？",
        "answer": "有意思的是，比如Mora团队，在发布其模型之前，会在Twitter上发布一些谜题让大家猜测，并提前向我们提供源码以便我们能快速适配，确保他们的新模型在发布后能得到第一时间的支持。",
        "time": "00:37:18"
      },
      {
        "question": "对于当前大模型发布前的宣传和热度，VM有何观察？",
        "answer": "我们观察到大模型发布前确实会有较多悬念被制造，无论是闭源还是开源模型，大家对其新特点都非常期待。例如，当VM支持千问模型时，会收到大量用户询问和反馈，这也帮助我们了解模型流行程度及适配需求。",
        "time": "00:38:40"
      },
      {
        "question": "加入LMVLM以来，凯超遇到过哪些印象深刻的技术难点？",
        "answer": "做开源系统的一大挑战是全面支持各种环境下的运行，包括用户可能遇到的各种硬件、驱动和软件环境问题。不同于研究环境下的特定条件测试，开源系统需要确保在多种环境下都能稳定运行，这要求我们解决很多corner case，虽然困难但也促进了个人成长和学习。",
        "time": "00:40:49"
      },
      {
        "question": "目前架构是否足够diverse，未来有哪些可能成为主流的模型结构？",
        "answer": "目前架构还不够diverse，接下来可能会出现更多样化的模型，例如state face model和混合形式的模型，它们结合了attention机制与局部性。对于未来的发展，大家正在追求模型的效率，当顶级模型能力接近天花板时，降低推理成本成为重点，如探索类似deep sick后期的attention等方法以提高效率。",
        "time": "00:42:58"
      },
      {
        "question": "在模型结构创新上，你们有何看法？",
        "answer": "我们预计未来会更加注重模型的效率优化，并且在特定应用场景下，例如语言生成、聊天机器人以及处理长文本、多模态信息（图片、视频、语音）时，模型架构会有一定的变化和创新。但基础的attention机制可能不会改变太多，而会根据不同的使用场景和模型变种进行调整。",
        "time": "00:44:47"
      },
      {
        "question": "在MVM发展过程中，有哪些取舍？",
        "answer": "我们选择不做商业化项目，致力于打造一个开源项目，让更多公司参与并加速AI或AGI的历史进程。技术上，我们未追求极致性能，而是用更易为人所用的代码编写方式来实现性能与易用性的平衡，同时也不执着于使用特定硬件如NVIDIA显卡，而是关注其他硬件上的AI功能和性能发展。",
        "time": "00:46:14"
      },
      {
        "question": "对于开源领域的工作，如何把握易用性和易开发性？",
        "answer": "易用性和易开发性至关重要。我们追求简洁、清晰的代码以方便他人参与贡献，而不是盲目追求极致性能。同时，鼓励用户参与并改进项目，比如通过简化几行代码就能显著提升推理速度，从而提高研究者发表论文的效率。",
        "time": "00:48:31"
      },
      {
        "question": "VLLM的主要贡献者有哪些？他们的动力和参与故事是什么？",
        "answer": "VLLM的贡献者包括云厂商（如AWS、Oracle、IBM）、模型厂商（如mr o和千问）、硬件厂商（如Intel、Google等）、产品公司（如LinkedIn、Robots）、新一代AI公司以及学生和独立工程师等。这些贡献者通过各种方式参与项目，如修复bug、开发新功能、优化模型适应性等，并积极反馈给社区，共同推动项目发展。",
        "time": "00:51:11"
      },
      {
        "question": "VLLM在短时间内如何吸引大量外部贡献者？",
        "answer": "这得益于项目本身的质量高、易用性和快速市场响应，使其迅速成为用户首选的开源项目。此外，通过建立稳定的贡献者关系网络，逐个联系潜在贡献者，了解他们的需求和兴趣，进而邀请他们加入项目。",
        "time": "00:54:21"
      },
      {
        "question": "VLLM线下活动是否有有趣的用户或故事可以分享？",
        "answer": "线下活动较少但充满活力，其中有一位英国工程师积极参与项目维护，每天 triage 和 close 旧的 issue，为社区做出了巨大贡献。此外，在 meet up 活动中，会遇到很多新用户使用 VLLM，形成了一种网络姻缘。",
        "time": "00:56:56"
      },
      {
        "question": "在湾区有一个全新的开源项目团队，我们可以分享给他们什么样的经验？",
        "answer": "首先，重要的是明确做开源的目的，比如是为了营销销售、招聘还是寻找产品市场契合度（PMF）。对于学生团队而言，则需考虑投入的时间和接受的贡献程度。此外，在开源项目早期阶段，接受用户需求和bug报告至关重要，要耐心引导社区成长，让社区成员逐渐适应并提高贡献质量。",
        "time": "00:59:24"
      },
      {
        "question": "开源项目团队搭建组成时有哪些经验可以分享？",
        "answer": "团队搭建初期，成员间的技术互补性很重要。例如，在V one项目中，我和乌苏分别负责高层管理和低级内核编写，Simon凭借其产品管理经验帮助项目更加有序发展，而其他成员如Lily和凯涛则从研究和创新角度为项目带来新思路和功能。",
        "time": "01:01:17"
      },
      {
        "question": "在VOM项目中，如何发现产品PMF的？",
        "answer": "当问题和需求（P2）指数级增长，而团队精力有限无法满足所有新增功能时，通常意味着产品可能已经找到了自己的PMF。",
        "time": "01:02:51"
      },
      {
        "question": "VRM团队在推理加速引擎领域有哪些前沿学术研究，并如何与开源项目结合？",
        "answer": "目前的研究重点包括speculative decoding、低精度推理和KV cash优化等方向。VRM会将这些基础特性实现后开源，吸引社区成员基于通用框架贡献自己的算法，通过active review PR来确保与框架兼容，这样既促进了科研成果的落地，也为科研人员提供了看到自己工作被实际应用和反馈的机会。",
        "time": "01:04:45"
      },
      {
        "question": "VRM下一步工作重点是什么？",
        "answer": "重点之一是增加对更多硬件的支持，确保VRM能够适应不同厂商和不同类型的GPU，提供性能优化和经济适用的选择；其次，继续提升性能，进行科研和工程优化，保持竞争力；最后，增强架构的可扩展性，支持多种模态输入输出，提升生产级别运维能力，并遵循开放-source最佳实践进行维护和更新。",
        "time": "01:07:22"
      },
      {
        "question": "对于VOM项目的商业化路径，团队有何考虑和计划？",
        "answer": "我们确实考虑过商业化问题，但单纯提供大语言模型推理服务的商业模式并不清晰。我们需要探索如何在开源技术和商业服务之间构建可持续的商业模式。",
        "time": "01:12:15"
      },
      {
        "question": "在大语言模型推理方面，你认为相比其他竞争者的优势可能是什么？",
        "answer": "我们目前更关注把开源社区做好，做成一个成功的open source project。同时，针对使用VM的公司，除了大语言模型推理外，还可以基于开源模型做训练和serve自己的模型、AI service等业务。此外，通过自研硬件，如Brock或38 Nova这类公司，利用自有硬件作为护城河也是一种可行的商业模式。",
        "time": "01:13:15"
      },
      {
        "question": "对于VOM项目创造的价值远超伯克利团队所能捕获的价值这一情况，您怎么看？",
        "answer": "这表明在VOM生态圈中，许多公司投入在VOM上的资源小于他们获得的回报，他们的ROI是正向的。作为非商业化项目，能让大家更好地商业化，特别是对博士生而言，这是一个理想的成果。",
        "time": "01:14:15"
      },
      {
        "question": "对于未来同质化项目的竞争以及VOM在这个环境中的想法和选择，您有何看法？",
        "answer": "虽然大家都在做大语言模型，但实际上大家都在做不同的事情，比如有的做硬件，有的做云服务，还有更客制化的LM服务。市场足够广阔，大家能找到各自的定位，不必担心过于拥挤。",
        "time": "01:16:03"
      },
      {
        "question": "最近有没有关注到什么有趣的开源项目或书籍推荐给听众？对于想要入门LM的人，有什么推荐的开源项目或书籍？",
        "answer": "最近看的一个有意思项目是on slots，它专注于模型微调fine tuning，采用try chino重写了整个项目，技术上有很大挑战性。推荐书籍是《创业维艰》（The Hard Things About Hard Things），有助于思考VOM作为非完全商业化项目与创业的共性和不同点。推荐Andrew capacity最近开源的一系列项目，这些项目提供了关于LM系统介绍和核心科技的浓缩内容，适合从零基础开始学习。另外，拍touch也是一个很好的open source样例，展示了如何平衡易用性和性能优化。",
        "time": "01:17:07"
      },
      {
        "question": "VRM和创业在哪些方面相似，又有哪些不同？",
        "answer": "在产品层面，VRM和创业都在寻找并维持产品市场契合度（PMF），并在指数型增长中保持相似性。但VRM不关注商业化活动，如招聘和销售，所以目的更纯粹，享受研发过程。",
        "time": "01:18:34"
      }
    ],
    "chapters": [
      {
        "time": "00:00:00",
        "title": "VLM开源项目的发展与社区建设",
        "summary": "在过去一年里，VLM从一项技术点发展成为一个成熟的框架，成为科研领域的百宝箱，支持各种创意的实现。项目团队强调，面对VLM的快速进步，重要的是抑制个人主导改进的冲动，而是应该给予社区成长的时间和空间，逐步通过社区成员的贡献来完善项目。此外，作为一项旨在成为大语言模型推理标准的开源引擎，VLM的目标是提供全面的支持，超越单一环境的限制。真格基金对VLM的捐赠体现了社区对项目价值的认可，也标志着VLM团队成员将与各领域领军人物分享项目创立的故事和作为国际影响力开源项目的成功经验。"
      },
      {
        "time": "00:02:31",
        "title": "加州大学伯克利分校博士生团队介绍",
        "summary": "该对话介绍了三位来自加州大学伯克利分校的博士生，他们共同参与了一个名为VM的项目。项目从一个初步的想法发展到开源项目，并致力于机器学习系统的优化，包括算法执行速度的提升和大规模并行执行的研究。团队成员各自负责不同方面的工作，包括技术框架设计、科研方向探索、推理加速和社区管理等，共同推动了项目的发展和技术的进步。"
      },
      {
        "time": "00:05:21",
        "title": "清华博士生游凯超分享在伯克利的访问研究体验及VOM项目进展",
        "summary": "游凯超，一名来自清华大学的博士生，在伯克利进行访问研究，介绍了他加入VOM项目的原因及其对项目的贡献。他最初因为对机器学习编译器的研究兴趣和VOM项目中大模型推理优化的需要而加入。加入后，游凯超发现VOM作为一个开源项目，在多个方面需进行优化，因此他开始致力于VOM的开源维护工作。他还简要介绍了VOM项目，强调了VOM在大模型推理与服务引擎方面的优势，包括其效率、成本控制、page attention算法的应用以及支持多种大语言模型的能力。VOM的目的是提高模型推理的速度和吞吐量，通过一系列技术实现对大模型的高效推理和部署，从而促进AI产品的快速落地。"
      },
      {
        "time": "00:09:59",
        "title": "VRM开源项目用户群体与技术优势解析",
        "summary": "VRM作为一个开源项目，其用户群体广泛，包括几乎所有的云服务平台和开源大模型的厂商，以及使用AI辅助产品的开发者。这些用户在不需注册的情况下即可使用VRM，体现了开源项目的特点。VRM的技术优势在于其对模型推理加速的处理，能够针对不同的硬件模型和场景提供定制化优化，例如通过模型量化、投机性解码等技术手段提高模型推理速度和效率。特别地，VRM在多请求调度和优化方面的创新，使其能够在大模型推理上实现显著的性能提升，从而吸引了广泛的用户群体。"
      },
      {
        "time": "00:15:37",
        "title": "从大语言模型优化到Page Attention算法的创新历程",
        "summary": "在2022年年底，一个团队在学校内启动了一个大语言模型的demo，最初使用的是Facebook的OPT175B模型。该项目旨在宣传他们的另一开发项目Alpha ALPA，一个自动进行模型并行和推理的研究项目。然而，demo的部署暴露了大语言模型推理中存在的问题，如运行速度慢和GPU利用率低下。意识到市面上缺乏针对大语言模型优化的开源系统，团队决定自主研发解决方案。在开发过程中，团队发现GPU内存是大模型推理的一大瓶颈，并且传统的内存管理方式存在较大浪费。经过多次迭代，团队提出了一种新的attention计算方法——Page Attention，该方法通过利用操作系统中的分页和retro memory技术来优化transformer中KV cache的内存使用，有效提升了模型的效率和内存利用。最终，该团队在2023年6月底开源了他们的扩展，并持续维护更新，得到了社区的广泛关注和使用。"
      },
      {
        "time": "00:18:21",
        "title": "开源项目VLM团队介绍与项目目标",
        "summary": "VLM作为一个开源项目，由伯克利的博士生主导，成员都来自同一实验室，基于对项目的热爱和对开源贡献的兴趣自愿集结，没有严格的组织形式和业绩考核，全靠内在驱动。项目初始目标是成为最快和最好用的开源引擎，专注于大语言模型推理的标准。团队通过不断优化框架和技术，努力实现这一目标，使得VLM成为了一个完善的推理框架，适用于多种功能和想法的实现。"
      },
      {
        "time": "00:22:03",
        "title": "开源项目VLM及其在科研与实践中的应用和优势",
        "summary": "VLM项目以其在减少推理延迟、提升推理效率等方面的显著特性，成功实现了在实际生产环境中的应用，验证了其技术的有效性。特别是在提升模型推理速度、降低延迟方面，VLM通过如page tension和连续处理等技术手段，实现了性能的显著提升。此外，通过开源和社区合作，项目吸引了来自不同公司的工程师参与贡献，从而进一步丰富了其功能性和实用性。这不仅显示了开源模式在推动技术发展方面的强大动力，也反映了VLM作为一项具有创新性和实用性的技术，在学界和业界都获得了广泛认可。"
      },
      {
        "time": "00:25:49",
        "title": "开源框架在大模型推理中的优势与实践",
        "summary": "对话集中在开源框架相较于传统大厂开发模式在大模型推理方面的优势。首先，对于大模型的推理，无需大量GPU资源，如对于较小模型使用一块A100 GPU便足够，而较大模型也只需4到8块GPU。其次，开源框架通过保持高代码质量、简洁易读和良好模块化，吸引了更多贡献者。特别地，代码质量控制有助于集成新的优化方法和功能，保持项目的先进性。此外，通过严格的代码审查和重构，确保框架能适应新算法，且不依赖特定硬件。团队成员背景丰富，具有在业界工作的经验，有助于维持代码的高质量。"
      },
      {
        "time": "00:30:23",
        "title": "开源项目如何促进学界与业界的合作与创新",
        "summary": "对话中讨论了开源组织相对于大型公司，在无明确KPI或发布压力下，能够实现大厂难以达成的目标。通过Simon在伯克利实验室的经验，展现了开源项目如何结合学界的新想法与业界的实际应用，推动技术和产品的创新。特别强调了开源项目在速度、实用性与创新性之间的平衡，以及开源社区对模型支持和系统改进的重要性。此外，还提到了VM项目的关键发展节点和团队思路的转变，强调了用户反馈对于项目迭代的重要性。"
      },
      {
        "time": "00:34:47",
        "title": "开源模型合作与优化集成的重要性",
        "summary": "讨论重点在于如何为重要的模型添加支持，并整合各种优化方法，以及如何构建更好的系统架构以方便其他贡献者加入新的优化。同时，强调了从专注于性能到服务开源社区和贡献者的转变。探讨了与其他推理加速引擎如Tensor RT相比，通过紧密合作与广泛的知识支持，如何建立起与开源模型开发者之间的合作关系。指出框架的易用性和性能提升是吸引用户和模型开发者的主要原因，从而促进了有机的社区合作过程。"
      },
      {
        "time": "00:36:45",
        "title": "大模型发布前的宣传策略与合作体验",
        "summary": "欧洲一家不知名的公司，后来成长为Mr O，在其模型首次发布时便与我们接触。这家公司采取了独特的宣传策略，例如在周五通过Twitter发布模型的简单信息，激发公众的好奇心并进行猜测。同时，他们会私下将模型详情分享给合作伙伴，以便在正式发布时能快速提供支持。这种合作模式和发布前的悬念营造，展现了大模型厂商在公关宣传上的创新方法。此外，开源模型如千问和Deep Sik的流行，也导致了大量用户询问和对这些模型的支持需求增加。通过这些合作和观察，可以看出模型的受欢迎程度以及社区对新模型的期待。"
      },
      {
        "time": "00:40:20",
        "title": "开源项目中的技术挑战与未来发展",
        "summary": "开源项目的核心优势在于其能够汇集广大社区的力量，共同面对技术挑战，分享知识与进展。特别地，讨论集中在了VOM项目上，该项目致力于打造一个广泛使用的系统，因此面对的挑战是如何确保系统在各种环境下的兼容性和稳定性，特别是在处理硬件、驱动以及软件环境的问题时。此外，讨论还涉及了模型支持的挑战，特别是面对各种LM模型时，如何处理复杂多样的corner case。未来，项目预计将更加注重模型结构的创新和效率，探索如何在保持模型能力的同时降低成本，并适应不同的使用场景。具体方向包括对效率极限的探索、适应长文本处理的模型、多模态能力的增强，以及基于场景的模型变体的开发。"
      },
      {
        "time": "00:45:52",
        "title": "开源项目MMVIM的发展策略与取舍",
        "summary": "在MMVIM的开发过程中，团队面临资源限制，必须做出多项关键决策。首先，选择不将项目商业化，以加速AI或AGI的发展进程，尽管这意味着放弃了一些盈利空间。其次，在技术选型上，不追求极致性能，而是采用易于参与的编程语言，如Python，确保项目对社区的友好性。此外，不专一于NVIDIA显卡，转而探索其他硬件，以适应未来的发展趋势。团队强调易用性和易开发性，避免加入可能增加代码复杂度的功能，目的是鼓励更多社区成员的参与和贡献。"
      },
      {
        "time": "00:50:50",
        "title": "开源项目VOM的国际贡献者及其动力",
        "summary": "VOM项目吸引了来自多个国家的贡献者，包括AWS、Oracle、IBM等云厂商，以及模型厂商如mr o和千问。这些贡献者不仅包括了长期团队，还有硬件厂商如AMD、Intel、Google等，以及专注于AI产品的企业。这些团队通过开源开发，优化和适配VM，来提升VOM项目在实际应用中的表现，同时让自己的产品和模型能够更好地服务于社区。VOM项目能在短时间内迅速吸引大量外部贡献者，一方面得益于项目本身的高质量和市场需求，另一方面则是通过持续的社区维护和生态系统建设实现的。项目团队通过与各方贡献者的交流，理解他们的需求和使用场景，进而建立稳定的贡献者关系网络，推动项目的持续发展和社区的活跃。"
      },
      {
        "time": "00:56:16",
        "title": "开源社区的活力与贡献者故事",
        "summary": "在VOM举办的四次线下活动中，社区成员通过网名相称，体验了网友见面的趣味。特别是一位位于英国的工程师，利用闲暇时间帮助项目分析旧issue，显著减少了未处理事项。社区内还存在着许多默默无闻的贡献者，他们的努力对于开源项目的发展至关重要。此外，每次Meet up活动都能发现新加入的公司用户，如Adobe，展示了社区的广泛影响。对比Control net项目，VOM展现了多元化和集体贡献的力量。对于新的开源项目团队，分享了关于开源目的、接受用户反馈和贡献、以及团队组建的经验，强调了耐心和社区成长的重要性。"
      },
      {
        "time": "01:01:17",
        "title": "项目管理与团队合作的实践分享",
        "summary": "在一个项目初期，团队主要由两位技术互补的成员负责研究和代码开发，他们分别擅长高层调度管理和底层技术实现。随着时间的推移，项目吸引了更多成员加入，包括具有产品管理经验的Simon和其他贡献者，如研究员和新的开发者。这些新增成员为项目带来了新的视角和功能，促进了项目的发展和管理的优化。在项目进展过程中，团队通过观察用户需求的增长和对新功能的请求，意识到产品的市场需求正在迅速扩大，从而找到了产品的市场匹配点（PMF）。这一过程中，他们认识到，当用户的issue和P2请求开始爆炸性增长，而团队已无法满足更多的功能需求时，就表明产品找到了其市场定位。"
      },
      {
        "time": "01:03:32",
        "title": "VRM团队在推理加速引擎领域的研究与开源项目结合的实践",
        "summary": "VRM团队专注于前沿学术研究，特别是在推理加速引擎领域，研究包括speculative decoding、低精度推理（如FP8）以及KV cache优化等，旨在降低延迟并节省计算资源。团队通过开发通用框架支持这些研究，使得不同研究者能够基于此框架实现自己的算法，促进了社区的贡献和交流。此外，开源项目不仅让研究得以实际应用，也帮助团队发现了新问题，从而推动了更多的科研工作，形成正向迭代的过程。"
      },
      {
        "time": "01:07:09",
        "title": "VRM框架未来发展重点及商业化思考",
        "summary": "VRM框架的未来发展将着重于增强硬件支持，性能优化，以及架构的可扩展性。特别强调了对不同硬件的良好兼容性，持续的性能优化，以及易于扩展的架构设计。此外，讨论了关于VRM框架的商业化前景，虽然对此持开放态度，但目前更专注于提升开源社区的活跃度和框架本身的质量，而非直接追求商业利益。指出通过开源模型的优化、提供AI服务或开发专用硬件等方式可能为商业化路径。强调了VRM项目旨在促进整个AI领域的发展，而非单一追求商业价值。"
      },
      {
        "time": "01:15:41",
        "title": "探讨AI领域的创新与竞争",
        "summary": "在当前激烈的竞争环境下，讨论集中在如何在同质化项目中脱颖而出，特别是大语言模型领域的竞争。提出对于云服务的AI特化需求和GPU优化的管理云模式的探讨，强调了市场的多样性与丰富性。对话中也提及了开源项目的关注，包括模型微调和从零开始的LM系统介绍项目，以及相关书籍的推荐，如《创业维艰》和关于Elon Musk的传记。同时，探讨了VRM与创业的异同，强调了在产品开发上的相似性以及对于团队构成和活动的不同。此外，还涉及了一些技术细节，如PyTorch的社区贡献和设计启发。最后，呼吁听众对VOM的使用和反馈，以及对未来AI领域创新的期待。"
      }
    ],
    "mindmap": {
      "children": [
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "目标：成为最快和最好用的开源引擎，成为大语言模型推理标准"
                },
                {
                  "children": [],
                  "content": "特点：支持多模型、优化性能、开源社区紧密集成"
                },
                {
                  "children": [],
                  "content": "用户：云服务平台、开源模型厂商、AI辅助产品开发者"
                }
              ],
              "content": "VRM作为大语言模型推理与服务引擎"
            }
          ],
          "content": "VRM项目介绍"
        },
        {
          "children": [
            {
              "children": [],
              "content": "重点技术：Page Attention算法、Quantization、Speculative Decoding、Prefetching"
            },
            {
              "children": [],
              "content": "优化方向：提升内存利用率、提高吞吐量、跨请求优化"
            },
            {
              "children": [],
              "content": "对大模型推理的贡献：提高模型推理速度和效率"
            }
          ],
          "content": "技术与优化"
        },
        {
          "children": [
            {
              "children": [],
              "content": "成员背景：主要来自伯克利博士生，覆盖系统、研究等领域"
            },
            {
              "children": [],
              "content": "团队组织：基于兴趣和对项目的热爱，无明确业绩考核"
            },
            {
              "children": [],
              "content": "贡献模式：鼓励社区参与，为爱发电"
            }
          ],
          "content": "团队组成与贡献"
        },
        {
          "children": [
            {
              "children": [],
              "content": "开源精神：继承伯克利开源传统，促进学界与业界合作"
            },
            {
              "children": [],
              "content": "社区成长：通过meetup、code review等促进交流和贡献"
            },
            {
              "children": [],
              "content": "项目维护：重视代码质量，确保项目易用性和扩展性"
            }
          ],
          "content": "开源与社区"
        },
        {
          "children": [
            {
              "children": [],
              "content": "商业化态度：重点在开源社区和技术创新，不追求直接盈利"
            },
            {
              "children": [],
              "content": "未来计划：持续优化性能、扩展模型支持、提升硬件兼容性"
            },
            {
              "children": [],
              "content": "长期目标：成为大语言模型推理的行业标准"
            }
          ],
          "content": "商业化与未来方向"
        },
        {
          "children": [
            {
              "children": [],
              "content": "科研与实践结合：将科研成果应用于实际项目，验证其有效性"
            },
            {
              "children": [],
              "content": "推动科研发展：通过开源项目反馈新科研问题，促进技术迭代"
            },
            {
              "children": [],
              "content": "社区与科研互动：社区需求驱动科研探索，科研成果反哺社区"
            }
          ],
          "content": "技术与科研结合"
        },
        {
          "children": [
            {
              "children": [],
              "content": "对VRM项目的展望：保持技术领先，加强社区互动，推动AI发展"
            },
            {
              "children": [],
              "content": "对开源文化的推崇：鼓励开源精神，促进技术创新和知识共享"
            },
            {
              "children": [],
              "content": "对未来的思考：探索开源项目与商业化之间的平衡，促进可持续发展"
            }
          ],
          "content": "结语与展望"
        }
      ],
      "content": "VRM项目与团队讨论脑图摘要"
    }
  }
}