{
  "pid": "5e5c52c9418a84a04625e6cc",
  "eid": "6605fe71338c31204886d3f1",
  "title": "E145 | 对话Meta田渊栋：被Transformer改变的世界与人类AGI的野心",
  "task_id": "zj78qpvpkrxzqxdp",
  "transcription": [
    {
      "time": "00:00:05",
      "text": "嗨大家好，罗永浩的直播间开始卖云产品了，还上了微博热搜。可能跟大家想象中不一样，都AI时代了，很多的中小企业依然在购置服务器。这里有一份数据，中国信通院2022年的数据显示，在美国的算力结构中，云计算占比超过了60%，欧洲超过50%。",
      "speaker": "发言人1"
    },
    {
      "time": "00:00:30",
      "text": "大家猜一猜中国有多少？只有28%。所以说中国的算力资源虽然现在是稳居世界第二，仅次于美国。但是其实现在这些算力的利用效率并不高，其中一个原因就是公有云这种高效的算力模式占比过低。",
      "speaker": "发言人1"
    },
    {
      "time": "00:00:50",
      "text": "之前老罗他其实自己也带火过很多的概念。我们这一次来看一看老罗能不能引领一场企业级的埃批认知运动，把云计算这个概念推向大众。他这次的选品也是覆盖了阿里云众多的热门产品价格，也是给出了史无前例的优惠。所以大家如果感兴趣的话，可以去淘宝APP搜索罗永浩。3月31号晚上七点，让我们一起围观连续创业者罗永浩卖云产品，以及他是如何解决创业者的核心痛点的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:01:44",
      "text": "2017年，谷歌一篇划时代的论文attention is all you need，掀开了这一轮人工智能的开幕式。这篇论文也是大家现在知道的大名鼎鼎的transformer。七年过去了，我们看到有人在这篇论文上加算力加算法，开启了第三次的科技浪潮。",
      "speaker": "发言人1"
    },
    {
      "time": "00:02:08",
      "text": "今天我们的嘉宾来自metafile的研究员田渊栋博士。他最近也发表了两篇论文，都与端侧的小模型相关。由于离应用更近，并且在解决更加实际的问题，所以他的论文经常被很多工业界的人问到。而过去的这些年，他所有的研究都在回答同一个问题，就是神经网络是如何工作的。今天我们就一起尝试探索这个问题，也跟大家一起聊一聊最近大火的sora transformer，还有AGI。",
      "speaker": "发言人1"
    },
    {
      "time": "00:02:45",
      "text": "Hello, 田博士你好你好。我大概跟可能还不太了解田园栋的听众简单介绍一下他过去的经历。田园洞是2018年围棋开源项目ELF open go研究及工程负责人和第一作者。当时我记得这个项目其实在开源社区里面名气还蛮大的。然后也是曾经获得了2021年国际机器学习大会icml杰出论文奖提名，以及2013年国际计算机视觉ICCV马尔奖提名。他的研究方向是深度强化学习，表示学习和优化。",
      "speaker": "发言人1"
    },
    {
      "time": "00:03:21",
      "text": "历任机器学习国际会议icml europe AAAIAI status领域的主席。而且我记得你是2013年到14年左右，是在谷歌的无人驾驶担任软件工程师。是的，那个时候应该是无人驾驶非常早期的时候，整个业界还没有注意到无人驾驶的时候。",
      "speaker": "发言人1"
    },
    {
      "time": "00:03:43",
      "text": "对，大概是这样子，或者确切说是国内还没有注意到，在硅谷这边其实已经有人注意到了，就是有挺多的机会，大概在2015年16年开始就起飞了。对我当时有很多人来找我，非常多的人找我，我说我不清楚。",
      "speaker": "发言人2"
    },
    {
      "time": "00:04:01",
      "text": "还是呆在原地。今天我特别想跟你聊一个话题，我们何时能实现AGI？非常巧的是我之前在知乎上看见你写过一个关于自动驾驶的帖子。我觉得你那个判断跟之后我们讨论的AGI某种程度上会有一些相似之处。我觉得待会儿我们可以详细的聊一下，就你为什么放弃了自动驾驶行业。但之后我记得其实在2017年的时候，你刚刚就我们提到的这个维持开源项目ELF open go也是基本上当时人工智能界最火的事情。因为当时谷歌的阿尔法狗alpha zero其实是大家看到了人工智能的一个大的突破的。当时你也是在研究人工智能怎么下棋的这样的一个问题。",
      "speaker": "发言人1"
    },
    {
      "time": "00:04:42",
      "text": "对，1718年的时候我们也在做AI for games，就是AI for游戏这样的一个方向。其实这个方向很早就做了，大概2015年的时候我们就开始做。我刚刚进fair的时候，这个项目就是我的第一个主要的项目。",
      "speaker": "发言人2"
    },
    {
      "time": "00:04:55",
      "text": "对，做围棋。当时我们做了一个boss叫dark forest黑暗森林name。这是刘慈欣的三体的第二部。当时我是三体的粉丝，当时是在阿法狗出来之前，我们的boss还是比较强的，能够跟当时最强的软件打个差不多平手。当然没到职业水平，但是还是很强不错。但是我们也是用神经网络做策略，做policy network，来做那个策略网络，得到一些比较好的结果，就是他们能够跟当时的最强的那个软件，他们花了十年打磨的水平差不多，让我觉得非常惊讶。",
      "speaker": "发言人2"
    },
    {
      "time": "00:05:29",
      "text": "其实你提到了你做dark forest的经历，我想到你还有一个身份，就斜杠青年。你其实也是一个科幻作家。",
      "speaker": "发言人1"
    },
    {
      "time": "00:05:37",
      "text": "对我从0607年的时候，应该说更早，05年时候开始写的，当时写的非常烂，对吧？你看到网上的小说，那些都不是一开始写的，后面有空会写一点小说吧，我觉得还挺有意思。因为对我来说就是不靠谱的想法就扔进小说里面写，靠谱的想法就来做科研。我觉得这样其实是一个比较有趣的一个组合。因为我也想过很多很多想法，但是不可能每个想法都最终能达到一个能发文章或者说能够产生影响力的这样一个一个一一个结果，所以有一些非常crazy的，非常天马行空的想法就可以把它放在一起。也许就有一天就也会有一本小说出来。",
      "speaker": "发言人2"
    },
    {
      "time": "00:06:19",
      "text": "对，其实我挺好奇的，研究过自动驾驶，研究过下围棋，包括当时你也做了dark forest的策略的研究。我挺好奇现在你是怎么转过来研究大模型的，而且最近你们团队很高产。",
      "speaker": "发言人1"
    },
    {
      "time": "00:06:32",
      "text": "对，这个中间也经历了很多的过程。大概18年我们的open go出来之后，19年发了文章之后，其实再往下这个方向就没有再做下去。一个是因为团队里面不同的人有不同的想法。Open go的最后一组Larry他现在做AF science，他有一些关于那个AF size的一些方向，比如说chemistry，像他们的catalyst这个project，每个人想法不一样，有些人已经走了，所以后来也就没有再继续下去。",
      "speaker": "发言人2"
    },
    {
      "time": "00:07:00",
      "text": "这对我来说，我当时其实有很大的一个初心，想要去理解神经网络到底为什么工作，他为什么能工作那么好。就是关于神经网络的理解，当时我就在想，其实我应该做这个。所以我在18年19年之后，就没有再做围棋，而是去做这方面的工作。所以如果你看我的这边的publication list，大概从19年开始，20年有很多的唯一做的文章是关于如何对神经网络的理解，就为什么他能够搞得比较好，这个话其实我们做了很长一段时间。其实当时19年的时候，OpenAI的伊利，他其实跑过来找过我的他跟我说有没有兴趣。当时我说我加入OpenAI.",
      "speaker": "发言人2"
    },
    {
      "time": "00:07:43",
      "text": "他当时对向你发出OpenAI的邀请。对。",
      "speaker": "发言人1"
    },
    {
      "time": "00:07:45",
      "text": "其实我以前背过一个包，就是Y120的包。那个包其实是当时我参加他们的活动，他们送给我的。他们当时有那个welcome manator有一些活动，当时伊利亚是请了我去了一次，然后问了我到底要没有兴趣。当时跟我说的是他们想做language model大语言模型，说我想做的是如何理解神经网络为什么能够work。当然你们谈不拢了，所以我就没有去。当时是有这个有趣的一个故事。当然了，最后大约模型是起飞了，都是在2022年的时候，22年的时候起飞，那时候就是真正的我就感受了，确实这个方向非常的有希望。同时这个方向其实也可以跟我这边的一些对神经网络的理解，其实是可以结合起来的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:08:29",
      "text": "去年我们发两篇关于如何理解transformer的工作，一篇叫scanner snap有一篇叫dma两篇都是我一做，其实就是说把我如何理解神经网络和大元模型结合起来了。因为transworld现在就成为了新一代的王者。很多很多网络，很多很多的神经架构现在都开始用transformer来作为这个方向。对这个东西进行理解，其实是一个很重要的一个问题。同时也可以通过这个可以得到更多的一些对于神经网络的改进的想法和建议。像这次的两篇文章，一篇是mobile IN还有一篇是glory。这两篇其实都跟我们之前的一些对神经网络的理解和分析是有很大的关系的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:09:10",
      "text": "Glory现在还是很火，前两天我们发了之后，网上有很多很多的反馈，包括我的微信也有很多人找我问一下接下来我们怎么做，或者说我们对这个东西的看法是什么样子的。已经有开源的社区重复了我们的工作。有意思。对对对，而且确实确认了我们的一些发现，比如说怎么省内存，确实可以跑，而且速度还可以，效果也不错，所以这个让我非常开心。",
      "speaker": "发言人2"
    },
    {
      "time": "00:09:37",
      "text": "这篇文章其实有很大的一个motivation是源于比如说2020年我们的一篇做网络分析的一些文章。这篇文章当时被拒绝了，被哪拒绝了？当时被艾克利亚拒了，同时也被ICM拒了。连续两次之后就是没有再投，因为已经灰心丧气了，就一直放在archive上。但这篇文章的两部分后来起了很大的作用，有一部分变成了去年我们这篇drama的一个文章的分析的基础。另外一部分它的分析的能力和它的一个思维方式，就变成了gera这篇文章的一个主要的foundation。所以我觉得这个很重要，就是基础研究对于整个领域的一个促进作用，还是要很后面才能看出来的。要过个43到4年的时间。",
      "speaker": "发言人2"
    },
    {
      "time": "00:10:19",
      "text": "对你刚刚的这一段话里面，我有太多好奇的问题了，我觉得一个一个来。首先是你提到了你的盖肉的这篇文章，我知道这篇文章确实在行业里面引起了非常大的反响。可不可以这样简单通俗的跟听众解释这篇文章其实你主要讲的是怎么样在一个24GB的消费级的GPU上去运行一个7B模型的可行性。",
      "speaker": "发言人1"
    },
    {
      "time": "00:10:42",
      "text": "可以这样理解吗？对，这个是一个。但是该有主要的focus，还是能不能在4090的卡上去训练。",
      "speaker": "发言人2"
    },
    {
      "time": "00:10:50",
      "text": "训练对。",
      "speaker": "发言人1"
    },
    {
      "time": "00:10:51",
      "text": "推理其实很多已经做到了。如果有一个模型，大公司或者说大团队帮你训练完了，你可以把它弄下来。然后可以在手机上，可以在你的个人电脑上进行推理。这个不是新鲜事，大家都在做，而且已经做得很不错。但是如何能在比较小的消费级卡上能够做训练，甚至是预训练，就是从头开始训练。那个其实是之前一直可以说是大团队或者说大算力的团队的一个独有的一个能力。对于消费级显卡来说，或者说对于你GPU破这一族来说就是比较难。但是这篇文章可以让大家看到了一个希望，如果有很多的这样的卡连在一起的话，也许也能做到大模型预训练的一个效果。",
      "speaker": "发言人2"
    },
    {
      "time": "00:11:35",
      "text": "当时我们现在预测算下来就是单卡4090训练一个7B的模型可能要110天，非常长对吧？但是也许如果你有很多卡的话，那也许会有并行。因为我们现在能够把整个7B模型放进24GB里面的话，那就意味着模型之间的不同层的一些交互，其实在单卡里面可以进行，就不用出现跨卡交互，高带宽交互的这样一个情况。这个其实会大量的省带宽。如果你有很多4090的卡的话，也许他们可以通过PCIE或者说通过AN net就能够连起来，然后才能够训练完。这样的话就有可能会发生这样的事情。",
      "speaker": "发言人2"
    },
    {
      "time": "00:12:14",
      "text": "我跟大家解释一下，4090是英伟达给大家用来玩游戏的显卡。它很难跟A100H100的算力相比，但是它也是一款可以说是性价比非常高的卡。简单来说是大家在训练的时候可以更节省显卡了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:12:30",
      "text": "对，他就说可以绕过以前的一些限制。比如说为什么现在显卡那么贵，主要一个是n vida现在有NV link，那个v link是有很高带宽，就是它可以提供很高的带宽。是因为一个模型的不同的部分放在不同的卡上，所以他们每次在训练的时候，他们之间是要交互的。这个交互其实是有很大的带宽要求，比如说几百GB每秒的速度跟交互。但是如果一个模型的所有的权重都能放在一张卡上，那么也就意味着他们在进行梯度迭代的时候，内部的计算可以在这一张卡上进行卡和卡之间的交互就可以大量的节省。这样的话也许有一些现当前的一些范式就会发生改变。",
      "speaker": "发言人2"
    },
    {
      "time": "00:13:09",
      "text": "你觉得哪些范式会发生改变？",
      "speaker": "发言人1"
    },
    {
      "time": "00:13:11",
      "text": "比如说现在有很多的mode parallel模型并行的一些方案，像FSDP这种方案，你要保证模型的一部分放在不同的卡上，模型太大了。比如说需要大量内存，比如需要100季内存，但是你每张卡只有40季或者只有24季。你怎么样把内存分配在不同的卡上，把权重分配在不同的内存上，然后让他们充分的高速的交互，让这个训练变得很有效率。但是如果有了这个方案之后，也许一张卡上可以放更多的权重。这样的话它的训练的这个过程就会加速。",
      "speaker": "发言人2"
    },
    {
      "time": "00:13:43",
      "text": "你是怎么做到的？",
      "speaker": "发言人1"
    },
    {
      "time": "00:13:44",
      "text": "我们在算法上做了一些改进。关键的一个点就是说因为大家都知道lora权重太多了。那么一个自然的想法是我们把权重经过重参化、reprimand、sation, 然后把权重变成小的矩阵相乘，这个叫低质矩阵分解。分解之后，权重的里面的参数的数目变少了。这样的话我的训练就能够把内存的要求降下来，这是罗拉的一个思想。",
      "speaker": "发言人2"
    },
    {
      "time": "00:14:09",
      "text": "那罗亚是有问题的，问题在哪里呢？就是他一个他不能用来做预训练，特别是在一开始的时候，用柔软直接去训练，会导致爆炸，会导致有各种问题。就是训练出来效果肯定没有全参数训练要好。这样就导致一个瓶颈，就是说你要省内存，你就不得不牺牲一些性能。",
      "speaker": "发言人2"
    },
    {
      "time": "00:14:27",
      "text": "我们怎么做到呢？就是说我们的观点是权重本身不应该是低质的，不应该是low rank的。但是它的梯度的迭代，这个梯度是可以是low rank的，这个是不一样的。对，因为其实大家可能都想的是权重本身是有这个性质，其实权重一定有这个性质。但是我们可以证明甚至可以证明出来梯度的迭代，梯度本身是low rank。因为梯度是low rank的话，那梯度对应的一些相应的内存开销都可以是low rank的。包括adam的一些状态，它的momentum有些virus这些东西都可以是low rank的。这样的话就一下子降了很多的内存。",
      "speaker": "发言人2"
    },
    {
      "time": "00:15:02",
      "text": "这样的话甚至是你比如说你要训练一个7B的模型，如果你用一般的方式训练的话，你至少要40GB以上的内存。你一个7GB的卡永远放不下。但是用我们的方式的话，我们就可以把它砍掉一半。比如说18到20G就可以放进去。这样的话就会让4090比如说重获新生。比如说通过这个方式在算法上可以改变计算的过程，可以让这个训练变得更有效率，更省内存。",
      "speaker": "发言人2"
    },
    {
      "time": "00:15:26",
      "text": "非常简单粗暴的理解，它其实是一个算法的改进，而不是说你加进去的预训练数据质量的提高。",
      "speaker": "发言人1"
    },
    {
      "time": "00:15:34",
      "text": "是的，是数据另外一回事情。因为这个改进跟数据是平行的关系，或者说是solo onal一个关系。那么数据还是可以再往里面加，那么效果也可以更好。但算法本身如果效果更好的话，其实跟这些数据的改进是叠加的关系。",
      "speaker": "发言人2"
    },
    {
      "time": "00:15:49",
      "text": "这篇文章的目的不是说要训练出一个跟现有的算法相仿的7B的model。我们这边实验就跑到了这20个BDM的token。但是算法如果你真的要训练一个比较好的7p model，至少要一个缺点，或者说是反正是3到5个吹脸，或者2到3个吹脸这个数量级。对，所以现在还没有到这一点，希望以后如果有机会还可以再往下做下去。",
      "speaker": "发言人2"
    },
    {
      "time": "00:16:13",
      "text": "你还有一篇论文是mobile LM，要不要跟大家简单解释一下这篇论文的主要思想？",
      "speaker": "发言人1"
    },
    {
      "time": "00:16:19",
      "text": "这篇论文就是说是我们能不能把全球网络运行链的财务数量压得更低。我们这边用的是350幂恋的更小的神经网络。在这个小神经网络下，我们是不是能够训练出更好的模型？这个模型的能力肯定没有大的模型效果那么好。但是小模型到什么程度，这个本身是一个很有意思的一个问题。这篇其实主要我这边是作为一个adviser这样的工作。因为这篇主要不是我们菲亚组的工作，这边是reality lab他们组的工作。我作为一个external adviser给他们一些建议，包括层和层之间自己可以共享参数这些建议，这个都是当时我们讨论出来。",
      "speaker": "发言人2"
    },
    {
      "time": "00:16:57",
      "text": "对我稍微穿插一下那个问题题，因为我一直都很好奇我们何时会达到AGI。其实业界有两种观点，一个是仅仅通过增加模型的规模，通过scaling law的形式实现ai那其实还有一种观点，仅靠扩大规模，他一定会遇到瓶颈的，就是他是不够的。你更倾向于哪种观点？",
      "speaker": "发言人1"
    },
    {
      "time": "00:17:20",
      "text": "一般很多人这种问题，我永远会倾向于，我觉得我们现在离AGI还差几个breakthrough。我觉得现在直接scale不一定能有效果。而且我会觉得可能真的过一阵子大家会发现scale慢慢它给你的benefit越来越小。",
      "speaker": "发言人2"
    },
    {
      "time": "00:17:36",
      "text": "因为c in law是什么？CN law本来它其实就是一个promise，是什么？就是说如果你的算力乘以2，你数据乘以2更容易乘以2，那么你的部分是一定会上升一个固定的百分点。",
      "speaker": "发言人2"
    },
    {
      "time": "00:17:47",
      "text": "但这个其实不是一个好事情。New network可以有这个skin law，但是同样的news sneider bor最近一个方法，用检索方法其实也可以有这个skin law一样的。就是说数据量越多formers越好，所有模型都是这样子。所以其实我们并没有完全理解。为什么现在的transformer或者说拉智能model这样的一个方案有那么好的效果，其实并不是很理解。而且这个skin的走上去之后，就意味着这跟自动驾驶是一样的，他们最终会达到这样的一个curve。",
      "speaker": "发言人2"
    },
    {
      "time": "00:18:16",
      "text": "这客户是什么意思呢？我当时在自动驾驶2017年的那个帖子上这么画过。一开始大家都非常的激动，说，我加点数据效果就这么好了。人类就马上就要迎来新的春天了。但是最后你会发现数据加的越多，它的普通分词提升就越难以被人发现。然后最后它离人类最后那根线可能还差一点，最后可能会有这个问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:18:38",
      "text": "那么对宗驾驶来说，它的这个问题是数据会越来越难获取。主要问题是在这儿。因为如果你开一个100外卖都没有任何交通事故的话，那就意味着你每100万卖才能收到一个数据点，这个效率是非常低的。这样的话你的收益效率越低，最后的结果就是你永远达不到人类的水平。对大语言模型来说，这个情况会好一点。因为数据很多时候不需要从各种事故中搜集，还有很多的数据能在网上能够用到。",
      "speaker": "发言人2"
    },
    {
      "time": "00:19:07",
      "text": "但是也是有同样的问题。也许我们会发现，比如说再过十年，有90%或者80%的日常的一些行为，我们都可以把它们建模的很好的。但还有20%或者10%这些行为，因为它是可能每个人独有的，或者说是私有的。那么这些数据哪些是拿不到的？因为拿不到的话，那也意味着就是拿不到，我就没有办法用它来训练模型，模型就不会真正的去理解这些情况。这个其实是一个很大的一个问题，这当然是一个了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:19:43",
      "text": "然后另外一个就是说大家可能会意识到数据会变得越来越重要，大家也不太愿意真的把数据分享出来。这个两方面的因素就会导致有问题。最终可能会发现就是数据越来越难获取数据难获取之后，你的模型就会变得越来越难，变得更强。所以这两个其实是有一些一致性的。所以我觉得而且你就回想一下，对人来说，比如我跟你对话，我能很快的去理解你的处境，你的状态，也不需要你的大量数据这种理解。但是机器就不一样。所以这种情况下，其实人在这方面还比机器要更多一层的理解能力和深度。在这方面其实我们现在还没有看到。",
      "speaker": "发言人2"
    },
    {
      "time": "00:20:21",
      "text": "其实刚刚你分析自动驾驶的这个思路特别好，这应该是我看过的最好的一个对自动驾驶非常独立思考，又用数学逻辑讲何时能达到完美的。我说的是完全的无人驾驶的种状态的一个分析思路。所以你觉得这个思路在AGI领域也是存在的对。",
      "speaker": "发言人1"
    },
    {
      "time": "00:20:39",
      "text": "也是存在。当然宗教是有它的特殊性，因为自动驾驶一个是100%不可靠，完全不能犯错。那个要求其实比大元模型要强多了。大语言模型没关系，你做错了大家想想就好了。他还是不能帮你改稿子，还能帮你总结各种文章，还是能帮你提供各种建议。所以他这种属于更多像是创新型的。如果没有什么好建议没问题，没有损失，有了好建议是更好。但是对增驾驶来说，如果没事儿是你的expectation，有了事儿就分的问题，这个是不一样的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:21:10",
      "text": "对我理解你是更倾向于第二种观点的。其实AGI不是skin零落，它可以无限的增长下去，而是它一定会遇到瓶颈。在这个瓶颈中怎么样依靠其他的我们跳出现有的技术范式来去实现它，反而应该是值得思考的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:21:27",
      "text": "对我觉得是这第一个依据。就是说人类在从小到大的过程中，他其实没有那么多数据给他喂的，但他还是做的很好。你看我们女儿怎么样学会各种技能，你会发现这些对她学会这个过程，一个是不需要人工干预，二是他学的速度是非常快的。你可能昨天他还不会拍这种小岩石，今天他就会喷了。他大概几个月的时候给他放在楼梯上，他从来没见过楼梯，但他十分钟之内就会往上爬，这个其实是非常impressive。",
      "speaker": "发言人2"
    },
    {
      "time": "00:21:56",
      "text": "也许有人会说很多东西是刻在基因里面的这是有可能的。但是他的学习能力也是非常强大，应该是存在一个新的学习算法，而且这个学习算法应该是远远超过现有的效率。我们现在只是摸到了一个就是scratching the surface，就是我们现在只摸到一点点。我们通过一些奇奇怪怪的组合，运气很好的碰到了一点点的皮毛，我们就已经打出那么大突破了。所以可见这个东西的潜力非常大。",
      "speaker": "发言人2"
    },
    {
      "time": "00:22:23",
      "text": "所以你在做glory的模型的时候，其实你已经是在思考这一层的问题了。怎么样在算法上的提高，让他的预训练更好。",
      "speaker": "发言人1"
    },
    {
      "time": "00:22:33",
      "text": "对，这个不仅仅是改造这篇文章，应该说这个是go throughout the entire research career。因为我整个research career或者一个很大的核心就是我怎么样去理解神经网络是如何工作的。根据这个理解，怎么样找到更好的算法，提高它的工作效率，这个其实是一个最大的一个方向。这个方向上我们有很多的一个是理解，一个是分析。然后用这个理解和分析，我们运用在现有的算法上，然后把它变成更好。",
      "speaker": "发言人2"
    },
    {
      "time": "00:23:00",
      "text": "比如说围棋，包括之前我们其实还有一篇文章也是挺火的，叫search former。对我知道那个对，那篇其实也是花了一些时间去做，这篇其实是transformer。现在的问题是他没办法做大长程的推理，他的推理力非常差。比如说让他去玩井字棋，你发现他不行。我也试过，包括最新的crowd 3玩1下发现也不行。他说的非常好对吧？但是他下的棋是错的那我跟他说，我应该下这个，下这个我们就赢了。他说好像你是对的，对他他就是他还是没有办法做这些非常简单的一些比较复杂的游戏式的推理。我们的search form AA这篇文章，通过先模仿传统的推理算法，优化算法、planning算法的过程，我们可以达到一些水平。所以就说我们一个很大的一个关键的一个思路，就是我们能不能找到为什么神经网络能够work的原因，然后能够用这个去理解去改进现在的算法。",
      "speaker": "发言人2"
    },
    {
      "time": "00:23:55",
      "text": "你刚刚提到了，其实你一直在研究神经网络到底是怎么工作。我自己对这个问题也非常的好奇，我把它具象成一个更加具体的问题。我知道你研究transformer研究的非常久，而且非常深。你要不要先说一下为什么在这么多条路径中，包括像OpenAI它的训练，它是用transformer的架构来训练完成的，他的优点是什么？他为什么走出来了？他现在的瓶颈跟缺点在哪里？",
      "speaker": "发言人1"
    },
    {
      "time": "00:24:21",
      "text": "首先open I用transworld不是他们的原创了，因为传颂的一开始是从google里面出来的。我觉得他们一开始的想法应该是对于GPU的计算的一些深入理解。因为transformer的一个好处是它可以有很大的并行能力。它比如很长的序列，我可以用self attention这个机制同时计算这序列里面所有两队的token他们之间的一个similarity，然后用它来算attention。我觉得这个原因可能是因为goole内部做GPU或者做GPU可能发现算力的价格远远比通信的价格便宜。既然算力那么多，那为什么我不能设计一个模型让这个算力获得更大的优势？那好，我们干脆就把所有的talk放在一起，然后我们让他们做powis的那个inner product，这个应该是他们的motivation。他们发现效果特别好，速度没有慢太多，所以他才有transformer这样子的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:25:11",
      "text": "但是其实后来发现transformer的skin能力非常好，往里面喂数据，这在数据多的情况下，它的效果也确实比以前现有的一些方法要好像CNA那些方法。它确实有在有些方面上不一定比得过transformer。因为CNA有一些预设的立场，他比如说第一层感受也非常小，这个对vision是对的，我先把最小的一些特征拼起来，然后再拼起来。但对于suma来说，它没有这个预设立场。那么就意味着数据多的话，它这个效果就会有更好的提升。",
      "speaker": "发言人2"
    },
    {
      "time": "00:25:40",
      "text": "它相当于是一个典型的用算力来换预设立场的例子。因为你要看人类社会的发展或者研究的发展的话，一般是这样。计算资源比较低的时候，就需要人类的大脑去想到一些比较好的模型，在这模型上找到最好的解。但算的越来越多的情况下，人类的一些条条框框就要被打破了。慢慢的人类把调整框架打破之后，把这些模型的建立的方式让人计算机自动去发现。这个可能就是传送一个在宏观上的一个很大的故事。",
      "speaker": "发言人2"
    },
    {
      "time": "00:26:11",
      "text": "对缺点对缺点当然是它需要大量算力。还有一个就是它的速度也没有那么快。就比如说你要在车上，你用transformer，这个其实是比较难的。其实如果你要做无人车，或者说做那种low latency y的延迟太高了，它延迟比较高。这种时候很多人就说你还不如就用CNNS net。它的效果还是不错的，也有很好的低延迟的一个特性。然后它算的也不需要太多，这个是一个很大的一个difference，这个都是有锤子off的。但是transformer作为头牌，作为AGI的希望？肯定大家还会继续往下挖下去。",
      "speaker": "发言人2"
    },
    {
      "time": "00:26:47",
      "text": "对我为什么刚刚花那么多时间在transformer上？是因为我其实在想，我们达到BCI的方法是不是transformer这一条路径。因为我知道现在有很多像新的路径在做，比如说像raku，但我知道他可能在处理图像跟视频的并行计算上，可能没有transformer表现的那么好。刚刚其实我们也提到了，仅仅通过增加规模，你要最终实现AGI是很难的。所以我们要找到新的范式。所以我不知道你有没有发现一些新的范式。",
      "speaker": "发言人1"
    },
    {
      "time": "00:27:18",
      "text": "整体上我是在想这样一个问题，就是在做学术跟科研的时候，什么时候大家应该是在一个小问题上不停的精进优化，就是我们把各个细节做到更好。同时另一个方面，就有的时候我觉得大的突破，它并不是说在改进细节上做出来的，而是说thank you out of the box。这是我们怎么又跳出来从本质上质疑这件事情来。",
      "speaker": "发言人1"
    },
    {
      "time": "00:27:41",
      "text": "达到的这是很好的问题。就是exploration和exportations的问题。强化学习里面一个根本性问题，一个是说我什么时候应该做探索，我什么时候应该利用现有的优势获得更大的利益。这个永远是一个好的problem。",
      "speaker": "发言人2"
    },
    {
      "time": "00:27:55",
      "text": "我觉得对研究员，对研究来说，很多时候是多线并行的，不是说我就盯着一个东西，因为你谁也不知道将会发生什么事，也许陈思莫会继续霸榜十年，或者说明天他就被一片新的阿凯干掉。你并不知道我将来会发生什么样的事情。对我来说，我觉得大的逻辑就是我们还是要从第一性原理出发，这是我的一个风格。我永远会非常喜欢从T恤人说就是OK。我对世界上所有的人说的话，我都不一定会100%相信。",
      "speaker": "发言人2"
    },
    {
      "time": "00:28:24",
      "text": "我们相信的是什么呢？是如果我们要出分析，如果像那些数学家一样，从最简单定理出发，OK这些数据之间是有相关性的。什么样的模型能够很高效的去模拟这个相关性，把相关性提取出来，用这个相关性去预测将来的一些事情。有这些东西之后，你就会自然然会构建出它整个框架应该是什么样子的。根据这个你再去找很多文章去验证。就我每看一篇文章，我不是说是真的去100%的把他们的所有细节都记住，这个没有意义，因为那么多文章根本看不完。我觉得我做的很多事情是这样，去看这篇文章，然后去看这篇文章里面哪些观点或者哪些现象和我心里面想的对神经网络，对传送的理解是不是契合，或者什么地方是让我觉得我的理解是错的。我应该改变我的想法，通过这个方式来看文章。",
      "speaker": "发言人2"
    },
    {
      "time": "00:29:13",
      "text": "最近有看到什么好的观点吗？",
      "speaker": "发言人1"
    },
    {
      "time": "00:29:15",
      "text": "没有特别surprising的。现在对，现在还是说在这个方向上，我们继续往下去。你像sora是一个很surprising的direction，这个是一个很有意思的一个就是这个效果确实比我能想象的要好好不少。但是也有很多的文章，比如说有些文章跟我们的研究方向是很契合的那我们就要去看一下这些文章在做些什么事情。比如说能够加速推理的那些文章我们去看。因为我们去年有一些文章是加速推理的神内存的文章，或者说一些对神经网络的分析的文章，我都会去看一下。这篇文章其实跟我研究方向是有关系的。他们号称比如能证这个东西能证出来，或者说有一些新的现象和观点，我也会去看一下。",
      "speaker": "发言人2"
    },
    {
      "time": "00:29:54",
      "text": "刚刚你正好提到了sora，我们也还蛮关注sora的。你觉得他让你最惊艳的点是什么？",
      "speaker": "发言人1"
    },
    {
      "time": "00:30:00",
      "text": "首先第一个就是我这边不是做扩大模型的。我很早以前做计算机视觉，但是现在也不做很多年了，可以这么说。我已经不是一个一线的，在所有方面是一线的，我肯定不是我当然可以给一些建议或者给一些想法。我觉得最爆的点就是它确实效果非常好，最让我觉得surprise就是它一致性非常好。",
      "speaker": "发言人2"
    },
    {
      "time": "00:30:19",
      "text": "你说的是所有的生成，现在市面上放出来的那些，还是说它的某几个demo.",
      "speaker": "发言人1"
    },
    {
      "time": "00:30:25",
      "text": "应该说所有他生产出来的东西实际性都非常好。这个不是说是一个两个比较好，但是我觉得基本上所有的拿出来都是非常好的，都是一个完整的场景。然后场景的前后人物的表现和他的穿着，还有各种行为都还是比较相似，非常的consistent，这个是非常强的，这个是为什么让我觉得非常surprise的原因。",
      "speaker": "发言人2"
    },
    {
      "time": "00:30:48",
      "text": "然后你去看它的技术报告，你可能会发现它并不是预测下一帧，通过这个方式预测出来。它是相当于把整个视频看成一个大的image，然后它有镶嵌了一个3D的音。这3d image我们通过diffusion mode一点defuse出来。这样的话确实可以保证一个consistency。因为整个图东西是一起出来的，它不是像predict next token一样，就是我predict一个、两个、三个、四个，然后我predict不是60帧，然后慢慢pret 100帧，慢慢把这个视频生成，这个会有问题。因为你预测到最后，你们慢慢你会发现有些人就走形了，或者说有些一致的概念不一致了，然后他会去别的地方。但是sora在那个latest space上做这个diffusion，他对整个image CD的special temporal volume做diffusion。把这个问题提到了新的高度。",
      "speaker": "发言人2"
    },
    {
      "time": "00:31:37",
      "text": "保持一致性有多难？或者说它跟时间是有关系的。因为比如说我们看像runway，像皮卡，他们是3至4秒，最多extend到10秒的视频。我看了所有的demo，它只有在东京街头那一个视频是60秒，其他的视频可能也是20秒左右，还是10到20秒之间，还有8秒的对吧？它并不是所有的视频生成出来都是60秒的，但是我想知道保持一致性，你看的是一个多大的时长，然后保持这个时长的一致性有多难？",
      "speaker": "发言人1"
    },
    {
      "time": "00:32:05",
      "text": "我觉得20秒内它的各种变化是非常大的。比如伊朗有一个还是让我非常impressive的，就是那个反光。就一个人在车里面，就像东京的新干线的街头，他站在那边看外面的场景，不定期的会看到外面，但不定期看到他自己的反光。它自己的反光在不同的时间段都是一致的，这个是一个非常让我非常surprise的情况。",
      "speaker": "发言人2"
    },
    {
      "time": "00:32:25",
      "text": "因为如果你只预测下一帧的话，你很有可能会发生这一帧。你测出来反光，下面的反光就是不一样的。但是它能做到两次的反光是一样。尽管这两个chunk比如隔十秒钟，他们还是能保证这个反光是一样的。所以这个其实是让我非常surprise的一个点。",
      "speaker": "发言人2"
    },
    {
      "time": "00:32:40",
      "text": "20秒或十几秒其实倒不是问题，问题还是在于这里面的视频里面有多少内容。这个内容是不是在经过很大的变化之下，它还是能够一致。因为它会看到很多这种镜头翻得非常大，整个人在很多的之前有一个视频，就是一个猫在一个废弃的垃圾箱走来走去，整个走路的过程是变动是非常大的，视角变化也非常大，它整个猫的形态还是没有发生改变，所以这个是非常impressive的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:33:08",
      "text": "所以sora是世界模型吗？",
      "speaker": "发言人1"
    },
    {
      "time": "00:33:11",
      "text": "这个是一个很好的问题，因为世界模型这句话是非常广泛的对我们先定义世界模型，什么叫世界模型？就是你只要预测下一帧，或者说你对将来有一些看法，也都是世界模型。三岁小孩说今天晚上我要去外面吃饭，这个世界模型一样的。所以我不觉得这个词这么高大上，其实你可以用来做世界模型对吧？",
      "speaker": "发言人2"
    },
    {
      "time": "00:33:32",
      "text": "你可以说我把前面几帧定下来，给定前面几帧之后，把后面的东西拿过来做的fusion，然后得到一个consistent video，这个就是一个世界模型，这个都可以。而且索拉可以做反过来的世界模型。我记得他有一个视频是给定后3分之1的真生成三个不同的视频，他们最终都会收敛到最后后3分之1。真最后3分之1真是说一个电车开进了三藩的闹市区，但是开的过程可以是不一样的。他可以说这个电池先从空中降下来进入闹市区，或者说从另外一个地方开不进来。然后最后它都会无缝衔接到最后的开进闹市区这个动作。",
      "speaker": "发言人2"
    },
    {
      "time": "00:34:10",
      "text": "所以去说索拉可以做任何的补全了。你去掉一些针，然后把其他针补全，它都可以做对。所以这种人来说，它确实是个世界模型。但是另外一个问题就是说这点上我同意一样的观点，就是它在物理上是有问题的，很大的问题。一个玻璃杯掉下来之后摔碎了，但是没有摔碎的过程，它直接会变成碎掉的状态。",
      "speaker": "发言人2"
    },
    {
      "time": "00:34:30",
      "text": "是因为数据的问题。就比如说玻璃杯掉下来，大家能看到碎成一个渣滓，这个是经常我们在图片或者视频中能看到的。但是它掉下来摔碎的这个过程，其实是我觉得在人类的生活中，它也是很快发生的，我们不太容易去捕捉到它的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:34:45",
      "text": "是的，对我觉得其实就是这个原因就是说一个数据不够。然后另外就是说这个物理过程非常难模拟，在机器人那边其实有同样的问题。机器人那边其实也要做世界模型，对吧？你要预测下一帧这个物体会在哪儿，它跟其他物体有什么样的交互，就预测的物体在哪非常简单，因为人家你只要套你的物理模型就行了，你把牛顿的定律拿进去套一下就好了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:35:08",
      "text": "但是他一旦跟别人物体交互的时候就会有问题。因为交互的动作它的变化是非常快的，交互的数据也不是特别多。最后他学出来的模型就质量就不好。但质量不好的问题就是而且模型变动一点点，它最后的输出也是完全不一样的。这些因素综合在一起，最后导致世界模型或者说特别是交互。",
      "speaker": "发言人2"
    },
    {
      "time": "00:35:28",
      "text": "比如说一个手砸在这个桌上，这个交互对吧？这个交互其实是很难的，因为在砸下去的一瞬间，你受到的力从零突然变得很大，这是非常小的，在很小时间内发生很大的变化，这是一个两个物体之间相互交互。比如说你要看两个人打斗，我觉得这个对苏老师的兴趣其实会比较难。",
      "speaker": "发言人2"
    },
    {
      "time": "00:35:48",
      "text": "我看见索尔它有一个是两个船在一个咖啡杯里面运转，那个也是很惊艳的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:35:54",
      "text": "真的是很惊艳的。就是说船和船之间是没有关系，船和里面的水是有关系的对。",
      "speaker": "发言人2"
    },
    {
      "time": "00:35:59",
      "text": "所以它也是会涉及到力学的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:36:01",
      "text": "它会涉及到一些这个我相信是因为网上有很多这样的视频，就是有很多水跟物体的模拟有视频，那么就会学到这个能力。但是如果数据不够的话，可能就比较难。",
      "speaker": "发言人2"
    },
    {
      "time": "00:36:13",
      "text": "我把我的问题稍微再拆解一下，就我把世界模型这个词儿拆解一下，你觉得索尔现在是不是能够理解世界运转的规律，把握物理法则，记忆检索信息，还有逻辑推理或者行动规划的能力。",
      "speaker": "发言人1"
    },
    {
      "time": "00:36:26",
      "text": "我总感觉像把sora当成一个全能的模型，我觉得他也要分不同的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:36:32",
      "text": "你说你觉得他这。",
      "speaker": "发言人1"
    },
    {
      "time": "00:36:33",
      "text": "方向其实挺难的，因为生成视频和预测物理世界是两回事。因为生成视频完全生成一个是而非的，看起来很有道理，但其实是不对的，是非常正常。因为物理世界对于视频来说最重要是好看，最重要不是说是它真实。对，有可能会这样，就是物理世界上有一点点发生小小的变化，这些小小的变化真的模型没有预测到你的planning计划，统筹能力就会变得非常差。因为我觉得他planning跟这个能力应该不会特别擅长。因为有很多的小的物理误差，它累积起来之后就会变得非常糟糕。我觉得这个是一个问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:37:13",
      "text": "但是逻辑其实很难说。就逻辑的模型你说话更像一个创作模型。不是，他可以创造出一个视频，对他来说是最熟悉的。但是如果你要考他一道题，这道题他没有见过，或者说他不太有把握，他不一定能回答的特别好。所以我觉得这个其实很难，不要把它当成一个万能的一个东西。我觉得他可能确实是往前走了一步。但是真的你要有万能的模型的话，你可能还需要很多很多的工作。",
      "speaker": "发言人2"
    },
    {
      "time": "00:37:40",
      "text": "其实我们过去提到大模型，大多数时候说的都是大语言模型。但我知道闫罗坤他有一套理论，他的理论是全脑模型。人体他觉得不仅仅是语言，他觉得可能也要用感官去认知世界。你怎么看这两个？其实我觉得这个也可以算是我们刚刚讨论的到底是thinking in the box earth，thinking out of the box.",
      "speaker": "发言人1"
    },
    {
      "time": "00:38:02",
      "text": "我觉得对央来说，他当然是希望起一个很大的大框架，包括里面所有东西都有。能用感官就是bring your perception，或者说employed的AI用感官去跟这个世界做交互，然后得到一些信息，这也是很重要的。因为对人来说，如果就给他看幻灯片，看看电影的话，他其实对他的学习或者对他的工作，他的进步不会有太大的反响。那么对人来说最重要还是能够对跟物体进行交互。这个就强化学习这部分内容，就是我看到一张纸，我可能要相互翻来翻译，未来会翻。我可以想知道就是有一些新的假设，你从你脑子里冒出来，你要从探索中把这些假设解决掉。我觉得这也是很重要的这也是exploration expectation这样的一个很重要的一个点。",
      "speaker": "发言人2"
    },
    {
      "time": "00:38:45",
      "text": "世界模型其实是其中的一个很重要的一个组成部分。因为你对这世界没有预测的话，你是没有办法得到你想要做什么事情的一个决策的。就比如说你看到一只老虎，你第一反应是老虎要把我吃了，那我就得跑。世界模型这边的决策是预测老虎会把我吃了。如果不动的话，决策模型会根据这个预测会决定我们要跑路，这都是相辅相成。",
      "speaker": "发言人2"
    },
    {
      "time": "00:39:06",
      "text": "的对所以整体来说它其实还是一个非常复杂的工作机制。是的。然后另外大家关于sora讨论的比较多的一点，他在做着sora的生成的过程中用了很多的合成数据，包括用到了虚幻引擎5。这个他没写他没写吗？他没写。",
      "speaker": "发言人1"
    },
    {
      "time": "00:39:23",
      "text": "OK这个我觉得有可能是因为误传。因为塑料刚出来的时候，有几个人在推特上写了一些猜测，包括我自己在上写个猜测。我说他一定是他是不是一定有引擎，他好像很多照片是用引擎生成的。当时我记得松妹四好像也有一些想法，就是开toch的一个壳方的对还包括这个饭还有一些脑洞。但是这个没有任何证实。",
      "speaker": "发言人2"
    },
    {
      "time": "00:39:45",
      "text": "没有证实他是一个分成数据的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:39:47",
      "text": "我觉得他肯定很可能是用了很多网上视频，这视频是包括了合成数据。比如说我有些人网上放一些游戏视频放在youtube上。",
      "speaker": "发言人2"
    },
    {
      "time": "00:39:56",
      "text": "我理解就是说他如果用了比如说游戏视频生成的那那些游戏视频可能是虚幻引擎5做的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:40:03",
      "text": "对，但他他自己不一定说用虚幻。",
      "speaker": "发言人2"
    },
    {
      "time": "00:40:06",
      "text": "但他没披露他的数据源。",
      "speaker": "发言人1"
    },
    {
      "time": "00:40:07",
      "text": "对他没有披露过他出去。所以这个其实是一个以讹传讹的一个典型例子。我觉得还是充分证明。",
      "speaker": "发言人2"
    },
    {
      "time": "00:40:14",
      "text": "了看第一手资料的重要性。",
      "speaker": "发言人1"
    },
    {
      "time": "00:40:17",
      "text": "对我觉得就是我当时第一个跳出来说，我感觉上这个是不是用sync data，然后你可以去看twitter red。当时资料出来之后，大概一两个小时之内就开始有用评论。所以我觉得这个不是confirm的的，是OK。",
      "speaker": "发言人2"
    },
    {
      "time": "00:40:32",
      "text": "这个纠正非常好。",
      "speaker": "发言人1"
    },
    {
      "time": "00:40:34",
      "text": "对我觉得你去看看youtube video，youtube v6很多也是合成数据对吧？游戏数据是当然恒数据了。对，他们用了一下之后，他们也可以说用恒数数据，但并不代表他们内部会用虚幻引擎生成。恒生数据不一定是他们做的，但是他们可以拿数据过来。",
      "speaker": "发言人2"
    },
    {
      "time": "00:40:51",
      "text": "那你觉得合成数据的方式去训练大模型，你怎么看这种方式呢？",
      "speaker": "发言人1"
    },
    {
      "time": "00:40:56",
      "text": "我觉得这个可能是以后的一个很大的一个趋势。我觉得像我们的three former其实用的是用合成数据。比如说我们先用传统的方法生成大量的推理步骤？然后把这步骤放进transform里面去训练？那么这所有数据都是合成的，就是通过已有的引擎去生成一堆数据，然后去训练。这种方式其实可以一个是比较有效的，能够避开现在是越来越多，越来越难找的一个窘境，我想以后应该会有很大的发展。另外一方面，可能数据有自己的一些局限性。你想之前围棋这个方向其实都是用的核数据，让围棋软件或者让AI自己跟自己下to play，那都是核心数据。",
      "speaker": "发言人2"
    },
    {
      "time": "00:41:34",
      "text": "你这个观点太有意思了，你突然提醒了我，包括网上我们用的一些训练视频，大家觉得是原始数据。其实它有可能它就是现在创作者的大量的视频，可能也是用合成数据来算的。所以我觉得是不是说合成数据跟真实数据，它的边界本身也在变得越来越模糊。",
      "speaker": "发言人1"
    },
    {
      "time": "00:41:52",
      "text": "我觉得以后边界一定会变得越来越模糊，就比如说一个抖音创作者，他可能用了虚幻引擎，再加上自己，最后搞出一个视频来，这个视频算不算个人数据，其实也算，但其实也是真实数据。因为是人加工的，所以我觉得其实没有特别必要区分这两者，因为最后可能会越来越模糊。",
      "speaker": "发言人2"
    },
    {
      "time": "00:42:12",
      "text": "所以其实在科研上大家都会用到，只要这个东西有帮助我们就都用。",
      "speaker": "发言人1"
    },
    {
      "time": "00:42:17",
      "text": "对它不是一个让大家觉得非常吃惊的一个操作。",
      "speaker": "发言人2"
    },
    {
      "time": "00:42:21",
      "text": "但为什么会问这个问题？我们觉得人类的数据是有限的，现在很多的数据已经被用来去训练OpenAI的大模型，包括各个机构的大模型了。接下来我们如果还要沿着大力出奇迹的方式，探索向AGI的路。数据从哪里来？",
      "speaker": "发言人1"
    },
    {
      "time": "00:42:40",
      "text": "对，一个方法就是合成数据了。因为合成数据就相当于用算力来换数据，你只要有无穷的算力，你就有很多很多数据，数据会越来越多，这是一个办法。当然了这个也有问题。因为合成出来的数据如果没有human intervention的话，它somehow应该还有一些比较重要的一些信息，它其实也不一定能抓住。",
      "speaker": "发言人2"
    },
    {
      "time": "00:42:58",
      "text": "就像OpenAI之前有一篇文章叫let's verify step by step，这篇是做数学推理的。他们先生成了大量的一个推理步骤，挑战数据，先让AI来决定哪数据好吧，哪个数据不好。但AI发现你可以做这些事情，但是做完之后，剩下数据还是要人去过一遍。因为剩下数据都是AI觉得很好，但其实是错的对，就相当于如果没有人类的参与的话，那AI就会在原地转圈子，他永远会觉得这个是好的，然后去推上，但其实这东西是不对的那它就没办法达到更高的那个level。所以核心数据有这个问题，像围棋的浑身数据，它的问题就在于它在围棋世界里面可以做到非常厉害，但出了世界之后就很难做到这一点。最终就是说你可以用个人数据把自己的这个能力提上去，但最终他会遇到个瓶颈。那么这个瓶颈人类能不能帮忙通过某种方式超过这个瓶颈到下个来，我这是一个问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:43:52",
      "text": "然后还有一个也是最近的一个新的消息，anthropic发了一个新的大模型。我看新闻稿里是说他在推理数学编码，还有多语言理解跟视觉等二十多个测试中，性能超过了OpenAI的GPT4。我不知道你有没有用过那个模型，自己的感你感受怎么样？",
      "speaker": "发言人1"
    },
    {
      "time": "00:44:10",
      "text": "他还是挺厉害的。但我感觉上因为我是个小说创作者，所以我会拿很多段落去测它它其实应该说是感性多于理性，就是写文章或者说写小说，或者说给小说续一下，它的细节丰富度还是挺好的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:44:25",
      "text": "你觉得他跟GPT four哪个在写小说上表现更好？",
      "speaker": "发言人1"
    },
    {
      "time": "00:44:29",
      "text": "我觉得其实是traffic好一点，我觉得但是推理上来说我不知道，就是我觉得。",
      "speaker": "发言人2"
    },
    {
      "time": "00:44:35",
      "text": "不一定就你没试过推理。",
      "speaker": "发言人1"
    },
    {
      "time": "00:44:37",
      "text": "推理其实试过一些，但是它有两方面它特别强。一个是长文的理解和分析，这个非常强我可以把我的球扔进去，然后他会给你一个章节一个章节做一些总结，总结的非常好。我觉得这个总结能力其实远远强过GPT four。",
      "speaker": "发言人2"
    },
    {
      "time": "00:44:54",
      "text": "你的GP four有些问题，一个是说湖北给他送小说稿的话，他会调code interpreter，他会用IEG加上它的搜索引擎。但这个IAG的效果就不好了。你像AG就相当于我就抽一段进来，然后用代码的方式生成一个代一个程序去抓里面的数据。这个肯定没有原生的用大约模型来做summarizing要好。",
      "speaker": "发言人2"
    },
    {
      "time": "00:45:15",
      "text": "但是atrophic给我的感觉是给他很大一段很长一段话。那么这段很长的一段故事，这故事可能他也没见过，但她能够总结的非常好，她的一些细腻程度也超过了现在的一些水平，这个是让我觉得非常好的。然后另外就是说续写什么的也不错，补全什么的都挺好的。但是在推理上来说，给他一些问题，他确实也做的挺好。但是你要说比GBT four强，我不知道，很难说清楚，就是我觉得没有非常清楚的界限。",
      "speaker": "发言人2"
    },
    {
      "time": "00:45:44",
      "text": "它不是一个非常清晰的目标，它是一种人的主观感受。",
      "speaker": "发言人1"
    },
    {
      "time": "00:45:48",
      "text": "对对对，所以其实我觉得这个很难讲。因为现在网上也看到有很多评价说GPT four好像最近这两天突然之间变强了。对，就说之前GPT four在偷懒，有很多人在抱怨GD four偷懒，说他很多事情干活干得不好。这个当然也非常有可能了。因为如果这世界上没有竞争者的话，我本来可能会决定上线一个比较差的模型以减少计算代价。但是如果有市场上竞争者的话，他们才会出一个更好的模型。",
      "speaker": "发言人2"
    },
    {
      "time": "00:46:15",
      "text": "这个太有意思了。所以我们很需要竞争。",
      "speaker": "发言人1"
    },
    {
      "time": "00:46:18",
      "text": "是需要竞争。一旦有竞争之后，马上就会让大家能真正的感觉到OK我这个模型一定要推出最强的版本。其实有很多网上有很多人抱怨了，就是G刚出来的时候非常惊艳，比如觉得好厉害，但是越用越差越用越差越用越差越用越差。甚至有人发现他会在周末偷懒。",
      "speaker": "发言人2"
    },
    {
      "time": "00:46:35",
      "text": "周末偷懒就是周末的模型是比周一到周五说。",
      "speaker": "发言人1"
    },
    {
      "time": "00:46:38",
      "text": "对对对，有可能效果要差的。对的，就是他可能也学到了人类的一些数据集的一些bias。如果你发现这个邮件是周末写的，可能这个邮件质量就变成差了。他会把这个连起来之后，可能会自动的有这个BIOS。周末的时候我就写的短一点，然后就不会回你的问题，或者说他会忽略你的一些情况，可能会有这个一个问题。但是如果的话GP four就不得不保证自己质量，这其实是个好事情。",
      "speaker": "发言人2"
    },
    {
      "time": "00:47:04",
      "text": "对，接下来我们聊一聊meta，因为meta其实也开源了，我觉得它在市场上还是蛮受关注的。你怎么看麦太开源跟他开源的好处？",
      "speaker": "发言人1"
    },
    {
      "time": "00:47:14",
      "text": "我首先先声明，我不太想评论埋汰的一些东西。因为我是埋汰员工，所以我不是official的一个news provider。言归正传，我们来说我们这个故事，我觉得开源本身是一个好的，我们可以想象大模型的终局。一种可能是小数巨头垄断市场，大家都向他跪拜，我不希望这种事情发生。另外一种事情发生是人人都有核武器，大家形成威慑平衡，我希望后者是成立的。如果是这样的话，那其实我们应该拥抱开源。我们应该希望开源做得很厉害，用的话能够让大所有人都用上。这样的话就是保证一个最好的一个生态，有竞争才有进步的空间。有竞争的话，大家才会愿意分享，愿意把整个世界往前推进。",
      "speaker": "发言人2"
    },
    {
      "time": "00:47:55",
      "text": "这样会比较好。对我一直都很好奇开源的商业模式是什么？General的说一下开源的商业模式。",
      "speaker": "发言人1"
    },
    {
      "time": "00:48:02",
      "text": "我觉得商业模式其实就是赚吆喝呗，我觉得这是一个因为对埋汰来说，它不像google。对google来说，大模型用过一些要些术要命的business critical的component。因为google个service大云模型其实提供了一个口袋版的google。如果哪天大云模型能力超过现在现有引擎的水平，最后的结果就是没有人认为google了。这个是对google不能接受的。所以对google来说，它是一个core business，所有在训练和推理上的一个优化都不能发表论文，应该是这样子的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:48:35",
      "text": "但对于meta来说，meta的call business它不是这个它Michael是人和人之间连接，通过人和人之间连接来卖广告。所以对他来说，他当然希望所有人都均是所有人都有自己的大约模型，相互之间能够交流。用这个方式然后把它可以作为平台获得一些利益，这是我的观点了。对他来说，开源其实是有利于他的将来的一个发展。",
      "speaker": "发言人2"
    },
    {
      "time": "00:48:58",
      "text": "Meta内部的科研氛围是怎么样的？这个可以讲一讲吗？",
      "speaker": "发言人1"
    },
    {
      "time": "00:49:02",
      "text": "我觉得还是比较自由的，有点像学校的大家表白的up可以自由讨论一些问题，讨论一些文章，最后能够找到一些想法做出来。",
      "speaker": "发言人2"
    },
    {
      "time": "00:49:11",
      "text": "所以其实科研也是一个蛮开放的对。",
      "speaker": "发言人1"
    },
    {
      "time": "00:49:14",
      "text": "还是比较开放的一个方向，我们还是可以自由的跟其他的院校合作的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:49:19",
      "text": "我记得当时扎克伯格去找了坤的时候，他说如果我加入你们，我的条件就是科研必须以一种开放的方式进行。如果你想单独的以一种封闭的方式只在meta发论文的话，我不知道我的工作该如何的进行下去。所以他希望所有的东西在学术圈都是公开的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:49:37",
      "text": "对我觉得这个是个很好的一个logic。然后我们确实把这个philosophy贯彻到现在了，我觉得这个是非常好的一个地方。就是一开始在创始的时候的一个诺言，现在还是能够保持。",
      "speaker": "发言人2"
    },
    {
      "time": "00:49:48",
      "text": "对最后一个问题，我知道在AI研究业余，你自己也是一个科幻作家。你写了很多很多的科幻小说。比如说像幽夜星火、破晓之中、血祭梦想既是使命这些非常长篇的科幻故事。因为我一直都觉得人工智能科幻，包括我们非常多前沿技术，有的时候是要靠一点点想象力的。所以我还挺好奇你自己的科研从科幻中产生了哪些灵感。",
      "speaker": "发言人1"
    },
    {
      "time": "00:50:14",
      "text": "首先第一点，这些小说不一定都是科幻。我其实一开始写玄幻的不一定是科幻。科幻可能就是从破晓中开始。其实之前也有一些科幻玄幻结合的一些例子。",
      "speaker": "发言人2"
    },
    {
      "time": "00:50:23",
      "text": "但是我觉得其实写小说动因倒不是因为它是科幻小说，动因是因为人和人之间的关系。比如说我觉得有一些很有趣的场景是应该给它写下来的。或者说人生有些经历，这个经历让我觉得我应该写下来，然后让我能够在十年之后再回味这种经历。这种感觉并不是因为科幻小说所以才写科幻小说。它的动因不是因为我做research，或者说不是因为我做科研。动因是因为另外一方面感性上的，或者说是某种人和人之间的交互的这种方式，人生另一种方式来动因的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:50:55",
      "text": "所以是自己生活的影子。",
      "speaker": "发言人1"
    },
    {
      "time": "00:50:57",
      "text": "对，是生活的影子。这个跟科幻关系倒是不大，科幻是一层皮，很多时候是这样子。小说最重要是人和人之间的关系人和人的角色的塑造。",
      "speaker": "发言人2"
    },
    {
      "time": "00:51:05",
      "text": "知识是最重要的。你会花多长时间写小说？",
      "speaker": "发言人1"
    },
    {
      "time": "00:51:08",
      "text": "这个要看。比如说破晓之中，大概在2020年年底的时候，在知乎上连载过。那段时间我连连载岁，我就不得不每天都要花点时间写一下。现在还在连载吗？现在已经连载完了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:51:19",
      "text": "这连载的时候你写完了这部小说之前其实准备了很长时间，因为有很多很多的小的interesting idea，我要把它写进去，说这部分我要写成行行。最后你要找到故事盘全部连起来。所以大概花了五年的时间吧，我觉得我大概在刚刚去meta的时候，去meta的前几年的时候，反正就是有些时候有一些想法，有些像interesting那个Spark对吧？你可以把它写下来，我一般会先写场景，场景写完之后再把它场景连起来，变成一部有趣的故事。这个过程其实要花一点时间的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:51:50",
      "text": "现在做爸爸了，还有时间写科幻小说吗？",
      "speaker": "发言人1"
    },
    {
      "time": "00:51:52",
      "text": "现在其实挺忙的，而且我们最近研究也挺忙。大约模型出来之后，我基本没有什么时间做一些其他的事情。但是如果有空的话，你还是会继续的，还是想继续写。会想因为总是会有一些想法，那只有一些想法，这些想法其实你把记录下来其实很可惜。你把它记录下来之后，你会有一些新的思路，新的想法。因为有的时候脑子再换一个思路，想一些不同东西很有意思。",
      "speaker": "发言人2"
    },
    {
      "time": "00:52:15",
      "text": "谁是你的科幻启蒙？",
      "speaker": "发言人1"
    },
    {
      "time": "00:52:17",
      "text": "很难讲，我也不是特别的明确，就是什么样的算一个科幻启蒙。我觉得以前看过漫画，这些漫画书可能会比较不能说是科幻启蒙，应该说是玄幻启蒙。或者说是对于一个世界应该怎么构造，或者说这个世界什么样东西让我觉得很有意思。",
      "speaker": "发言人2"
    },
    {
      "time": "00:52:36",
      "text": "你喜欢哪些作品中的构造？",
      "speaker": "发言人1"
    },
    {
      "time": "00:52:39",
      "text": "这个其实挺难说的，当然三体肯定是一个例子，但是三体其实算出来的比较晚了，在那之前应该有很多。",
      "speaker": "发言人2"
    },
    {
      "time": "00:52:45",
      "text": "的暴露年龄了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:52:47",
      "text": "对，我们都很老了，我已经很老了。最早比如说漫画，像最近鸟山明先生去世了。其实我很早以前就特别喜欢看他的七龙珠系列，他们这种画的很好的大师，他们每一帧和每个镜头的切换和悬念塑造都非常好。直接导致了这就是我在写的时候，我还是会想很多多线并行的剧情和悬念的塑造和人物的塑造，这都会去思考，这个可能会潜移默化改变我的一些想法，或者一些小说的一些思路。我觉得我很幸运的是，我们在小时候接触的漫画都是顶级的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:53:21",
      "text": "真的是真的是对。",
      "speaker": "发言人1"
    },
    {
      "time": "00:53:23",
      "text": "那个时候确实是非常好的。如果就真的去想整个故事的逻辑的话，那其实就会学到很多东西。",
      "speaker": "发言人2"
    },
    {
      "time": "00:53:31",
      "text": "好的，非常感谢田博士。",
      "speaker": "发言人1"
    },
    {
      "time": "00:53:40",
      "text": "这就是我们今天的节目。如果大家喜欢我们的节目，欢迎在你所收听的播客渠道订阅我们，也欢迎给我们一个五星好评。中国的听众可以在苹果播客、小宇宙、喜马拉雅、蜻蜓FM、荔枝FM、网易云音乐来关注我们。海外的听众可以通过apple podcast、spotify还有youtube来关注我们。感谢大家的收听，谢谢。",
      "speaker": "发言人1"
    }
  ],
  "lab_info": {
    "summary": "罗永浩在直播中销售云产品的举动引起了社交媒体的广泛关注，此事件也引发了对中国云计算产业现状与未来趋势的讨论。尽管中国在云计算领域的算力结构占比远低于美国和欧洲，且公有云使用率低导致算力利用率不高，这些情况可能制约了中国在AI时代的潜力。罗永浩的行动或可促进企业级用户对云服务的认识与应用，解决中小企业的关键问题。同时，对话深入到人工智能技术，特别是Transformer模型的最新进展，提及田渊栋博士对Transformer的理解与改进，以及Meta公司在AI研究和开源方面的贡献，展示了科技与艺术如何共同推动社会进步，从技术革新到科幻创作的相互启发，体现了科技发展的广泛影响和深远意义。",
    "qa_pairs": [
      {
        "question": "罗永浩直播间售卖云产品的目的是什么？",
        "answer": "罗永浩通过直播售卖阿里云众多热门产品，并提供史无前例的优惠，目的是引领一场企业级的云认知运动，将云计算概念推向大众，并解决创业者的核心痛点。",
        "time": "00:00:50"
      },
      {
        "question": "在AI时代，为什么很多中小企业依然在购置服务器，中国的算力资源利用效率是否不高？",
        "answer": "是的，虽然中国稳居世界第二位，但算力资源利用效率并不高，其中一个重要原因就是公有云这种高效的算力模式占比过低。",
        "time": "00:00:30"
      },
      {
        "question": "神经网络是如何工作的？",
        "answer": "田博士一直在探索神经网络为何能有效工作，并通过研究来解答这一问题，最近的研究聚焦于sora transformer和AGI领域。",
        "time": "00:02:08"
      },
      {
        "question": "田渊栋博士的研究方向是什么？",
        "answer": "田渊栋博士的研究方向是深度强化学习、表示学习和优化，他曾是谷歌无人驾驶项目软件工程师，并在谷歌发表过划时代的论文，现在关注端侧小模型的研究。",
        "time": "00:02:45"
      },
      {
        "question": "田博士为何从自动驾驶转向研究大模型？田博士如何看待其团队在大模型研究上的产出？",
        "answer": "在OpenGo项目取得成果后，团队成员各有研究方向，田博士基于对理解神经网络工作的初心，于2019年开始转向研究大模型，并取得了一系列关于神经网络理解的重要成果。田博士提到团队过去的研究成果，特别是在神经网络理解和大模型结合方面取得了显著进展，比如去年发表的两篇论文就体现了他们对神经网络深入理解的成果。",
        "time": "00:07:00"
      },
      {
        "question": "田博士的最新研究工作以及相关社区反馈如何？",
        "answer": "最近他们团队发表的一篇关于如何在消费级GPU上运行大模型的研究文章，在行业内引起了很大反响，得到了开源社区的积极反馈和重复验证，确认了他们的一些发现。",
        "time": "00:09:10"
      },
      {
        "question": "在消费级显卡上能否实现大模型的训练或预训练？4090显卡在训练大型模型时需要多长时间？",
        "answer": "以往在消费级显卡上做训练，尤其是预训练，对于消费级GPU而言是较难的。但最近一篇文章表明，通过将多个4090显卡连接起来，理论上有可能实现大模型预训练的效果。单张4090训练一个7B模型可能需要大约110天的时间，非常耗时。但如果有多张4090显卡并行计算，并且模型可以被压缩进有限内存中，减少跨卡交互，通过PCIE或AN net连接，训练时间可能会有所缩短。",
        "time": "00:10:51"
      },
      {
        "question": "4090显卡与A100、H100相比有何特点？如何绕过显卡昂贵的问题并进行更有效率的训练？",
        "answer": "4090是英伟达面向消费级市场设计的游戏显卡，相较于A100、H100，在性价比上更高，可以更节省显卡资源，在训练过程中能够有效节省显存消耗。我们通过算法改进，比如对权重进行低质矩阵分解（重参化），减少参数数量，降低内存需求，使得即使在单张卡上也能放置更多的权重，从而提高训练效率和内存使用率。",
        "time": "00:12:14"
      },
      {
        "question": "低质矩阵分解（Lora）在预训练中的问题是什么？关于训练7B模型所需内存的问题？",
        "answer": "Lora的一个主要问题是，由于权重本身并不一定是低质的，但在梯度迭代过程中，梯度可以是低rank的。通过证明并利用这个特性，我们可以在保持内存开销低的同时，实现有效率的训练。使用改进后的算法，原本需要40GB以上内存才能训练的7B模型，现在只需要约18到20GB的内存，这让4090等显卡得以胜任更大型模型的训练任务。",
        "time": "00:14:27"
      },
      {
        "question": "该算法改进是否提高了数据质量或预训练效果？",
        "answer": "算法改进与数据质量是平行关系，而非因果关系。数据质量的提高可以与算法优化叠加，进一步提升模型效果，但算法改进主要关注于如何更高效地利用现有数据进行训练。",
        "time": "00:15:34"
      },
      {
        "question": "论文“mobile LM”的主要思想是什么？",
        "answer": "“mobile LM”论文探讨的是能否通过减小神经网络规模，用更小的模型达到更好的效果。该研究由Reality Lab团队完成，其中包含了关于层间共享参数等建议，而我作为外部顾问给他们提供了这些建议。",
        "time": "00:16:19"
      },
      {
        "question": "业界对于通过增加模型规模实现AGI的看法有哪些分歧？",
        "answer": "业界存在两种观点，一种认为仅通过增加模型规模（scaling law）就能实现AGI，另一种则认为这种方法最终会遇到瓶颈。我个人倾向于认为目前离AGI还有一段距离，单纯靠规模扩大可能无法取得有效进展。",
        "time": "00:16:57"
      },
      {
        "question": "在自动驾驶领域，为什么它的要求比大语言模型更为严格？",
        "answer": "自动驾驶要求100%可靠，不能犯错，这个标准远高于大语言模型。如果自动驾驶出错，后果严重且无法自我修正，而大语言模型即使犯错也无实质损失。",
        "time": "00:20:39"
      },
      {
        "question": "您是否赞同AGI发展中会遇到瓶颈的观点？",
        "answer": "我更倾向于第二种观点，即AGI发展过程中确实会遇到瓶颈，而关键在于如何依靠现有技术范式之外的方法来突破这个瓶颈。",
        "time": "00:21:10"
      },
      {
        "question": "人类在成长过程中如何学会各种技能？",
        "answer": "人类在成长过程中，通过无须人工干预的方式快速学习新技能，如爬楼梯、拍摇篮等，这表明存在一种高效的学习算法，目前我们只探索到了皮毛，但该算法潜力巨大。",
        "time": "00:21:56"
      },
      {
        "question": "在开发GLORY模型时，是否在思考如何改进算法以提高预训练效果？",
        "answer": "是的，在开发GLORY模型时，我们一直在思考如何在算法层面提高预训练的质量，并且这一问题贯穿于整个研究生涯，核心在于理解神经网络的工作原理并寻找更好的算法以提升其工作效率。",
        "time": "00:22:23"
      },
      {
        "question": "为什么OpenAI选择使用transformer架构进行训练，并且它的优点和瓶颈在哪里？",
        "answer": "OpenAI采用transformer架构是因为其具有强大的并行计算能力，能有效利用GPU算力计算序列中所有token之间的相似性。然而，transformer的训练需要大量算力，速度相较于CNN等方法较慢，且在对延迟要求高的场景中表现不佳。尽管如此，由于其优秀的skin能力和对大规模数据的良好适应性，transformer仍被视为AGI研究的重要方向。",
        "time": "00:24:21"
      },
      {
        "question": "在科研中，何时应深入研究细节优化，何时应跳出框架寻求新范式？",
        "answer": "科研人员需在不断精进细节优化的同时，也要敢于质疑现有框架，尝试从第一性原理出发，通过探索和创新来寻找新的研究范式。例如，在神经网络领域，需要从基础原理出发，结合数学家的方式去分析和模拟数据之间的相关性，进而构建出新的理论框架和模型。",
        "time": "00:27:18"
      },
      {
        "question": "近期有没有看到什么好的研究观点或成果？",
        "answer": "最近看到的一些研究如Sora，虽然没有带来特别惊讶的效果，但它是值得关注的方向。同时，会关注那些与研究方向相关的文章，比如加速推理和对神经网络进行分析的文章，以便了解最新的进展和观点。",
        "time": "00:29:15"
      },
      {
        "question": "你对Sora在生成视频时保持一致性的表现有何看法？",
        "answer": "我觉得Sora在保持一致性的表现上非常出色，它生成的所有视频都展示了非常好的整体性和连贯性。无论是场景、人物行为还是穿着，都表现得非常相似且一致，这在所有demo中都得到了体现，令人印象深刻。",
        "time": "00:30:25"
      },
      {
        "question": "Sora是如何实现这种一致性的？",
        "answer": "Sora并不是通过逐帧预测的方式来生成视频，而是将整个视频看作一个大的image，并在其中嵌入3D结构。然后通过diffusion模式进行扩散，以此来保证整体视频的consistency，而不是像逐帧预测那样可能会出现前后帧之间不一致的问题。",
        "time": "00:30:48"
      },
      {
        "question": "保持一致性的难度与视频时长有何关系？",
        "answer": "在20秒或十几秒的时长内，视频内容变化很大，而Sora能在如此短的时间内保持各种变化的一致性，比如在伊朗的一个demo中，一个人在车里的反光能在不同时间段保持一致，这是非常令人惊讶的。",
        "time": "00:31:37"
      },
      {
        "question": "Sora是否可以被视为一个世界模型？",
        "answer": "Sora可以被看作是一个世界模型，因为它可以根据给定的前几帧进行未来帧的预测或融合，以生成连贯的视频。例如，它可以模拟不同情况下物体进入同一场景的过程，显示出其能够做任何补全的能力。",
        "time": "00:33:11"
      },
      {
        "question": "Sora在物理模拟方面是否存在问题？",
        "answer": "是的，Sora在物理模拟方面存在问题，尤其是在涉及到物体间快速交互和物理过程复杂变化时，由于数据不足和物理过程难以模拟，导致其模型质量不佳，无法精确捕捉到物理世界的真实变化。",
        "time": "00:34:45"
      },
      {
        "question": "Sora是否能理解世界运转的规律、把握物理法则、记忆检索信息以及逻辑推理和行动规划的能力？",
        "answer": "将Sora视为全能模型可能并不恰当。生成视频与预测物理世界是两个不同的任务，Sora在视觉生成方面表现出色，但在面对未见过的情境时，其规划和统筹能力可能会受到一些限制。不过，它在创作和理解视频方面展现出了一定的逻辑推理能力。",
        "time": "00:36:13"
      },
      {
        "question": "Sora在生成过程中是否使用了虚幻引擎5？",
        "answer": "没有明确证据表明Sora使用了虚幻引擎5，可能是因为误传。网上有猜测和一些推特上的讨论，但并没有得到证实。实际上，Sora可能使用了很多网上视频数据，包括合成数据，这些数据不一定是通过虚幻引擎生成的。",
        "time": "00:39:23"
      },
      {
        "question": "对于使用合成数据训练大模型这种方式怎么看？",
        "answer": "合成数据是未来的一个重要趋势，像Three former等模型就采用了合成数据进行训练。通过已有的引擎生成大量数据再进行训练，这种方式有效且能解决真实数据越来越难获取的问题。同时，合成数据与真实数据的边界在未来将会越来越模糊。",
        "time": "00:40:56"
      },
      {
        "question": "数据来源的问题如何解决？",
        "answer": "解决方法之一就是利用合成数据，即用算力来换取更多的数据。然而，合成数据如果没有人类干预，可能会丢失一些关键信息，不一定能完全抓住真实数据的特点。例如，OpenAI之前的文章中提到，虽然生成了大量推理步骤的数据，但最终仍需要人工验证以确保正确性。",
        "time": "00:42:40"
      },
      {
        "question": "你认为Anthropic的新模型在写小说方面的表现如何？",
        "answer": "Anthropic的新模型在写文章和小说续写方面表现出细腻丰富的细节，长文理解和分析能力很强，能够对长段故事进行很好的总结，这一点优于GPT-4。而在推理方面，两者各有优势，难以明确界定哪个更好。",
        "time": "00:44:37"
      },
      {
        "question": "对于Meta开源大模型的看法及开源的意义是什么？",
        "answer": "开源本身是一个好的事情，有助于形成一个竞争平衡的生态，让所有人都能使用上先进的技术。希望看到人人都有核武器的局面，这样能推动整个行业进步，而不仅仅是被少数巨头垄断市场。",
        "time": "00:47:14"
      },
      {
        "question": "对于meta来说，开源对于其未来的发展有何影响？",
        "answer": "对于Meta来说，开源能够促进人与人之间的连接和交流，让他们通过这样的方式来作为平台获取利益。因此，对Meta而言，开源是有利于其未来发展的一个策略。",
        "time": "00:48:35"
      },
      {
        "question": "Meta内部的科研氛围是怎样的？",
        "answer": "Meta内部的科研氛围比较自由开放，类似于学校的学术氛围，员工可以自由讨论问题、文章，并通过合作产生创新想法。",
        "time": "00:49:02"
      },
      {
        "question": "扎克伯格在加入某团队时提出了什么样的科研合作条件？",
        "answer": "扎克伯格在加入某团队时要求科研必须以开放的方式进行，强调所有研究成果应在学术圈内公开，以促进科研进步和个人工作的持续进行。",
        "time": "00:49:19"
      },
      {
        "question": "科幻创作是否为你的科研或研究带来灵感？",
        "answer": "科幻创作并非直接源于科研或研究，而是源于对人与人之间关系的深刻理解和生活经历的记录。科幻元素更像是故事的外壳，而小说的核心是塑造人物角色和展现人与人之间的互动。",
        "time": "00:50:57"
      },
      {
        "question": "你写科幻小说时，一般会花费多少时间？",
        "answer": "写科幻小说所需的时间因作品而异，例如《破晓之中》是在2020年底开始连载，那段时间每天都会投入一定时间写作，整个创作过程大约花了五年时间准备和构思。",
        "time": "00:51:08"
      },
      {
        "question": "成为爸爸后，是否有更多时间进行科幻创作？",
        "answer": "现在成为爸爸后，由于工作繁忙和研究任务重，没有太多空闲时间进行科幻创作。但如果有机会，仍然会继续创作，因为记录下这些想法和灵感对于新的科研思路很有帮助。",
        "time": "00:51:52"
      },
      {
        "question": "你的科幻启蒙来自于哪些作品或作者？",
        "answer": "科幻启蒙难以明确指出具体来源，但小时候看过的漫画如鸟山明的《七龙珠》系列等对他的创作思路产生了潜移默化的影响，尤其在剧情多线并行、悬念塑造和人物塑造方面。",
        "time": "00:52:47"
      }
    ],
    "chapters": [
      {
        "time": "00:00:00",
        "title": "罗永浩直播间推广云产品引发关注",
        "summary": "罗永浩的直播间开始销售云产品，并在微博热搜上引起热议。虽然进入了AI时代，但许多中小企业仍在购买服务器。根据中国信通院2022年的数据，中国云计算的占比仅为28%，远低于美国和欧洲。罗永浩希望通过他的平台，推动云计算概念的普及，并解决创业者面临的核心问题。此次他选中的产品覆盖了阿里云的热门产品，并提供了前所未有的优惠。有兴趣的人可在3月31日晚上七点，通过淘宝APP搜索罗永浩，观看直播。"
      },
      {
        "time": "00:01:27",
        "title": "探索神经网络及AGI未来：田渊栋博士的研究之旅",
        "summary": "自2017年谷歌发布Transformer模型以来，人工智能领域经历了显著进展。田渊栋博士，作为Metafile研究员，专注于研究神经网络的运作机制，并在端侧小模型领域发表了重要论文，受到业界广泛关注。田博士的经历包括在ELF OpenGo项目上的工作和对AGI的深入探索。他对于神经网络如何工作的问题，以及自动驾驶与AGI的未来持开放态度，期待着技术的进一步突破。"
      },
      {
        "time": "00:05:29",
        "title": "从围棋AI到大模型研究的转型之旅",
        "summary": "对话者从2005年开始科幻写作，将不靠谱的想法放进小说，而靠谱的想法用于科研。研究过自动驾驶、围棋AI（包括dark forest策略），并最终转向大模型研究。在Open Go项目后，由于团队成员想法各异，决定不再继续围棋研究。对话者对神经网络的工作原理产生了浓厚兴趣，因此在2018-2019年后，转向研究神经网络如何工作，并发表了多篇相关论文。曾受到OpenAI的邀请但因兴趣不同未加入，后因大模型领域的起飞，感受到此方向的希望。通过研究，提出了对Transformer的理解，发表了两篇重要论文，进一步促进了神经网络的理解和改进。"
      },
      {
        "time": "00:09:10",
        "title": "开源社区对 Glory 的反馈及后续研究方向",
        "summary": "Glory在发布后获得了社区的广泛关注和反馈，开源社区成员重复了相关工作，验证了一些技术发现，如如何优化内存使用，提升运行速度和效果。此前，一篇关于网络分析的文章曾被多次拒绝，但其部分内容后来对后续的研究产生了重要影响，特别是在如何在消费级GPU上运行和训练大型模型方面。这项研究显示了基础研究对领域发展的长期影响，并探讨了使用消费级硬件进行大规模模型预训练的可行性，提出了利用多卡并行以缩短训练时间的可能方案。"
      },
      {
        "time": "00:13:09",
        "title": "改进模型并行训练的算法",
        "summary": "讨论集中在如何通过改进算法来提高模型并行训练的效率，特别是针对内存需求大的模型。提出了一种方法，通过将权重进行低秩矩阵分解来减少训练过程中的内存需求，从而使得在有限的资源条件下能够更有效地训练更大的模型。这种方法不同于传统的模式并行方案，如FSDP，它不依赖于增加硬件资源，而是通过算法上的创新来解决模型并行训练中内存不足的问题。特别地，讨论指出即使在一开始的预训练阶段，这种方法也能有效减少内存使用，从而实现更高效的模型训练。此外，这种方法与提高预训练数据质量是并行不悖的，意味着算法的改进和数据质量的提升可以相互叠加，共同提升模型的性能。最终目的是让现有的硬件设备（如4090显卡）能够更有效地利用，从而实现对更大模型的训练。"
      },
      {
        "time": "00:16:12",
        "title": "探讨小型神经网络在移动LM中的应用及AGI的未来",
        "summary": "讨论集中在是否能通过使用小型神经网络（特别是350幂恋的神经网络）来提高模型效率，从而降低成本。虽然小型模型可能无法完全媲美大型模型的效果，但对于模型的效能与规模之间达到一个平衡点是十分有趣的探索。此外，还提到了对AGI实现的两种不同观点：一种是通过模型规模的增加来实现，另一种观点则认为仅仅扩大规模存在瓶颈。对话中强调了对于实现AGI，可能还需要几个突破，以及当前对Transformer模型等效果的理解还不够深入。此外，还提出了数据获取的困难与模型理解能力的限制是当前研究中面临的重要问题。"
      },
      {
        "time": "00:20:21",
        "title": "探索AGI与自动驾驶技术的未来",
        "summary": "对话中深入讨论了自动驾驶和AGI领域的挑战与潜力，强调了两者的不同特性和要求。在自动驾驶方面，需要达到完全无错误的性能，而AGI则允许一定程度的错误。此外，讨论指出人类的学习能力远超现有技术所能达到的效率，暗示存在更高效的学习算法等待发现。进一步地，通过理解和改进神经网络的工作原理，有可能提升现有算法的性能，这不仅限于自动驾驶，也适用于AGI的研究。特别提到了如何通过模仿传统推理算法来优化神经网络，从而实现更复杂的任务处理能力。"
      },
      {
        "time": "00:23:55",
        "title": "Transformer模型的优劣及其在AI发展中的角色",
        "summary": "对话中讨论了Transformer模型在众多神经网络架构中的突出地位，特别是其被用于大型语言模型训练的原因。Transformer的优点在于其高效的并行处理能力，适合处理长序列数据，且在大量数据输入下展现出比传统CNN模型更优秀的效果。此外，还探讨了该模型的缺点，包括对大量算力的需求和较高的延迟，这使得它在实时性要求高的场景中应用受限。同时，对话触及了Transformer在推动人工智能（AI）发展，尤其是实现通用人工智能（AGI）方面的潜力和面临的挑战，强调了探索新的范式和方法的重要性。"
      },
      {
        "time": "00:27:18",
        "title": "探索与优化：研究中的策略选择",
        "summary": "在学术和科研领域，存在着两种主要的研究策略：一种是在小问题上不断精进优化，追求细节的完善；另一种是跳出传统思维，从本质上质疑和探索新领域，以实现大的突破。这种探索与优化的平衡，对于研究者而言是至关重要的。通过不断寻找和验证与自己研究方向契合或能提供新视角的文章，研究者能够逐步构建和完善自己的研究框架。特别地，关注那些能加速推理或提出新现象和观点的文章，对于推动研究进展具有重要意义。"
      },
      {
        "time": "00:29:54",
        "title": "探讨Sora技术及其在视频生成中的应用",
        "summary": "对话者讨论了Sora技术在视频生成领域的应用，特别是其效果和一致性的优异表现。Sora通过将整个视频视为一个大的图像，并利用3D音镶嵌和扩散模型（diffusion model）进行处理，从而实现了视频内容的一致性。此外，还讨论了Sora技术在处理视频前后内容一致性、反光一致性等方面的优势。同时，探讨了“世界模型”的概念，以及Sora技术如何能够完成视频的补全和生成，但同时也指出了在物理模拟和物体交互方面的挑战。"
      },
      {
        "time": "00:35:48",
        "title": "探讨索尔对物理世界理解和视频生成能力",
        "summary": "对话者认为索尔可能难以准确理解和模拟物理世界的运转规律，特别是面对物理误差时的行动规划和逻辑推理能力可能有限。此外，虽然索尔能生成视频，但这些视频可能与真实物理世界存在偏差，强调了将索尔视为全能模型的复杂性，并指出需更多工作来提升其理解力和预测准确性。"
      },
      {
        "time": "00:37:40",
        "title": "探讨全脑模型与大语言模型的区别及应用",
        "summary": "对话中讨论了大语言模型和全脑模型的差异，强调全脑模型不仅仅是关于语言，还包括使用感官认知世界。指出这种模型能更全面地模拟人类的学习和决策过程，如通过与物体交互强化学习，以及利用世界模型进行预测和决策。此外，还提到了一个利用合成数据和虚幻引擎5进行模型训练的案例，强调了查看第一手资料的重要性以避免误传。"
      },
      {
        "time": "00:40:49",
        "title": "合成数据在大模型训练中的应用与挑战",
        "summary": "合成数据成为训练大模型的重要趋势，有效解决真实数据获取难题，但存在局限性。通过使用已有引擎生成大量数据训练模型，如在围棋和科研领域应用，提升模型能力同时面临质量验证难题。没有人类干预，模型难以突破自身错误认知，存在达到能力瓶颈的风险。"
      },
      {
        "time": "00:43:51",
        "title": "比较新型AI模型在多领域应用中的表现",
        "summary": "最近，Anthropic发布了一个在推理数学编码、多语言理解、视觉等二十多个测试中性能超越OpenAI的GPT-4的大模型。讨论者分享了自己使用该模型的感受，特别是在文学创作方面，指出其在细节丰富度和长文理解与分析方面具有优势。此外，对比了该模型与GPT-4在小说创作方面的表现，并讨论了模型在推理任务上的能力。同时，也提到了竞争对于推动技术进步的重要性，并对Meta开源模型的市场影响进行了展望。"
      },
      {
        "time": "00:47:14",
        "title": "开源模式对未来技术生态的影响及Meta的科研氛围",
        "summary": "开源被视为推动技术进步和保持生态竞争性的重要方式。它有助于防止少数巨头垄断市场，而是促使形成一种平衡，让每个人都能受益于技术发展。特别是对于Meta这样的公司而言，开源不仅有助于其核心业务的发展，还有助于在学术圈中保持开放和合作的态度。Meta内部的科研氛围被描述为自由和开放，这种环境鼓励创新和合作，与公司对外开源的承诺相一致。"
      },
      {
        "time": "00:49:47",
        "title": "科幻作家与AI研究者的跨界对话",
        "summary": "本次对话聚焦于一位在人工智能研究领域业余时间从事科幻小说创作的作家。通过对话，揭示了作家如何将科幻与现实生活、人与人之间的关系相结合，创作出诸如《幽夜星火》、《破晓之中》、《血祭梦想既是使命》等长篇科幻故事。作家强调，其写作灵感并非源自科研，而是源于对人生经历的回味和个人情感的投射。尽管如今作为一位繁忙的父亲和AI研究者，作家仍希望能继续探索和记录自己的创意和想法，表明科幻对于他来说是一种生活的影子和对未知世界的好奇。此外，对话中还提及了作家受到的科幻启蒙和影响他创作的一些作品，包括《三体》以及鸟山明的《七龙珠》系列，展现了他如何将多线并行的剧情和人物塑造融入自己的小说创作中。"
      }
    ],
    "mindmap": {
      "children": [
        {
          "children": [
            {
              "children": [],
              "content": "中国信通院2022年数据显示，中国云计算占比28%，低于美国（>60%）和欧洲（>50%）。"
            },
            {
              "children": [],
              "content": "罗永浩直播间推广云产品，引发市场关注。"
            },
            {
              "children": [],
              "content": "公有云模式在中国的利用效率较低。"
            }
          ],
          "content": "云计算与市场动态"
        },
        {
          "children": [
            {
              "children": [],
              "content": "2017年谷歌论文“Attention is All You Need”开启新一轮人工智能发展。"
            },
            {
              "children": [],
              "content": "变革中AI、云计算成为关键。"
            },
            {
              "children": [],
              "content": "Metafile研究员田渊栋博士研究端侧小模型及神经网络工作机制。"
            }
          ],
          "content": "AI与技术发展"
        },
        {
          "children": [
            {
              "children": [],
              "content": "AGI（通用人工智能）的实现路径存在分歧，一种观点认为规模扩大可以实现，另一种认为需要技术突破。"
            },
            {
              "children": [],
              "content": "田渊栋博士探讨神经网络的工作机制及其应用，如transformer的潜力与局限。"
            }
          ],
          "content": "科研与未来预测"
        },
        {
          "children": [
            {
              "children": [],
              "content": "Meta开源模型，推动科研共享和进步。"
            },
            {
              "children": [],
              "content": "开源模式对企业意味着什么？如何通过开源保持竞争力？"
            }
          ],
          "content": "开源与商业模式"
        },
        {
          "children": [
            {
              "children": [],
              "content": "田渊栋博士同时是科幻作家，科幻作品对他的科研工作有启发。"
            },
            {
              "children": [],
              "content": "科幻与现实技术的交融，对未来技术趋势的预测与思考。"
            }
          ],
          "content": "科幻与创新"
        },
        {
          "children": [
            {
              "children": [],
              "content": "Google、OpenAI、Meta在AI领域的不同策略和贡献。"
            },
            {
              "children": [],
              "content": "科技公司如何平衡商业利益与开源贡献。"
            }
          ],
          "content": "科技公司与AI"
        },
        {
          "children": [
            {
              "children": [],
              "content": "AI和云计算是未来技术发展的关键，特别是在提高效率和实现AGI方面。"
            },
            {
              "children": [],
              "content": "开源有助于促进技术进步，但同时也需考虑商业模式和竞争策略。"
            },
            {
              "children": [],
              "content": "科幻文学对科技创新有启发作用，展现了对未来技术的想象力和预视。"
            }
          ],
          "content": "对话结论"
        }
      ],
      "content": "对话内容脑图摘要"
    }
  }
}