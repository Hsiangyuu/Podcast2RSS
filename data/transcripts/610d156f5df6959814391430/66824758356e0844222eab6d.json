{
  "pid": "610d156f5df6959814391430",
  "eid": "66824758356e0844222eab6d",
  "title": "episode 26｜聚焦ComputeX：芯片算力的革新与未来",
  "task_id": "dej8nby5rrb59pog",
  "transcription": [
    {
      "time": "00:00:07",
      "text": "Hello, 大家好，欢迎大家来到中金研究院的播客节目，我是主持人玄多。大家好。",
      "speaker": "发言人1"
    },
    {
      "time": "00:00:13",
      "text": "我是左璇璇。",
      "speaker": "发言人2"
    },
    {
      "time": "00:00:14",
      "text": "2024年中国台湾computer x大会在六月初的中国台湾召开了。作为全球科技硬件领域的重要盛会，每年都会吸引着来自世界各地的顶尖科技公司和行业专家。中金研究部的科技硬件团队也是在活动后就发布了三篇对本次大会的深度洞察。在今天的节目中，我们也非常有幸邀请到了报告作者之一，中金研究部科技硬件行业研究员陈乔生，从computer x大会的视角出发，帮助我们一起了解AI芯片技术的最新技术进步，以及这些技术进步如何加速AI应用的落地和普及。我们欢迎陈乔生老师。",
      "speaker": "发言人1"
    },
    {
      "time": "00:00:54",
      "text": "大家好，我是中金研究部科技硬件行业的分析师陈乔生，今天很高兴能借这个机会，跟大家分享一下我们对于整个AI相关的这些硬件。在最近的一些更新，以及我们对整个行业未来发展的一些观点和看法。",
      "speaker": "发言人3"
    },
    {
      "time": "00:01:11",
      "text": "我们就进入第一趴，我们想先聊一下这个行业的概览和技术进步。首先我们想问一下，为什么发展这个AI需要专门的AI芯片，而不是用通用芯片呢？",
      "speaker": "发言人1"
    },
    {
      "time": "00:01:21",
      "text": "是这样，我觉得这个问题其实作为开场的一个问题，也是非常重要的，希望能帮大家理解一下为什么现在有AI芯片这个概念，以及这个产品。我觉得核心来看，AI芯片它的出现的原因，以及为什么通用芯片不能被用作这个AI加速，其实核心的原因还是由于这个任务类型的不同。",
      "speaker": "发言人3"
    },
    {
      "time": "00:01:42",
      "text": "我们看到现在做AI任务来看，其实你主要的这个任务类型是要去加速神经网络的计算跟通用计算其实是有一些差别的。之前我们看到比如说你在这个电脑上运行一个APP，或运行各类的这种应用程序。其实你需要支撑的这个应用程序是很多的。而且每一个应用程序里面，它其实更多的不一定是以这个计算作为密集型的这样的一个我们说的这个任务的场景或者怎么样。实际上其实很多时候你可能也需要进行很多的复杂逻辑的控制。所以说这样的一个实际的应用场景，是非常适合以前我们讲的通用计算芯片，或者说以CPU为主导去承担的这样的一个任务类型。但现在来看，我们做AI实际上其实你要做的事情就是要加速神经网络的计算。我就想要把这种所谓的人工智能算法，比如说卷积计算，这个矩阵的这种乘法，向量矩阵乘法等等的这些相对比较有特点的专一的一类的应用，我需要去加速它。",
      "speaker": "发言人3"
    },
    {
      "time": "00:02:43",
      "text": "那这样来看，其实我们也自然能理解，就是原来可能类似CPU这种通用类的计算芯片，其实它并不一定适合我们说现在的这种AI的这种加速类型的应用。其实我们反而是需要一种专门可以加速神经网络计算这样的一个芯片。那能达到了一个更高效以及更低成本的最后的一个效果。这个是我们觉得为什么发展AI需要专门的这个AI芯片，而不是用原来这个通用芯片。其实它也可以去做，但是最终因为任务类型的单一化，导致了你的这个成本和最后的效率，可能不能达到一个最优的一个结果。",
      "speaker": "发言人3"
    },
    {
      "time": "00:03:18",
      "text": "那刚才你也提到了，其实就是针对不同的APP和不同的需求，它是有不同种类的AI芯片的。这个有几类？这个AI芯片可以给我们介绍一下吗？",
      "speaker": "发言人1"
    },
    {
      "time": "00:03:29",
      "text": "好的，没问题。如果从AI芯片的分类来看，我们先按部署的位置来分，其实主要还是两大块，一块是云测，一块是端测。云测很明显也比较好理解，那就是部署在数据中心里面的AI芯片。当然数据中心你也可以分为比较大的互联网数据中心，但也有一些小的可能边缘的区域的数据中心。只不过可能它的任务的场景可能有大有小。不过它整体应该说是一大类属于云测的AI芯片。",
      "speaker": "发言人3"
    },
    {
      "time": "00:03:55",
      "text": "还有一类，我们就说就是端测的。端测的话其实也比较好理解。就是我们比如说电脑里面个人PC然后手机里面，包括还有像汽车，包括ALT设备这种物联网设备。比如你家里的这个音箱里面，你要去让音箱比如能跟你去进行智能对话，那里面也有一些AI芯片，他要去对你这些AI的算法去做加速，做支持，所以我觉得如果整体来分，应该说分为云测和端测两大部署场景。当然如果我们在云测里面再做细分，包括端测里面其实我们按任务类型来分，不是按照这个部署位置来分。",
      "speaker": "发言人3"
    },
    {
      "time": "00:04:31",
      "text": "还有这个训练和推理两种，一种芯片叫训练芯片，训练是干嘛的呢？就是说你现在我需要一个人工智能行业的这种语言模型，或者说多模态模型。现在这个模型它还是一个偏野生的一个状态，我现在无法就让他马上的投入工作。它很多的东西，比如说参数还没有设定好在这时候我需要在做模型的调整的时候，我要去训练它。把它最后从去变成一个可以很精确的得到一个我们想要的一个输入输出对应关系的这样的一个算法的模型。这样的话我们需要去做训练。",
      "speaker": "发言人3"
    },
    {
      "time": "00:05:05",
      "text": "那对于训练芯片来讲，我可能对于整个的计算的吞吐量的要求相对来说会比较高，但是我对这个时效性要求可能不高。就是说我不会在乎，我可能一定要把这个模型，比如说在一两天之内就弄出来。当然就是说你的模型的快速交付能力，可能也是衡量你在大模型或者咱们说之前的小模型赛道上，你能做到领先与落后的一个衡量标准。但是实际上就是这个东西可能短期内来看，对时延的要求不是那么高。",
      "speaker": "发言人3"
    },
    {
      "time": "00:05:36",
      "text": "那端侧来看，刚才我们讲说包括云测还有一类芯片，主要就是推理。推理其实说白了就是实际任务部署，任务上线的时候，我整个模型都已经做好了。那我拿来就是用，我就是为了去做商业落地的那我云测其实可以部署这个推理芯片，端测同样可以部署。只不过可能云测的芯片从性能上价值量上要高一些，处理一些比较复杂的任务。端测可能我就处理一些相对更加简单的一个任务。但这类芯片相对来说，我刚才讲了，可能跟云测的训练芯片不太一样。实际上我更多的就是说会在乎这个时延，包括这个低成本。这些其实跟这个商业落地和真正的我们说最后做人机交互相关的核心的这些指标。",
      "speaker": "发言人3"
    },
    {
      "time": "00:06:18",
      "text": "所以说再总结一下刚才徐老师问的这个问题。一方面是说我们按部署的位置来看，可以分为云测和端测的这个AI芯片。如果按这个应用的场景来分，我可以分为用训练的还有推理的这两类，大概是这样的一个情况。",
      "speaker": "发言人3"
    },
    {
      "time": "00:06:34",
      "text": "那您刚才其实提已经提到了，就是在云测这一块如何去衡量这个AI芯片的技术发展。那在观测这边是不是也有这样的衡量指标？",
      "speaker": "发言人1"
    },
    {
      "time": "00:06:43",
      "text": "其实类似的，我只是刚才讲到就是说对于云测来说，可能大家对于算力的要求，包括其他的，我们就说如果你从这个指标上来看，可能还有比如我们现在讲的这个存储存储的容量和带宽，包括芯片间的互联能力等等的一些，说白了可以加速你训练的一个过程的这些核心指标。这个相关的客户在评估产品的时候都是比较看重的那端侧来看，我觉得第一刚才讲的就是其实我可能对于算力的要求没有那么高。因为我实际上模型我不需要经历那么大数据量的一个计算，把我这些参数都调整好，其实我已经是拿到了一个成品，我的计算量是有明显变变少的。但是我更多的，我可能是会衡量我的这个芯片的一个所谓的刚才讲了对于这个时延的敏感性。",
      "speaker": "发言人3"
    },
    {
      "time": "00:07:29",
      "text": "那这块可能我是比如说对于这些存储的容量，包括带宽这些东西可能有一些需求。对于这个多芯片间的互联能力，可能在端侧我要求也不高。更多的我可能希望的是我的这个成本可能会更低一些。我并不一定追求那么极致性能，但是最终我落到这个端侧以后，我能让商业模式的闭环我能赚起来。我这东西不能很贵。那贵的话我最后其实做商业落地的话，我实际上客户也最后是不会买单的那消费者也不会买单。",
      "speaker": "发言人3"
    },
    {
      "time": "00:08:00",
      "text": "所以说其实整体来看，我觉得端侧的这个芯片可能更多的如果从技术指标来衡量，可能对于存储的容量，包括这个存储带宽可能是有一定的需求的。可能尤其是容量这块就涉及到你到底能不能把模型放得下。然后另外的话，就是说所谓的成本来衡量成本来说，其实说白了就是到底你同样的这个功耗下，你能输出多大的计算能力。其实它并不是一个算力的绝对值并比较大，但是效率你一定要高等等的。所以我觉得可能跟云测的这个需求来看，还是呈现了一定的差异。",
      "speaker": "发言人3"
    },
    {
      "time": "00:08:36",
      "text": "明白了。那我们想问一下，就是目前的这些芯片厂商在这个开发上的着力点是在哪些方面呢？是针对这个AI芯片。",
      "speaker": "发言人1"
    },
    {
      "time": "00:08:45",
      "text": "明白。这块儿来说，其实之前我刚才也讲到了。最早我们说就是去年来看，当ChatGPT刚火那一块儿，其实大家还是对性能上一些核心的性能指标是有极致的追求的。比如说我刚才讲了对于算力。这个算力越做越大，存储也是一个大的存储的容量以及大的存储带宽。甚至还有刚才讲到的一些运力，我追求这个互联。比如说因为我现在不是一个AI芯片的工作，我是多个芯片工作。那我的协同能力我必须要特别的好。就相当于你一个人做事和100个人做事，那100个人做事儿的效率肯定不等于一个人乘100。这些其实我刚才讲的都是一些性能指标上。",
      "speaker": "发言人3"
    },
    {
      "time": "00:09:25",
      "text": "其实大家最早在整个生成式AI的对算力需求爆发的初期，大家都在在硬件上投入的这样的一个状态之下，其实大家追求的是极致的性能。但现在我们觉得来看，其实大家会从追求极致性能变化为追求降本趋势，因为确实你做任何一个行业的发展来说，前期应该说是有不断的投入。但如果这种不断的这种资金的投入，最终不会带来实际的商业落地，不会让你的这个商业模式形成一个闭环的话，那这种投入可能是不持续的。所以我们现在看起来，可能大家对于算力硬件的性能不是说没有需求了，不是说性能升级不追求。因为我们看到现在比如以这个大模型来看，确实大家从这个大模型的参数量、数据量这些东西其实都在往上增长。那这个落地以后，你对于算力的这个需求，由于用户数接入用户数变多以后也在向前增长，所以算力的需求是不缺的。但是关键大家现在非常在乎成本，就是如果你的成本降不下来，那实际商业模式你没法像一个滚雪球一样不停的滚大。你可能投入到一定时间点上，大家觉得投不动了，没有资金实力再去往里投入了，那可能行业的发展就会进入一个停滞期。",
      "speaker": "发言人3"
    },
    {
      "time": "00:10:39",
      "text": "所以我们觉得现在大家开发的着力点就不仅是对性能的极致追求，更多的我觉得是在硬件和软件上想怎么去做优化。包括系统上做这个工程优化。导致最后整个的硬件的单位算力成本是下降的。我觉得这个是现在大家核心所关注的一个问题。",
      "speaker": "发言人3"
    },
    {
      "time": "00:11:01",
      "text": "好的，讲到这次这个compute x大会上，作为AI算力的头部的两家厂商，AMD和英伟达在这个大会上有没有发布什么让您觉得比较眼前一新的产品呢？",
      "speaker": "发言人2"
    },
    {
      "time": "00:11:14",
      "text": "肯定的，这次我们看到这个大会上的变化还是比较多的，这些刚才您提到这些头部厂商应该说都做了一些组织演讲。首先我们看英伟达的一个情况。这次大会上我们觉得英伟达是在这个算力网络软件这个应用层面，四个层面其实都展示了他们家最新的一个战略展望。第一是这个算力方面，英伟达其实不仅是展示了之前已经公布的这个量产版的blank wall芯片，而且也讲到了2025年会推出这个blackout ultra AI芯片，2026年又会推出下一代的这个AI计算平台rubin，然后2027年又是rubin ultra，所以说整个的更新节奏也是提升到了一年一次。那根据英伟达来讲，之前从2016年的这个pass code架构到2024年的，他自己芯片的这个计算能力，浮点运算以及人工智能的浮点运算能力，其实提升是超过了1000倍的，超越了摩尔定律。用于训练GPT4的这个模型能耗也节省了350倍。这个算力的上升发展性能提高是非常快速的，而且迭代节奏来说也比原来变快。",
      "speaker": "发言人3"
    },
    {
      "time": "00:12:20",
      "text": "第二点上来说，不仅是算力层面，也在迭代哈那网络层面就我刚才讲到了，因为你现在涉及到整个的这个AI芯片，不是一个单卡的布局的这种状态了。其实你是一个多卡协同的。所以我们也看到在网络层面上，其实英伟达对于以太网的生态展望也是比较积极的那这一次其实也公布了一些新的产品的路线图。我们看到其实2024到26年，英伟达也是计划发布了一系列的产品。包括不同速率的以太网的交换机，以及网卡等等一些产品。实现以以太网的技术的这种标准，实现了一个从万卡10万卡，甚至未来到这个百万卡集的连接。这块来看，我们觉得同样是也会跟这个计算能力一样，按照每年为一个维度往前去迭代，我觉得这个也是我们看到的一个变化。当然软件方面这边也做了很多的一些更新，这边就不做赘述了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:13:16",
      "text": "相同的就是AMD这边其实也跟英伟达在往前以同样的节奏进行迭代自己的产品。这一次的computer x大会上，公司也是展示了新的一个云端加速卡的路线图哈那我们看到AMD这边也是会在今年推出325X的加速卡，2025年是350，2020年是产这个MI400。根据最新推出的MI325X来看，AMD的这一颗新的芯片，其实有望采用这个HBM3E的内存，内存带宽也有所提升。相比较英伟达的这个H20的上一代芯片而言，AMD的这个MS25X在FP8以及FP16的算力精度下，其实是有望提升1.3倍的性能的。这个芯片公司也公告是在4Q24会开始出货。后面的话再下一代产品到MS350，AMD也会引入更低一些的数据精度格式，比如说像IP4、IP6，以推动更快的推也能落地。所以整个来看，我觉得总结一下，其实两家大的这种头部的云端算力芯片厂商，其实都加速的迭代了他们自己的云端算力芯片产品。包括英伟达，它不仅是在算力芯片上去做迭代，相关的网络通信配套上也去做迭代。也看出了大家对于整个算力需求，往后2到3年的一个发展的一个乐观的展望，以及公司的这个算力降本的一个信心。",
      "speaker": "发言人3"
    },
    {
      "time": "00:14:44",
      "text": "我还有一个问题，就是人工智能我们说它有三要素，算力、算法和数据。之前大家一直都在讨论这个算力是不是会对AI发展的一个很大的制约。听您讲下来，感觉还是AMD，包括英伟达在这方面有非常大的一个突破和进展。您觉得对于AI需求的响应，是不是有可能会给我们一个更大的一个想象的空间在未来。",
      "speaker": "发言人2"
    },
    {
      "time": "00:15:08",
      "text": "是的，没错，就是您讲的我非常同意。其实现在来看就是大家算力的这个配套，我觉得第一是性能上的配套，现在是已经达到了这个客户的需求，或者说不作为算法迭代的一个掣肘。然后另外一部分从成本上，大家也积极在推动降本。刚才说降温可能有多少种方式，一种是硬件迭代本身性能上，通过比如制程的升级，封装升级，存储升级等等，这些都可以把这个算力的成本做下来。然后另外一方面，可能我在软件优化上也会做一些相关的一些迭代，使这个算力的成本不断的在往下降。低廉的一个算力成本自然会释放。进一步释放这个算力的需求，也会对未来这个应用落地会有更大的响应空间。当然把应用的这个想象空间推出来以后，应用的这个用户数的一个放量，我们说也会反过来反哺继续去推动算力硬件的这个需求，以一个这种类似飞轮效应的这样的一个模式，去积极的往前的去迭代。所以我觉得这样的一个正向商业模式，一个正反馈的这样的一个循环，慢慢就会随着整个的硬件的进步，以及这个应用的落地。",
      "speaker": "发言人3"
    },
    {
      "time": "00:16:17",
      "text": "慢慢就会跑起来。正如您所说的，这个供给和需求它其实是相互创造的，相互影响的。现在AI的这个需求，包括对AI芯片的一个核心的需求，主要是在哪些行业上会比较多呢？",
      "speaker": "发言人2"
    },
    {
      "time": "00:16:30",
      "text": "现在来看，我觉得云端的需求，我们看到其实应该说是最强烈的。或者说从应用场景上来看，现在大家对于大模型的这种我们所谓的这个参数量的增多。数据量的增多。其实大家还是在这种或者说至少头部企业还是在skin loss。或者说我们讲到现在这种大模型的这种规模定律上去不断的探索，所以我们也看到在这块儿的一个需求。其实在未来1到2年，我们还是能看到比较清晰的一个客户的一个下单，或者说我们至少看到持续性是足够的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:17:04",
      "text": "然后另外一方面，是对于端测的，那端测来看，我们觉得今年最主要的还是会以这个AIPC和AI手机做落地的这个场景。因为我们也看到，苹果也是在24年的开发者大会上公布了自己在AI领域的一些布局和更新。也讲到了可能下一代的硬件，新一款的苹果手机，或者说上一款的部分的手机，才能对AI的相关的模型在端侧落地做支持。刚才讲了包括AIPC这边，今年可能出货量也会有一个一定幅度，可观出货量的一个增长。所以我们觉得在端侧来看，其实主要可能还是会在传统的一些终端上，像手机和PC线性落地。后面的话我们觉得可以更加期待，可能未来3到5年的维度，比如说会延伸到比如说像自动驾驶，包括还有ARVR等等这些可能更有想象力的一些场景。",
      "speaker": "发言人3"
    },
    {
      "time": "00:18:00",
      "text": "还有一个问题，就是除了AMD和英伟达之外，其他的一些海外的厂商，他们不是也有一些突破吗？他们的这些突破会不会对现在的这个市场格局会产生一些影响呢？",
      "speaker": "发言人2"
    },
    {
      "time": "00:18:12",
      "text": "这个确实也能看得到，实际上除了英伟达和这个AMD以外，所有的我们可以看到老牌的这种半导体芯片公司，其实都在AI芯片上做了一些努力。在这一次这个computer x大会上，我们也看到，其实英特尔也是更新了下一代的，他自己叫内部代号叫lunar lake的一个处理器架构的一个最新细节。整个的处理器架构其实会提供一个超80%增长的一个游戏性能，然后五倍的AI性能，同时也是加速这个AI计算的同时，还保持了一个低功耗的一个水平。公司也提到在这个3Q24或者2024年下半年来看，应该会有超80款的PC机型，会搭载它下一代的lunar lake的处理器，也是对这个AIPC做一个全面的一个支持。",
      "speaker": "发言人3"
    },
    {
      "time": "00:19:06",
      "text": "另外我们也看到还有高通，高通其实在之前公司也是发布了骁龙的x elite系列的处理器。那高通这边的新的产品，NPU，就它的这个AI核的能效，其实是竞品MCI芯片的将近三倍了。AI算力也达到了45 tops，也是很高的一个观测的水平。我们也看到其实这个所有的，就是我们说上述讲到过，还有包括我们刚才也讨论过的，这些大厂的这个产品案例，其实都在往AI在端侧落地的这种支持上去做努力。我们也看到大家无论是在这个性能和功耗上，其实都会不断的去迭代自己的产品，给这个AIPC去赋能。我觉得这个应该是所有的科技巨头，我们看到都在追求的一个战略的一个方向。",
      "speaker": "发言人3"
    },
    {
      "time": "00:19:57",
      "text": "包括我们看到像OpenAI，他现在也在自己加长做这个芯片，你怎么看？这些就不仅仅是大厂，就是包括这些相当于算法这种工，他们自己也在做芯片，这个趋势你怎么看？",
      "speaker": "发言人1"
    },
    {
      "time": "00:20:10",
      "text": "是这样，我觉得所谓的这个大模型厂商，或者说这种云厂商自己下现场做芯片，这也是一个非常逻辑上，应该说是非常直观的一个事情。因为说实话做AI刚才我讲的，它其实是一种可能更偏专用的一种应用。或者说我的这个硬件和软件，其实说白了就是我的硬件结构和我的算法，其实绑定的应该说是比较深的。或者说对我的算法的现有的这种结构以及未来的演进方向，知道的越清楚的厂商应该说能够明白这个硬件往哪儿去定义。",
      "speaker": "发言人3"
    },
    {
      "time": "00:20:43",
      "text": "其实过去英伟达一直也在做这样的一个，我们说做这样的一个事情，为什么他能在整个AI芯片领域一直处于一个行业的垄断地位？其实核心的一点也是在于，他其实是绑定了大家的需求。他非常知道这个算法未来的迭代方向是什么。对于大厂来讲，他如果在算法上有自己的一个发展方向的判断，以及他想去找一个更低成本，与自己的这个软件更耦合的这样的一个方向去做落地的话。",
      "speaker": "发言人3"
    },
    {
      "time": "00:21:14",
      "text": "我觉得实际上我们也可以理解到，就是他们自己做芯片的一个动机。但这个东西就是说跟这个所谓的三方供应商是不是冲突的？我觉得其实也并不一定。就是大厂做芯片这个事儿，其实也不是这两年才开始的。我们看到像谷歌、亚马逊、微软等等这些厂商，其实他们做芯片已经都是很多年的一个努力了，对吧？",
      "speaker": "发言人3"
    },
    {
      "time": "00:21:38",
      "text": "现在也有多代的芯片已经实际投入使用。但是我们也没有发现，他们真的就是对英伟达的这个商业机会，有多大的我们说侵蚀。我觉得这个东西可能更多的还是在不同的业务场景上。有些业务场景可能确实定制化需求比较高大厂或者算法的自己开发商他有一个需要一个软硬件紧耦合的这样的一个更低成本的方案，所以他会去选择做这么一个芯片，但是并不代表所有的场景他都不需要三方芯片供应商的一个支持了。目前来看，这两个方式并不是一个零和的一个结果，其实大家还是会有各自不同的一个痛点在。",
      "speaker": "发言人3"
    },
    {
      "time": "00:22:18",
      "text": "那在本次大会上是否还有提到其他与科技硬件相关的这些发展动态呢？",
      "speaker": "发言人1"
    },
    {
      "time": "00:22:24",
      "text": "好的，这次的这个大会上，我们觉得其实除了刚才讲到的一些云测端测的芯片的变化以外，这个硬件变化以外，其实整个的这个系统散热这块，也是非常值得关注的一个主题。我们看到随着服务器的功率提升，其实服务器中包括像CPU，GPU以及相关的存储设备，电源以及网络设备及冷却系统，其实自身产生的热量都在往上走。我们认为其实更加高效的一个散热方式，其实也是保证这个数据中心正常工作的一个必须。",
      "speaker": "发言人3"
    },
    {
      "time": "00:22:57",
      "text": "其实整个的散热的方式也会有一个升级。以前我们看到基本上是以这个风冷，就是风扇散热的方式去做这个服务器的散热。现在我们看到在风扇散热的整个的效率不够了以后，其实我们看到还有这个液冷相关的，就是说液体相关的这个散热方式，慢慢走入我们的这个演练当中了。我们也看到现在随着整个算力要求的提升以后，服务器的功率是越来越大。现在英伟达的一个机柜的功率其实都在100千瓦以上。可能这么大的功率，我们觉得一定对这个散热技术也有着升级的要求。",
      "speaker": "发言人3"
    },
    {
      "time": "00:23:33",
      "text": "前面提到了，可能这个液冷也是作为散热效率更高的方式。凭借着提升算力部署的密度，以及降低系统功耗的一个优势。有望逐步的对传统的这种风冷型的散热，其实是出现一个替代的一个关系。未来我们觉得随着整个AI算力的规模的增长，相关的液冷的散热技术市场的一个增速，应该也是一个偏正向的一个展望。",
      "speaker": "发言人3"
    },
    {
      "time": "00:24:02",
      "text": "陈老师我们看这个应该如何去评估因为此轮生成式AI的浪潮带来的硬件市场的机会呢？",
      "speaker": "发言人1"
    },
    {
      "time": "00:24:10",
      "text": "好的，这个事儿我们觉得还是从两个维度来说。第一还是从全球的这个大周期的维度，可能整个的市场机会还是由一个大的一个逻辑主线。由云测到端测，由训练的推理，以这个逻辑主线去进行演进的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:24:28",
      "text": "首先我们还是看到这个训练市场机会其实已经持续了有一年多的时间，但是我们还是觉得这个训练端的硬件的采购，暂时也没有见到停止，或者说短期内有调整的这样的一个状态。大家还是在这些大模型的头部厂商的对我们说规模效应，以及大模型的前沿技术探索的这样的一个趋势之下。对于整个训练芯片，包括训练的算力硬件的采购，应该说还是非常积极的那也就是带来了云端的这个AI的加速芯片服务器，包括这个交换机，交换芯片，光模块等等跟互联相关的这些硬件。我们觉得整个的机会还是都在的。而且以后就算在模型的迭代上，我们觉得可能未来比如说如果有一天会相对走向放缓，但是从整个的模型往后来看，即便上模型的规模的参数，数据量，如果说出现一定的放缓的迭代的趋势以后，那我们也看到对于整个模型的金条，或者说蒸馏的这些需求，其实我们觉得也都会持续带有。所以说训练这个事情应该说一直都不会停止。",
      "speaker": "发言人3"
    },
    {
      "time": "00:25:40",
      "text": "即便是这种算算力的需求，或者硬件的需求是从训练开始，但是它也会持续贯穿到整个的AI浪潮的始终，这是第一点。第二点上来说，当模型训练到一定程度，具备这个商业落地能力了以后，我们会把整个的关注度一向推一段。因为整个英伟达也在他的公开议会上指出了，目前他可能整个的AI芯片的出货，有40%可能都是来自于这个推理测了已经那这样的一个变化，其实我们也看到整个的市场的增长的动能，逐渐从训练再往推理去转移。当然英伟达讲的可能更多的是云测的这个推理。那我们也看到现在大家在AI芯片上，其实更多的是去做这个成本的优化，做这个算力的降本，其实更多的也是要去配合当下在这个云端推理上，客户需求开始放量的这样的一个时间点，其实也是对这种需求做的一个配套。所以很明显，我们觉得这1到2年，或者说从24年今年开始，到25年去展望。那推理市场对于硬件的这个需求的拉动应该说是不亚于训练的，是这样的一个情况。",
      "speaker": "发言人3"
    },
    {
      "time": "00:26:54",
      "text": "在云端推理之后，更多我们觉得慢慢随着这模型能力的进一步增强，包括我们刚才也讲到了，整个现在我们看到一些智能手机厂商，包括苹果，其实也在端侧开始推AI的一些应用实际落地了。那这一块的一个变化就是说在云端的这个推力体量的同时，那端侧的我们讲到这个AI手机，包括AIPC，甚至到未来的这个车，还有ARVR眼镜等等。端侧的体量应该也是一个同步的。或者说随着云测推理的放量，循序渐进的，端测的机会也会慢慢的到来。而且端测得益于整个的设备数的一个基数的一个庞大，那所带来的AI硬件的市场空间，我们认为其实也不会低。所以说整个的一个逻辑线是说先从这个训练开始，训练可能也会贯穿在整个AI浪潮发展的始终，去驱动这个硬件市场的增长。慢慢训练到的一个模型，达到一个商业化落地的一个能力之后，我们看到先出来的是云端推理，慢慢的从云端推理再转向端侧推理，转向二者的结合，会推动整个AI相关的硬件市场达到一个相对比较新的一个高度。我们对整个的AI浪潮带来的硬件市场机会，应该说还是一直维持着比较积极的一个看法的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:28:18",
      "text": "好的。非常感谢陈乔生老师这次做客我们中金研究院的播客节目。如果大家对本次播客节目的内容感兴趣的话，也欢迎大家可以关注我们中金研究院的微信公众号和中金点睛的微信公众号。希望大家可以跟我们多多互动，非常感谢大家的收听，本期节目到此结束。",
      "speaker": "发言人1"
    }
  ],
  "lab_info": {
    "summary": "在讨论中，焦点放在了2024年中国台湾Computer X大会的亮点及AI芯片技术的最新进展。大会汇聚了全球科技巨头与行业精英，展示了AI芯片在加速神经网络计算方面的显著优势，及其对AI应用普及的推动作用。特别地，讨论深入到AI芯片的分类，如云测与端测芯片，分析了其技术发展、应用前景和市场机遇。同时，分析了英伟达、AMD等主要芯片厂商的最新产品，探讨了硬件市场的机遇与大模型厂商自主研发芯片的趋势。最后，讨论转向高效散热技术的进步及其对数据中心的影响，突出了散热技术在提升AI芯片性能中的关键作用。整个对话提供了对AI芯片技术及其市场动态的深刻洞察。",
    "qa_pairs": [
      {
        "question": "为什么发展AI需要专门的AI芯片，而不是用通用芯片呢？",
        "answer": "AI芯片的出现是由于任务类型的不同。传统通用芯片如CPU更适合处理多种复杂逻辑控制的任务，而AI任务主要是加速神经网络计算，尤其是针对卷积计算、矩阵乘法等特定类型的应用。由于任务类型的单一化，通用芯片在执行AI任务时成本和效率可能无法达到最优结果，因此需要专门设计的AI芯片来实现更高效和低成本的加速效果。",
        "time": "00:02:43"
      },
      {
        "question": "推理芯片的特点和应用场景是什么？",
        "answer": "推理芯片主要用于实际任务部署和商业落地阶段，当模型已经构建好时，推理芯片负责快速响应和处理请求，确保人机交互的及时性和低成本。云测和端测均可以部署推理芯片，但云测芯片性能更强，用于处理复杂的任务，而端测芯片则针对相对简单任务优化时延和成本指标。",
        "time": "00:05:36"
      },
      {
        "question": "AI芯片可以分为几类？能否介绍一下？",
        "answer": "AI芯片主要按部署位置分为两大类：云测和端测。云测AI芯片部署在数据中心，包括互联网数据中心和边缘区域的数据中心，用于大规模的AI模型训练和推理。端测AI芯片则安装在个人电脑、手机、汽车及物联网设备等终端设备中，负责对AI算法进行加速和提供智能交互支持。在云测内部和端测内部，还可根据任务类型进一步细分为训练芯片和推理芯片。",
        "time": "00:03:29"
      },
      {
        "question": "训练芯片的作用是什么？",
        "answer": "训练芯片主要用于人工智能模型的训练过程，将一个偏野生状态的模型调整为精确的输入输出对应关系的算法模型。对于训练芯片来说，计算吞吐量的要求相对较高，而对时效性要求不高，重点在于能够高效地完成模型训练任务。",
        "time": "00:04:31"
      },
      {
        "question": "对于云测而言，在评估产品时，客户看重的指标有哪些？",
        "answer": "客户在评估云测产品时，重点关注的指标包括算力、存储容量、带宽以及芯片间的互联能力等，这些因素能够加速训练过程。而在端侧，客户对算力的要求可能不高，更注重芯片对于时延的敏感性，以及存储容量和带宽的需求，同时也会考虑成本因素，追求在保证一定计算能力的同时降低成本。",
        "time": "00:06:43"
      },
      {
        "question": "目前芯片厂商在开发上的着力点是什么？",
        "answer": "目前芯片厂商在开发上的着力点不仅限于追求极致性能，而是转向如何在硬件和软件层面进行优化，以实现单位算力成本的下降，从而满足用户对性能持续增长的需求和对成本控制的关注。",
        "time": "00:10:39"
      },
      {
        "question": "在最近的compute x大会上，AMD和英伟达有没有发布令人眼前一新的产品？",
        "answer": "英伟达在大会上展示了其在算力网络软件应用层面的最新战略展望。在算力方面，英伟达介绍了已公布的blank wall芯片，并预告了未来几年将推出一系列AI芯片和计算平台，其计算能力相较于2016年的pass code架构提升超过1000倍，能耗效率也显著提升。此外，在网络层面，英伟达也公布了针对以太网生态的积极展望和相关产品的路线图，旨在实现从万卡到百万卡级别的连接能力，并按年迭代更新相关产品。",
        "time": "00:11:14"
      },
      {
        "question": "在最近的computer x大会上，AMD展示了新的云端加速卡路线图，能具体说一下AMD的最新产品和性能提升吗？",
        "answer": "是的，在大会上AMD展示了其云端加速卡的迭代进程。今年预计推出的是325X加速卡，2025年将推出350系列，而2023年则会产出MI400。最新的MI325X芯片有望采用HBM3E内存，内存带宽有所提高。相较于英伟达上一代H20芯片，在FP8和FP16算力精度下，AMD的新芯片性能有望提升1.3倍，并且预计在4Q24开始出货。此外，下一代的MS350产品还会引入更低数据精度格式如IP4、IP6以实现更快的应用落地。",
        "time": "00:13:16"
      },
      {
        "question": "现在AI的核心需求主要集中在哪些行业？",
        "answer": "目前云端需求最为强烈，特别是在大模型领域，各大企业仍在探索参数量和数据量的提升。在未来1-2年内，我们能看到客户持续下单并下单清晰。另一方面，端侧需求今年主要以AIPC和AI手机落地为主，苹果公司在其开发者大会上也公布了在AI领域的布局和更新，预计下一代硬件将支持更多AI模型在端侧的应用。",
        "time": "00:17:04"
      },
      {
        "question": "除了AMD和英伟达，其他海外厂商在AI芯片领域的突破是否会对现有市场格局产生影响？",
        "answer": "确实有一些老牌半导体芯片公司在AI芯片上做出努力，例如英特尔在computer x大会上介绍了内部代号为lunar lake的处理器架构，提供了超80%的游戏性能增长和五倍的AI性能提升，预计在2024年下半年会有超80款PC机型搭载支持AIPC的lunar lake处理器。此外，高通也发布了骁龙x elite系列处理器，其AI核能效是竞品MCI芯片的将近三倍，AI算力达到45 tops。这些大厂都在不断迭代产品以增强端侧AIPC支持，预示着AI在端侧落地的趋势将持续加强。",
        "time": "00:18:12"
      },
      {
        "question": "对于OpenAI等算法公司自己研发芯片的现象，您怎么看？",
        "answer": "我认为大模型厂商或云厂商自主研发芯片是非常合理且直观的，因为AI是一种专用性较强的领域，硬件结构与算法紧密相关，对算法演进方向理解更深的厂商能够更好地定义和优化硬件结构。",
        "time": "00:20:10"
      },
      {
        "question": "英伟达为何能在AI芯片领域保持行业垄断地位？",
        "answer": "英伟达的核心优势在于它绑定了客户的需求，并预见了算法未来的迭代方向。对于大厂而言，如果他们有自己的算法发展方向判断，并寻求更低成本与自家软件更紧密耦合的解决方案，会选择自研芯片；但这并不意味着所有场景都不需要第三方芯片供应商的支持。",
        "time": "00:20:43"
      },
      {
        "question": "大厂自研芯片与三方供应商是否存在冲突？",
        "answer": "大厂自研芯片并不排斥三方供应商。例如谷歌、亚马逊、微软等已经多年投入芯片研发，并有多代芯片实际投入使用，但这些大厂并未完全替代英伟达的商业机会，而是在不同业务场景上存在互补关系。",
        "time": "00:21:38"
      },
      {
        "question": "本次大会上还有哪些与科技硬件相关的最新发展动态？",
        "answer": "大会上另一个重要主题是服务器散热技术的升级。随着服务器功率提升，包括CPU、GPU、存储设备、电源和网络设备及冷却系统在内的整体热量也在增加，高效散热解决方案对于数据中心正常运行至关重要。散热方式正从传统的风冷向液冷等更高效率的方式转变。",
        "time": "00:22:24"
      },
      {
        "question": "液冷散热技术是否会取代风冷成为主流散热方式？",
        "answer": "液冷散热因其提升算力部署密度和降低系统功耗的优势，有望逐步替代传统的风冷型散热技术，并随着AI算力规模增长，液冷散热技术市场增速也将呈现正向展望。",
        "time": "00:23:33"
      },
      {
        "question": "如何评估本轮生成式AI浪潮带来的硬件市场机会？",
        "answer": "评估本轮AI浪潮带来的硬件市场机会应从两个维度考虑：一是从全球大周期逻辑主线看，由云测到端测，从训练到推理的演进过程将持续推动硬件需求；二是当模型训练达到商业落地能力后，云端推理和端侧推理市场的硬件需求将同步增长，尤其是随着智能手机厂商等在端侧AI应用的实际落地，端侧AI硬件市场空间也将逐渐显现，从而整体推动AI相关的硬件市场达到新高度。",
        "time": "00:26:54"
      }
    ],
    "chapters": [
      {
        "time": "00:00:00",
        "title": "2024年中国台湾Computer X大会：AI芯片技术进步与应用",
        "summary": "在2024年中国台湾举办的Computer X大会上，中金研究部科技硬件团队发布了关于AI芯片技术的深度洞察。讨论重点在于为什么AI需要专门的芯片而不是通用芯片，主要是因为AI任务需要加速神经网络计算，与通用计算任务不同。AI芯片专门设计用于加速如卷积计算和矩阵乘法等特定类型的应用，以实现更高的效率和更低的成本。"
      },
      {
        "time": "00:03:18",
        "title": "AI芯片的分类与应用",
        "summary": "AI芯片主要分为两大类：云测和端测。云测芯片部署在数据中心，适用于大规模计算任务，包括互联网数据中心和边缘数据中心。端测芯片则应用于个人电脑、手机、汽车及物联网设备等，实现设备的智能化功能，如智能音箱的语音对话。此外，AI芯片还按任务类型分为训练芯片和推理芯片，训练芯片侧重于模型训练，对计算吞吐量要求高但时效性要求相对较低；推理芯片主要用于模型部署和实际任务执行，强调低时延和低成本，适用于商业落地和人机交互。"
      },
      {
        "time": "00:06:33",
        "title": "AI芯片技术发展衡量指标与端侧需求差异分析",
        "summary": "在AI芯片技术发展中，云测与端侧有着不同的衡量指标和需求重点。云测方面，算力、存储容量与带宽、芯片间的互联能力等成为衡量技术发展的关键指标，这些因素能显著加速训练过程。而在端侧，由于计算量相对较少，对算力的要求不高，更多关注的是时延敏感性、存储容量、带宽和成本。端侧的芯片需求强调低成本和效率，即使不追求极致性能，也要确保商业模式的可持续性。这表明，云测和端侧在AI芯片技术的需求上呈现出明显的差异。"
      },
      {
        "time": "00:08:35",
        "title": "AI芯片开发趋势：从追求极致性能到注重成本效益",
        "summary": "在AI芯片领域，随着生成式AI的兴起，特别是ChatGPT的流行，最初业界对AI芯片的性能有着极致的追求，重点关注算力、存储容量及带宽、互联能力等方面的提升。然而，随着时间的推移，行业焦点开始转向对成本效益的重视，即如何在保持性能提升的同时，有效降低成本，确保商业模式的可持续发展。这种转变反映出，在持续的性能需求增长背景下，成本控制成为推动行业健康发展的关键因素。"
      },
      {
        "time": "00:10:57",
        "title": "Compute X大会：AMD和英伟达发布新一代AI算力芯片",
        "summary": "在最近的Compute X大会上，AMD和英伟达作为AI算力领域的头部厂商，分别展示了其最新的产品路线图和战略展望。英伟达不仅展示了量产版的AI芯片，并计划在2025年推出Blackout Ultra AI芯片，2026年和2027年分别推出下一代AI计算平台Rubin及Rubin Ultra，显示了其每年一次的产品更新节奏。自2016年以来，英伟达的芯片计算能力提升了超过1000倍，显著超越了摩尔定律。此外，英伟达还公布了以太网交换机和网卡等产品路线图，致力于实现更高速率的网络连接。而AMD方面，展示了新的云端加速卡路线图，计划于今年推出325X加速卡，后续将推出350和MI400产品，显示出AMD在算力芯片迭代上的积极步伐。这些发布表明，两大头部厂商对未来的算力需求和发展持乐观态度，并致力于降低算力成本。"
      },
      {
        "time": "00:14:43",
        "title": "算力进步与AI应用的正向循环",
        "summary": "随着AMD和英伟达在算力技术上的突破，人工智能（AI）的发展获得强大支撑，不仅性能满足当前客户需求，且通过硬件迭代和软件优化降低了成本，进而激发更广泛的AI应用需求。这种正向循环促进硬件进步和应用落地，特别是在云端服务和终端设备（如AIPC和AI手机）上，展现出对AI芯片的巨大需求。未来，自动驾驶和AR/VR等领域预期将带来更多创新应用。"
      },
      {
        "time": "00:17:56",
        "title": "海外半导体芯片公司在AI芯片领域的突破及其影响",
        "summary": "除了AMD和英伟达外，其他海外半导体芯片公司也在AI芯片领域取得突破，可能对市场格局产生影响。英特尔更新了其下一代处理器架构\"lunar lake\"，提供超80%的游戏性能增长和五倍的AI性能提升，同时保持低功耗。预计2024年下半年将有超过80款PC机型搭载此处理器。高通也发布了骁龙X Elite系列处理器，其NPU的能效是竞品的三倍，AI算力达到45 TOPS。这些进展表明，科技巨头正在不断提升产品性能和降低功耗，以支持AI在端侧的落地，这是他们共同的战略方向。"
      },
      {
        "time": "00:19:56",
        "title": "大模型厂商自研芯片的趋势与影响",
        "summary": "随着AI技术的发展，大模型厂商和云服务提供商开始自行研发芯片，以满足特定算法的需求并降低成本。这种做法并不是新现象，谷歌、亚马逊、微软等公司早已涉足芯片研发，并已有多代产品投入实际使用。尽管这可能被视为对传统芯片供应商如英伟达的挑战，但实际上，大厂自研芯片和第三方芯片供应商之间并不是零和关系，两者可以在不同业务场景中共存。自研芯片主要针对定制化需求高的场景，寻求软硬件紧密耦合的低成本解决方案，而并不意味着放弃与三方芯片供应商的合作。"
      },
      {
        "time": "00:22:24",
        "title": "数据中心散热技术的演进与展望",
        "summary": "随着服务器功率的提升，传统的风冷散热方式效率逐渐不足，液冷散热技术因其高效率和能够提升算力部署密度、降低系统功耗的优势，开始受到关注并逐渐应用于数据中心。液冷技术的引入，预示着未来数据中心散热技术将可能实现从风冷到液冷的转变，特别是在AI算力规模增长的背景下，液冷散热技术市场预计将呈现正向增长趋势。"
      },
      {
        "time": "00:23:58",
        "title": "评估生成式AI浪潮对硬件市场的影响",
        "summary": "陈老师在中金研究院的播客节目中讨论了生成式AI浪潮如何影响硬件市场。从全球大周期的角度看，市场机会主要沿着从云侧到端侧、从训练到推理的逻辑演进。训练市场的硬件采购在过去一年多里持续增长，显示出对大模型规模效应和前沿技术探索的积极态度。此外，随着模型训练达到商业落地能力，市场关注点将转向推理，进而推动云端推理和端侧推理的硬件需求。预计未来1到2年内，推理市场对硬件的需求将不亚于训练市场。随着技术进步，端侧AI应用如智能手机、AIPC、车辆和AR/VR眼镜等也将带来庞大的硬件市场空间。陈老师对AI浪潮带来的硬件市场机会持积极看法。"
      }
    ],
    "mindmap": {
      "children": [
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "任务类型不同：AI芯片专门加速神经网络计算"
                },
                {
                  "children": [],
                  "content": "性能要求：加速人工智能算法（如卷积计算、矩阵乘法等）"
                },
                {
                  "children": [],
                  "content": "成本和效率：实现高效及低成本的计算效果"
                }
              ],
              "content": "发展AI芯片的原因"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "按部署位置分：云测和端测"
                },
                {
                  "children": [],
                  "content": "按应用场景分：训练芯片和推理芯片"
                }
              ],
              "content": "AI芯片分类"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "云测：算力要求高、存储容量和带宽、芯片间互联能力"
                },
                {
                  "children": [],
                  "content": "端测：时延敏感性、存储容量、成本考量"
                }
              ],
              "content": "衡量指标"
            }
          ],
          "content": "AI芯片技术"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "英伟达：展示算力网络软件应用，未来产品发布路线图"
                },
                {
                  "children": [],
                  "content": "AMD：新的云端加速卡路线图，性能提升计划"
                },
                {
                  "children": [],
                  "content": "其他厂商：英特尔、高通等也在AI芯片上努力"
                }
              ],
              "content": "厂商动态"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "云端：大模型训练和推理需求持续增长"
                },
                {
                  "children": [],
                  "content": "端测：智能手机、PC、自动驾驶和AR/VR等场景"
                }
              ],
              "content": "行业需求"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "散热技术：从风冷向液冷转变，适应更高功率需求"
                }
              ],
              "content": "技术突破"
            }
          ],
          "content": "市场发展与趋势"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "从训练到推理，再到端测的应用落地"
                },
                {
                  "children": [],
                  "content": "预期推理市场硬件需求将持续增长"
                }
              ],
              "content": "硬件市场机会"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "降本趋势：成本优化成为重要着力点"
                },
                {
                  "children": [],
                  "content": "商业闭环：算力成本降低促进AI应用落地"
                }
              ],
              "content": "行业影响"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "性能与成本平衡：追求高效能和低成本"
                },
                {
                  "children": [],
                  "content": "软硬件结合：算法迭代与硬件优化相结合"
                }
              ],
              "content": "AI芯片开发趋势"
            }
          ],
          "content": "未来展望"
        },
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "content": "OpenAI、谷歌、亚马逊等自有芯片开发"
                }
              ],
              "content": "大厂动态"
            },
            {
              "children": [
                {
                  "children": [],
                  "content": "硬件与算法的紧密结合要求"
                },
                {
                  "children": [],
                  "content": "高效散热技术的开发和应用"
                }
              ],
              "content": "行业挑战"
            }
          ],
          "content": "行业内外动向"
        }
      ],
      "content": "中金研究院播客节目脑图摘要"
    }
  }
}